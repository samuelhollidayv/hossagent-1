{"file_contents":{"lead_service.py":{"content":"\"\"\"\nLead Service for HossAgent.\nHandles lead generation, deduplication, and management.\n\nDeduplication rules:\n- Primary: Match by email address\n- Secondary: Match by (company_name + normalized_domain)\n- Domain normalization: strips www., http://, https://, trailing paths\n\nLogs all dedupe events with [LEADS][DEDUPED] tag.\n\"\"\"\nimport json\nimport re\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Optional\nfrom pathlib import Path\nfrom sqlmodel import Session, select\nfrom models import Lead\nfrom lead_sources import (\n    get_lead_source_config,\n    get_lead_source_provider,\n    LeadCandidate,\n    LeadSourceConfig\n)\n\n\nLEAD_SOURCE_LOG_FILE = Path(\"lead_source_log.json\")\nMAX_LOG_ENTRIES = 5000  # Capped to prevent unbounded growth\n\n\ndef _normalize_domain(website: Optional[str]) -> Optional[str]:\n    \"\"\"\n    Normalize a website URL to a domain for deduplication.\n    \n    Examples:\n        https://www.example.com/page -> example.com\n        http://example.com -> example.com\n        www.example.com -> example.com\n    \"\"\"\n    if not website:\n        return None\n    \n    domain = website.lower().strip()\n    domain = re.sub(r'^https?://', '', domain)\n    domain = re.sub(r'^www\\.', '', domain)\n    domain = domain.split('/')[0]\n    domain = domain.split('?')[0]\n    \n    return domain if domain else None\n\n\ndef _normalize_company(company: str) -> str:\n    \"\"\"Normalize company name for comparison.\"\"\"\n    return company.lower().strip()\n\n\ndef _load_lead_source_log() -> Dict[str, Any]:\n    \"\"\"Load lead source run log.\"\"\"\n    try:\n        if LEAD_SOURCE_LOG_FILE.exists():\n            with open(LEAD_SOURCE_LOG_FILE, \"r\") as f:\n                return json.load(f)\n    except Exception:\n        pass\n    return {\"runs\": [], \"last_run\": None, \"last_created_count\": 0}\n\n\ndef _save_lead_source_log(log_data: Dict[str, Any]) -> None:\n    \"\"\"Save lead source run log.\"\"\"\n    try:\n        if \"runs\" in log_data:\n            log_data[\"runs\"] = log_data[\"runs\"][-MAX_LOG_ENTRIES:]\n        with open(LEAD_SOURCE_LOG_FILE, \"w\") as f:\n            json.dump(log_data, f, indent=2)\n    except Exception as e:\n        print(f\"[LEADS] Warning: Could not save lead source log: {e}\")\n\n\ndef log_lead_source_run(\n    provider: str,\n    niche: str,\n    geography: Optional[str],\n    candidates_fetched: int,\n    leads_created: int,\n    leads_skipped: int\n) -> None:\n    \"\"\"Log a lead source run for admin visibility.\"\"\"\n    log_data = _load_lead_source_log()\n    \n    run_entry = {\n        \"timestamp\": datetime.utcnow().isoformat(),\n        \"provider\": provider,\n        \"niche\": niche,\n        \"geography\": geography,\n        \"candidates_fetched\": candidates_fetched,\n        \"leads_created\": leads_created,\n        \"leads_skipped\": leads_skipped\n    }\n    \n    log_data[\"runs\"].append(run_entry)\n    log_data[\"last_run\"] = run_entry[\"timestamp\"]\n    log_data[\"last_created_count\"] = leads_created\n    \n    _save_lead_source_log(log_data)\n\n\ndef get_lead_source_log() -> Dict[str, Any]:\n    \"\"\"Get lead source run log for admin display.\"\"\"\n    return _load_lead_source_log()\n\n\ndef get_recent_auto_leads(limit: int = 10) -> List[Dict[str, Any]]:\n    \"\"\"Get most recent auto-created leads for admin display.\"\"\"\n    log_data = _load_lead_source_log()\n    return log_data.get(\"recent_leads\", [])[-limit:]\n\n\ndef _lead_exists(session: Session, email: str, company: str, website: Optional[str] = None) -> Optional[int]:\n    \"\"\"\n    Check if a similar lead already exists.\n    \n    Matches by:\n    1. Email (primary) - exact match, case-insensitive\n    2. Company + Domain (secondary) - normalized domain comparison\n    \n    Returns:\n        Lead ID if exists, None otherwise\n    \"\"\"\n    if email:\n        email_lower = email.lower().strip()\n        existing = session.exec(\n            select(Lead).where(Lead.email == email_lower)\n        ).first()\n        if existing and existing.status != \"invalid\":\n            return existing.id\n    \n    if website and company:\n        domain = _normalize_domain(website)\n        company_norm = _normalize_company(company)\n        \n        if domain:\n            all_leads = session.exec(select(Lead).limit(1000)).all()\n            for lead in all_leads:\n                if lead.status == \"invalid\":\n                    continue\n                lead_domain = _normalize_domain(lead.website)\n                lead_company = _normalize_company(lead.company)\n                if lead_domain == domain and lead_company == company_norm:\n                    return lead.id\n    \n    return None\n\n\ndef generate_new_leads_from_source(session: Session) -> str:\n    \"\"\"\n    Main entry point for auto-generating leads via HossNative.\n    \n    This function:\n    1. Gets lead source config from environment\n    2. Uses HossNative provider (autonomous discovery via SignalNet + web scraping)\n    3. Fetches candidates up to max_new_leads_per_cycle\n    4. Deduplicates against existing leads\n    5. Creates new Lead records with status=\"new\"\n    6. Logs the run for admin visibility\n    \n    HossNative discovers leads through SignalNet signals and web scraping.\n    \n    Returns:\n        Status message describing what was done\n    \"\"\"\n    config = get_lead_source_config()\n    provider = get_lead_source_provider()\n    \n    print(f\"[LEADS][SOURCE] Starting lead generation (provider={provider.name}, max={config.max_new_leads_per_cycle})\")\n    \n    candidates = provider.fetch_candidates(config, limit=config.max_new_leads_per_cycle)\n    \n    if not candidates:\n        msg = f\"LeadSource: No candidates from {provider.name}\"\n        print(f\"[LEADS][SOURCE] {msg}\")\n        log_lead_source_run(\n            provider=provider.name,\n            niche=config.niche,\n            geography=config.geography,\n            candidates_fetched=0,\n            leads_created=0,\n            leads_skipped=0\n        )\n        return msg\n    \n    leads_created = 0\n    leads_skipped = 0\n    leads_deduped = 0\n    created_leads = []\n    \n    for candidate in candidates:\n        if not candidate.email and not candidate.website:\n            print(f\"[LEADS][SOURCE] Skipping {candidate.company_name}: no email or website\")\n            leads_skipped += 1\n            continue\n        \n        existing_id = _lead_exists(session, candidate.email or \"\", candidate.company_name, candidate.website)\n        if existing_id is not None:\n            print(f\"[LEADS][DEDUPED] {candidate.company_name}: matches existing lead {existing_id}\")\n            leads_deduped += 1\n            leads_skipped += 1\n            continue\n        \n        email_to_use = candidate.email\n        if email_to_use:\n            email_to_use = email_to_use.lower().strip()\n        else:\n            email_to_use = f\"info@{candidate.company_name.lower().replace(' ', '')}.com\"\n        \n        lead = Lead(\n            name=candidate.contact_name or \"Contact\",\n            email=email_to_use,\n            company=candidate.company_name,\n            niche=candidate.niche or config.niche,\n            status=\"new\",\n            website=candidate.website,\n            source=candidate.source\n        )\n        session.add(lead)\n        session.flush()\n        \n        created_leads.append({\n            \"id\": lead.id,\n            \"company\": lead.company,\n            \"email\": lead.email,\n            \"source\": lead.source,\n            \"created_at\": datetime.utcnow().isoformat()\n        })\n        \n        leads_created += 1\n        print(f\"[LEADS][SOURCE] Created lead: {lead.company} ({lead.email})\")\n    \n    session.commit()\n    \n    log_data = _load_lead_source_log()\n    if \"recent_leads\" not in log_data:\n        log_data[\"recent_leads\"] = []\n    log_data[\"recent_leads\"].extend(created_leads)\n    log_data[\"recent_leads\"] = log_data[\"recent_leads\"][-50:]\n    _save_lead_source_log(log_data)\n    \n    log_lead_source_run(\n        provider=provider.name,\n        niche=config.niche,\n        geography=config.geography,\n        candidates_fetched=len(candidates),\n        leads_created=leads_created,\n        leads_skipped=leads_skipped\n    )\n    \n    msg = f\"LeadSource: Created {leads_created} new leads, skipped {leads_skipped} (provider={provider.name}, niche=\\\"{config.niche[:30]}...\\\")\"\n    print(f\"[LEADS][SOURCE] {msg}\")\n    return msg\n","path":null,"size_bytes":8220,"size_tokens":null},"lead_sources.py":{"content":"\"\"\"\nLead Source System for HossAgent - HossNative ONLY.\n\nHossNative is the ONLY lead source - autonomous discovery without 3rd-party APIs.\nNo Apollo. No Hunter. No Clearbit. Just HossAgent's native intelligence.\n\nConfiguration via environment variables:\n  LEAD_NICHE - Target ICP description (e.g., \"med spa, HVAC, realtor\")\n  LEAD_GEOGRAPHY - Region constraint (default: \"Miami, Broward, South Florida\")\n  MAX_NEW_LEADS_PER_CYCLE - Cap on leads generated per cycle (default: 10)\n\nLead discovery flow:\n  1. SignalNet identifies business signals (news, reviews, events)\n  2. HossNative resolves company domains from signals\n  3. HossNative scrapes websites to find contact emails\n  4. Verified leads become LeadEvents ready for outbound\n\"\"\"\nimport os\nfrom datetime import datetime\nfrom typing import List, Optional, Dict, Any\nfrom pydantic import BaseModel\n\n\nclass LeadSourceConfig(BaseModel):\n    \"\"\"Configuration for lead source targeting.\"\"\"\n    niche: str = \"med spa, HVAC, realtor, roofing, immigration attorney, marketing agency\"\n    geography: Optional[str] = \"Miami, Broward, South Florida\"\n    max_new_leads_per_cycle: int = 10\n\n\nclass LeadCandidate(BaseModel):\n    \"\"\"A potential lead discovered by HossNative.\"\"\"\n    company_name: str\n    contact_name: Optional[str] = None\n    email: Optional[str] = None\n    website: Optional[str] = None\n    niche: Optional[str] = None\n    source: str = \"HossNative\"\n    raw_data: Optional[Dict[str, Any]] = None\n\n\ndef get_lead_source_config() -> LeadSourceConfig:\n    \"\"\"\n    Build LeadSourceConfig from environment variables.\n    Falls back to sensible defaults if not configured.\n    \"\"\"\n    config = LeadSourceConfig(\n        niche=os.getenv(\"LEAD_NICHE\", \"med spa, HVAC, realtor, roofing, immigration attorney, marketing agency\"),\n        geography=os.getenv(\"LEAD_GEOGRAPHY\", \"Miami, Broward, South Florida\"),\n        max_new_leads_per_cycle=int(os.getenv(\"MAX_NEW_LEADS_PER_CYCLE\", \"10\"))\n    )\n    return config\n\n\nclass HossNativeProvider:\n    \"\"\"\n    HossNative Lead Discovery Provider.\n    \n    The ONLY lead source for HossAgent - autonomous discovery using:\n    - SignalNet for business signals and opportunities\n    - Web scraping for email discovery\n    - Domain resolution from company names\n    - Pattern-based email validation\n    \n    No external APIs. No paid enrichment. Just native intelligence.\n    \"\"\"\n    \n    name: str = \"HossNative\"\n    last_error: Optional[str] = None\n    last_status: str = \"ready\"\n    \n    def fetch_candidates(self, config: LeadSourceConfig, limit: int) -> List[LeadCandidate]:\n        \"\"\"\n        Fetch lead candidates from HossNative discovery.\n        \n        HossNative doesn't \"fetch\" candidates like an API - instead it:\n        1. Uses SignalNet to identify business signals\n        2. Extracts company information from those signals\n        3. Resolves domains and scrapes for contact emails\n        \n        The actual discovery happens through the enrichment pipeline.\n        This method returns empty since leads come from SignalNet events.\n        \"\"\"\n        print(\"[LEADS][HOSSNATIVE] Using SignalNet for autonomous lead discovery\")\n        print(f\"[LEADS][HOSSNATIVE] Geography: {config.geography}, Niche: {config.niche[:50]}...\")\n        \n        self.last_status = \"active\"\n        self.last_error = None\n        \n        return []\n\n\ndef get_lead_source_provider() -> HossNativeProvider:\n    \"\"\"\n    Get the HossNative lead source provider.\n    \n    HossNative is the ONLY provider - autonomous discovery with no external APIs.\n    Leads come from SignalNet events, not from traditional API-based lead sources.\n    \"\"\"\n    print(\"[LEADS][STARTUP] HossNative (Autonomous Discovery) active\")\n    print(\"[LEADS][STARTUP] Lead discovery via SignalNet + web scraping\")\n    return HossNativeProvider()\n\n\ndef get_lead_source_status() -> Dict[str, Any]:\n    \"\"\"\n    Get current lead source status for admin display.\n    \n    Returns HossNative status and configuration.\n    \"\"\"\n    config = get_lead_source_config()\n    \n    from release_mode import get_release_mode_status\n    release_status = get_release_mode_status()\n    \n    status = {\n        \"niche\": config.niche,\n        \"geography\": config.geography,\n        \"max_new_leads_per_cycle\": config.max_new_leads_per_cycle,\n        \"provider\": \"HossNative\",\n        \"provider_type\": \"autonomous_discovery\",\n        \"provider_configured\": True,\n        \"provider_ready\": True,\n        \"release_mode\": release_status[\"mode\"],\n        \"release_mode_message\": release_status[\"message\"],\n        \"last_status\": \"active\",\n        \"last_error\": None,\n        \"discovery_method\": \"SignalNet + Web Scraping\",\n        \"external_apis\": \"None - fully autonomous\"\n    }\n    \n    return status\n\n\n_lead_source_log: Dict[str, Any] = {\n    \"runs\": [],\n    \"last_run\": None,\n    \"last_created_count\": 0,\n    \"recent_leads\": []\n}\n\n\ndef log_lead_source_run(created: int, skipped: int, provider: str, error: Optional[str] = None):\n    \"\"\"Log a lead source run for admin visibility.\"\"\"\n    global _lead_source_log\n    \n    run_entry = {\n        \"timestamp\": datetime.utcnow().isoformat(),\n        \"provider\": provider,\n        \"created\": created,\n        \"skipped\": skipped,\n        \"error\": error\n    }\n    \n    _lead_source_log[\"runs\"].append(run_entry)\n    _lead_source_log[\"runs\"] = _lead_source_log[\"runs\"][-50:]\n    _lead_source_log[\"last_run\"] = run_entry[\"timestamp\"]\n    _lead_source_log[\"last_created_count\"] = created\n\n\ndef log_lead_created(lead_name: str, email: str, source: str):\n    \"\"\"Log a newly created lead.\"\"\"\n    global _lead_source_log\n    \n    _lead_source_log[\"recent_leads\"].append({\n        \"name\": lead_name,\n        \"email\": email,\n        \"source\": source,\n        \"created_at\": datetime.utcnow().isoformat()\n    })\n    _lead_source_log[\"recent_leads\"] = _lead_source_log[\"recent_leads\"][-20:]\n\n\ndef get_lead_source_log() -> Dict[str, Any]:\n    \"\"\"Get the lead source run log.\"\"\"\n    return _lead_source_log\n","path":null,"size_bytes":5973,"size_tokens":null},"test_harness.py":{"content":"\"\"\"\nHossAgent End-to-End Test Harness\nTests Stripe + Email + Billing loop without modifying core logic.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport requests\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Tuple\n\nBASE_URL = \"http://localhost:5000\"\n\nclass TestResult:\n    def __init__(self, name: str, passed: bool, details: str = \"\"):\n        self.name = name\n        self.passed = passed\n        self.details = details\n    \n    def __str__(self):\n        status = \"PASS\" if self.passed else \"FAIL\"\n        return f\"[{status}] {self.name}: {self.details}\"\n\nclass HossTestHarness:\n    def __init__(self):\n        self.results: List[TestResult] = []\n        self.stripe_enabled = False\n        self.stripe_api_key = os.environ.get(\"STRIPE_API_KEY\", \"\")\n        self.enable_stripe = os.environ.get(\"ENABLE_STRIPE\", \"\").upper() == \"TRUE\"\n    \n    def log(self, message: str):\n        print(f\"[TEST] {message}\")\n    \n    def add_result(self, name: str, passed: bool, details: str = \"\"):\n        result = TestResult(name, passed, details)\n        self.results.append(result)\n        print(str(result))\n        return result\n    \n    def test_stripe_config(self) -> bool:\n        \"\"\"Test 1: Stripe Configuration Detection\"\"\"\n        self.log(\"=\" * 60)\n        self.log(\"TEST 1: STRIPE CONFIGURATION\")\n        self.log(\"=\" * 60)\n        \n        if not self.enable_stripe:\n            self.add_result(\"Stripe Enable Flag\", False, \"ENABLE_STRIPE != TRUE\")\n            self.stripe_enabled = False\n            return False\n        \n        self.add_result(\"Stripe Enable Flag\", True, \"ENABLE_STRIPE = TRUE\")\n        \n        if not self.stripe_api_key:\n            self.add_result(\"Stripe API Key\", False, \"STRIPE_API_KEY not set in Secrets\")\n            self.stripe_enabled = False\n            return False\n        \n        key_preview = f\"{self.stripe_api_key[:7]}...{self.stripe_api_key[-4:]}\" if len(self.stripe_api_key) > 11 else \"[hidden]\"\n        self.add_result(\"Stripe API Key\", True, f\"Present ({key_preview})\")\n        \n        try:\n            response = requests.get(\n                \"https://api.stripe.com/v1/charges\",\n                params={\"limit\": 1},\n                auth=(self.stripe_api_key, \"\"),\n                timeout=10\n            )\n            if response.status_code == 200:\n                self.add_result(\"Stripe API Connection\", True, \"Authorized - API key valid\")\n                self.stripe_enabled = True\n                return True\n            else:\n                error = response.json().get(\"error\", {}).get(\"message\", \"Unknown error\")\n                self.add_result(\"Stripe API Connection\", False, f\"HTTP {response.status_code}: {error}\")\n                self.stripe_enabled = False\n                return False\n        except Exception as e:\n            self.add_result(\"Stripe API Connection\", False, f\"Connection error: {str(e)}\")\n            self.stripe_enabled = False\n            return False\n    \n    def test_stripe_status_endpoint(self) -> bool:\n        \"\"\"Check internal Stripe status endpoint\"\"\"\n        try:\n            response = requests.get(f\"{BASE_URL}/api/stripe/status\", timeout=10)\n            if response.status_code == 200:\n                data = response.json()\n                enabled = data.get(\"enabled\", False)\n                api_key_present = data.get(\"api_key_present\", False)\n                self.add_result(\"Stripe Status Endpoint\", True, \n                    f\"enabled={enabled}, api_key_present={api_key_present}\")\n                return True\n            else:\n                self.add_result(\"Stripe Status Endpoint\", False, f\"HTTP {response.status_code}\")\n                return False\n        except Exception as e:\n            self.add_result(\"Stripe Status Endpoint\", False, str(e))\n            return False\n    \n    def test_payment_link_generation(self) -> Tuple[bool, List[Dict]]:\n        \"\"\"Test 2: Payment Link Generation\"\"\"\n        self.log(\"=\" * 60)\n        self.log(\"TEST 2: PAYMENT LINK GENERATION\")\n        self.log(\"=\" * 60)\n        \n        try:\n            response = requests.get(f\"{BASE_URL}/api/invoices\", timeout=10)\n            if response.status_code != 200:\n                self.add_result(\"Fetch Invoices\", False, f\"HTTP {response.status_code}\")\n                return False, []\n            \n            invoices = response.json()\n            if not invoices:\n                self.add_result(\"Invoice Count\", True, \"No invoices in system (OK for fresh install)\")\n                return True, []\n            \n            self.add_result(\"Invoice Count\", True, f\"{len(invoices)} invoices found\")\n            \n            invoice_results = []\n            all_pass = True\n            \n            print(\"\\n| Invoice ID | Status  | Expected Link? | Found Link | Result |\")\n            print(\"|------------|---------|----------------|------------|--------|\")\n            \n            for inv in invoices:\n                inv_id = inv.get(\"id\")\n                status = inv.get(\"status\", \"unknown\")\n                payment_url = inv.get(\"payment_url\")\n                \n                expected_link = self.stripe_enabled and status != \"paid\"\n                has_link = bool(payment_url)\n                \n                if status == \"paid\":\n                    result = \"PASS\"\n                elif expected_link and has_link:\n                    result = \"PASS\"\n                elif not expected_link and not has_link:\n                    result = \"PASS\"\n                else:\n                    result = \"FAIL\"\n                    all_pass = False\n                \n                link_preview = payment_url[:30] + \"...\" if payment_url and len(payment_url) > 30 else (payment_url or \"None\")\n                print(f\"| {inv_id:10} | {status:7} | {'Yes' if expected_link else 'No':14} | {link_preview:10} | {result:6} |\")\n                \n                invoice_results.append({\n                    \"id\": inv_id,\n                    \"status\": status,\n                    \"expected_link\": expected_link,\n                    \"has_link\": has_link,\n                    \"result\": result\n                })\n            \n            self.add_result(\"Payment Link Generation\", all_pass, \n                f\"{sum(1 for r in invoice_results if r['result'] == 'PASS')}/{len(invoice_results)} invoices OK\")\n            \n            return all_pass, invoice_results\n            \n        except Exception as e:\n            self.add_result(\"Payment Link Generation\", False, str(e))\n            return False, []\n    \n    def test_customer_portals(self) -> Tuple[bool, List[Dict]]:\n        \"\"\"Test 3: Customer Portal Access\"\"\"\n        self.log(\"=\" * 60)\n        self.log(\"TEST 3: CUSTOMER PORTAL ACCESS\")\n        self.log(\"=\" * 60)\n        \n        try:\n            response = requests.get(f\"{BASE_URL}/api/customers\", timeout=10)\n            if response.status_code != 200:\n                self.add_result(\"Fetch Customers\", False, f\"HTTP {response.status_code}\")\n                return False, []\n            \n            customers = response.json()\n            if not customers:\n                self.add_result(\"Customer Count\", True, \"No customers in system (OK for fresh install)\")\n                return True, []\n            \n            self.add_result(\"Customer Count\", True, f\"{len(customers)} customers found\")\n            \n            portal_results = []\n            all_pass = True\n            \n            print(\"\\n| Customer ID | Name                | Portal Token       | HTTP | Content Check | Result |\")\n            print(\"|-------------|---------------------|--------------------|------|---------------|--------|\")\n            \n            for cust in customers:\n                cust_id = cust.get(\"id\")\n                name = cust.get(\"name\", \"Unknown\")[:20]\n                token = cust.get(\"public_token\", \"\")\n                \n                if not token:\n                    print(f\"| {cust_id:11} | {name:19} | {'No token':18} | N/A  | N/A           | SKIP   |\")\n                    continue\n                \n                try:\n                    portal_response = requests.get(f\"{BASE_URL}/portal/{token}\", timeout=10)\n                    http_status = portal_response.status_code\n                    \n                    if http_status == 200:\n                        html = portal_response.text\n                        has_invoices = \"Outstanding Invoices\" in html or \"Payment History\" in html\n                        has_tasks = \"Recent Work\" in html\n                        content_ok = has_invoices or has_tasks\n                        \n                        if content_ok:\n                            result = \"PASS\"\n                        else:\n                            result = \"WARN\"\n                            all_pass = False\n                    else:\n                        content_ok = False\n                        result = \"FAIL\"\n                        all_pass = False\n                    \n                    token_preview = token[:15] + \"...\" if len(token) > 15 else token\n                    print(f\"| {cust_id:11} | {name:19} | {token_preview:18} | {http_status:4} | {'Yes' if content_ok else 'No':13} | {result:6} |\")\n                    \n                    portal_results.append({\n                        \"id\": cust_id,\n                        \"name\": name,\n                        \"http_status\": http_status,\n                        \"content_ok\": content_ok,\n                        \"result\": result\n                    })\n                    \n                except Exception as e:\n                    print(f\"| {cust_id:11} | {name:19} | {token[:15]:18} | ERR  | {str(e)[:13]} | FAIL   |\")\n                    all_pass = False\n            \n            self.add_result(\"Customer Portal Access\", all_pass,\n                f\"{sum(1 for r in portal_results if r['result'] == 'PASS')}/{len(portal_results)} portals OK\")\n            \n            return all_pass, portal_results\n            \n        except Exception as e:\n            self.add_result(\"Customer Portal Access\", False, str(e))\n            return False, []\n    \n    def test_email_system(self) -> bool:\n        \"\"\"Test 4: Email System Health\"\"\"\n        self.log(\"=\" * 60)\n        self.log(\"TEST 4: EMAIL SYSTEM\")\n        self.log(\"=\" * 60)\n        \n        try:\n            settings_response = requests.get(f\"{BASE_URL}/api/settings\", timeout=10)\n            if settings_response.status_code == 200:\n                settings = settings_response.json()\n                email_mode = settings.get(\"email_mode\", \"UNKNOWN\")\n                self.add_result(\"Email Mode Detection\", True, f\"Mode: {email_mode}\")\n            else:\n                self.add_result(\"Email Mode Detection\", False, f\"HTTP {settings_response.status_code}\")\n        except Exception as e:\n            self.add_result(\"Email Mode Detection\", False, str(e))\n        \n        try:\n            test_response = requests.post(\n                f\"{BASE_URL}/admin/send-test-email\",\n                params={\"to_email\": \"test@hoss.com\"},\n                timeout=10\n            )\n            if test_response.status_code == 200:\n                result = test_response.json()\n                success = result.get(\"success\", False)\n                mode = result.get(\"mode\", \"unknown\")\n                self.add_result(\"Test Email Endpoint\", success, f\"Response: {json.dumps(result)}\")\n                return success\n            else:\n                self.add_result(\"Test Email Endpoint\", False, f\"HTTP {test_response.status_code}\")\n                return False\n        except Exception as e:\n            self.add_result(\"Test Email Endpoint\", False, str(e))\n            return False\n    \n    def test_webhook_simulation(self) -> bool:\n        \"\"\"Test 5: Stripe Webhook Simulation (Internal)\"\"\"\n        self.log(\"=\" * 60)\n        self.log(\"TEST 5: WEBHOOK SIMULATION\")\n        self.log(\"=\" * 60)\n        \n        if not self.stripe_enabled:\n            self.add_result(\"Webhook Simulation\", True, \"SKIPPED - Stripe disabled\")\n            return True\n        \n        try:\n            response = requests.get(f\"{BASE_URL}/api/invoices\", timeout=10)\n            invoices = response.json()\n            \n            unpaid_invoices = [inv for inv in invoices if inv.get(\"status\") != \"paid\" and inv.get(\"payment_url\")]\n            \n            if not unpaid_invoices:\n                self.add_result(\"Webhook Simulation\", True, \"SKIPPED - No unpaid invoices with payment links\")\n                return True\n            \n            test_invoice = unpaid_invoices[0]\n            invoice_id = test_invoice.get(\"id\")\n            amount_cents = test_invoice.get(\"amount_cents\", 0)\n            \n            self.log(f\"Testing webhook with invoice {invoice_id} (amount: ${amount_cents/100:.2f})\")\n            \n            self.add_result(\"Webhook Simulation\", True, \n                f\"Would test invoice {invoice_id} - manual verification required for signature\")\n            \n            return True\n            \n        except Exception as e:\n            self.add_result(\"Webhook Simulation\", False, str(e))\n            return False\n    \n    def test_admin_console(self) -> bool:\n        \"\"\"Test 6: Admin Console Visibility\"\"\"\n        self.log(\"=\" * 60)\n        self.log(\"TEST 6: ADMIN CONSOLE\")\n        self.log(\"=\" * 60)\n        \n        try:\n            response = requests.get(f\"{BASE_URL}/admin\", timeout=10)\n            if response.status_code != 200:\n                self.add_result(\"Admin Console Load\", False, f\"HTTP {response.status_code}\")\n                return False\n            \n            html = response.text\n            \n            has_outbound_panel = \"OUTBOUND\" in html or \"EMAIL\" in html\n            has_stripe_panel = \"STRIPE\" in html\n            has_invoices_table = \"Invoices\" in html or \"INVOICES\" in html\n            \n            self.add_result(\"Admin Console Load\", True, \"HTTP 200\")\n            self.add_result(\"Outbound Email Panel\", has_outbound_panel, \"Present\" if has_outbound_panel else \"Missing\")\n            self.add_result(\"Stripe Panel\", has_stripe_panel, \"Present\" if has_stripe_panel else \"Missing\")\n            self.add_result(\"Invoices Section\", has_invoices_table, \"Present\" if has_invoices_table else \"Missing\")\n            \n            return has_outbound_panel and has_stripe_panel and has_invoices_table\n            \n        except Exception as e:\n            self.add_result(\"Admin Console\", False, str(e))\n            return False\n    \n    def test_invoice_state_transitions(self) -> bool:\n        \"\"\"Test 7: Invoice State Validation\"\"\"\n        self.log(\"=\" * 60)\n        self.log(\"TEST 7: INVOICE STATE TRANSITIONS\")\n        self.log(\"=\" * 60)\n        \n        try:\n            response = requests.get(f\"{BASE_URL}/api/invoices\", timeout=10)\n            if response.status_code != 200:\n                self.add_result(\"Invoice State Check\", False, f\"HTTP {response.status_code}\")\n                return False\n            \n            invoices = response.json()\n            \n            if not invoices:\n                self.add_result(\"Invoice State Check\", True, \"No invoices to validate\")\n                return True\n            \n            status_counts = {}\n            for inv in invoices:\n                status = inv.get(\"status\", \"unknown\")\n                status_counts[status] = status_counts.get(status, 0) + 1\n            \n            status_summary = \", \".join([f\"{k}={v}\" for k, v in status_counts.items()])\n            self.add_result(\"Invoice Status Distribution\", True, status_summary)\n            \n            paid_without_date = 0\n            for inv in invoices:\n                if inv.get(\"status\") == \"paid\" and not inv.get(\"paid_at\"):\n                    paid_without_date += 1\n            \n            if paid_without_date > 0:\n                self.add_result(\"Paid Invoice Timestamps\", False, f\"{paid_without_date} paid invoices missing paid_at\")\n                return False\n            else:\n                self.add_result(\"Paid Invoice Timestamps\", True, \"All paid invoices have timestamps\")\n                return True\n            \n        except Exception as e:\n            self.add_result(\"Invoice State Check\", False, str(e))\n            return False\n    \n    def run_all_tests(self):\n        \"\"\"Run complete test suite and generate final report\"\"\"\n        print(\"\\n\" + \"=\" * 70)\n        print(\"   HOSSAGENT END-TO-END TEST HARNESS\")\n        print(\"   \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n        print(\"=\" * 70 + \"\\n\")\n        \n        stripe_ok = self.test_stripe_config()\n        self.test_stripe_status_endpoint()\n        \n        payment_ok, _ = self.test_payment_link_generation()\n        portal_ok, _ = self.test_customer_portals()\n        email_ok = self.test_email_system()\n        webhook_ok = self.test_webhook_simulation()\n        admin_ok = self.test_admin_console()\n        state_ok = self.test_invoice_state_transitions()\n        \n        print(\"\\n\" + \"=\" * 70)\n        print(\"   FINAL REPORT\")\n        print(\"=\" * 70)\n        \n        passed = sum(1 for r in self.results if r.passed)\n        failed = sum(1 for r in self.results if not r.passed)\n        \n        print(f\"\\nTotal Tests: {len(self.results)}\")\n        print(f\"Passed: {passed}\")\n        print(f\"Failed: {failed}\")\n        \n        print(\"\\n--- Test Summary ---\")\n        print(f\"Stripe Status:           {'OK' if stripe_ok else 'DISABLED/FAIL'}\")\n        print(f\"Email System:            {'OK' if email_ok else 'FAIL'}\")\n        print(f\"Payment Links:           {'OK' if payment_ok else 'FAIL'}\")\n        print(f\"Customer Portals:        {'OK' if portal_ok else 'FAIL'}\")\n        print(f\"Webhook Simulation:      {'OK' if webhook_ok else 'FAIL'}\")\n        print(f\"Admin Console:           {'OK' if admin_ok else 'FAIL'}\")\n        print(f\"Invoice States:          {'OK' if state_ok else 'FAIL'}\")\n        \n        critical_tests = [email_ok, admin_ok, state_ok]\n        stripe_tests = [stripe_ok, payment_ok, webhook_ok] if self.stripe_enabled else [True, True, True]\n        \n        all_critical_pass = all(critical_tests)\n        all_stripe_pass = all(stripe_tests)\n        \n        print(\"\\n\" + \"=\" * 70)\n        if all_critical_pass and all_stripe_pass:\n            print(\"   SYSTEM READY FOR LIVE BILLING: YES\")\n        elif all_critical_pass and not self.stripe_enabled:\n            print(\"   SYSTEM READY FOR LIVE BILLING: NO (Stripe not configured)\")\n        else:\n            print(\"   SYSTEM READY FOR LIVE BILLING: NO (Critical tests failed)\")\n        print(\"=\" * 70 + \"\\n\")\n        \n        if failed > 0:\n            print(\"--- Failed Tests ---\")\n            for r in self.results:\n                if not r.passed:\n                    print(f\"  - {r.name}: {r.details}\")\n        \n        return all_critical_pass\n\n\ndef main():\n    harness = HossTestHarness()\n    success = harness.run_all_tests()\n    sys.exit(0 if success else 1)\n\n\nif __name__ == \"__main__\":\n    main()\n","path":null,"size_bytes":18908,"size_tokens":null},"bizdev_templates.py":{"content":"\"\"\"\nBizDev Template Engine for HossAgent.\nProvides niche-tuned email template packs for outbound prospecting.\n\nEnvironment Variables:\n  BIZDEV_NICHE_TEMPLATE - Template pack to use (default: \"general\")\n  BIZDEV_SENDER_NAME - Sender name in emails (default: \"HossAgent\")\n  BIZDEV_SENDER_EMAIL - Sender email for replies (optional)\n  BIZDEV_OFFER - Current offer/service description (optional)\n\"\"\"\nimport os\nimport random\nimport json\nfrom datetime import datetime\nfrom typing import Dict, Any, Optional, List\nfrom pathlib import Path\nfrom dataclasses import dataclass, asdict\n\n\n@dataclass\nclass GeneratedEmail:\n    subject: str\n    body: str\n    template_pack: str\n    template_index: int\n    placeholders_used: Dict[str, str]\n\n\nTEMPLATE_LOG_FILE = Path(\"bizdev_template_log.json\")\nMAX_TEMPLATE_LOG_ENTRIES = 5000\n\n\nTEMPLATE_PACKS: Dict[str, Dict[str, List[str]]] = {\n    \"general\": {\n        \"subject_lines\": [\n            \"Quick idea for {{company_name}}\",\n            \"Taking the grunt work off your plate\",\n            \"You + 1 autonomous ops brain\",\n            \"Quick idea to de-risk your pipeline\",\n            \"{{company_name}} - one less thing to worry about\"\n        ],\n        \"body_templates\": [\n            \"\"\"Hi {{first_name}},\n\nI've been looking at small shops like {{company_name}} that are doing solid work but still relying on a mess of spreadsheets, email threads, and late-night invoicing to keep cash coming in.\n\nI built something for that: an autonomous \"back office\" that does three things on repeat:\n- Finds and contacts qualified leads for you\n- Tracks what work is being done for whom\n- Generates invoices and shows you, in one dashboard, where the money is\n\nIt's not a CRM and it's not an agency. Think of it as a self-driving ops assistant that only cares about two things: pipeline and cash.\n\nIf you gave it one current offer (e.g., how you usually package {{niche}} work), it could start running a small, controlled experiment for you this month.\n\nWould you be open to a 15-minute call so I can show you what that looks like with real numbers from your world?\n\n- {{sender_name}}\"\"\",\n            \"\"\"Hi {{first_name}},\n\nRunning a {{niche}} business means you're juggling a lot - client work, new leads, invoicing, follow-ups. What if half of that ran itself?\n\nI've built an autonomous system that handles the back-office grind:\n- Prospecting and outreach on autopilot\n- Task tracking with profit calculations\n- Invoice generation when work is done\n\nNo hiring. No learning new tools. Just set your offer and let it run.\n\nWant to see how it would work for {{company_name}}?\n\n- {{sender_name}}\"\"\",\n            \"\"\"{{first_name}},\n\nQuick question: how much time do you spend each week on admin work that isn't billable?\n\nFor most {{niche}} folks I talk to, it's 10-15 hours. That's $2K-$5K in lost revenue every month.\n\nI built a system that handles leads, tasks, and invoicing automatically - so you can focus on the work that actually pays.\n\nWorth a quick look?\n\n- {{sender_name}}\"\"\"\n        ]\n    },\n    \"agency\": {\n        \"subject_lines\": [\n            \"Your agency's invisible back office\",\n            \"Stop losing deals to slow follow-up\",\n            \"{{company_name}} - what if client ops ran itself?\",\n            \"The agency owner's leverage play\",\n            \"Quick idea for {{company_name}}\"\n        ],\n        \"body_templates\": [\n            \"\"\"Hi {{first_name}},\n\nAgency life: you're great at the creative work, but the pipeline management, client onboarding, and invoicing? That's where things fall through the cracks.\n\nI built an autonomous system specifically for this:\n- Finds leads that match your ideal client profile\n- Sends personalized outreach (you approve the templates)\n- Tracks projects and auto-generates invoices\n\nIt's like having a silent ops partner who never sleeps and never forgets to follow up.\n\nMind if I show you how it would work for {{company_name}}?\n\n- {{sender_name}}\"\"\",\n            \"\"\"{{first_name}},\n\nMost agency owners I know are stuck in the feast-or-famine cycle because they only prospect when they're desperate.\n\nWhat if your pipeline stayed full without you thinking about it?\n\nI've built an autonomous engine that handles lead gen, outreach, and follow-ups on autopilot - while you focus on client delivery.\n\n{{company_name}} seems like a good fit. Want to take a look?\n\n- {{sender_name}}\"\"\"\n        ]\n    },\n    \"saas\": {\n        \"subject_lines\": [\n            \"{{company_name}} - ops that scale with you\",\n            \"Your SaaS deserves automated back-office\",\n            \"Quick automation idea for {{company_name}}\",\n            \"From manual to autonomous operations\"\n        ],\n        \"body_templates\": [\n            \"\"\"Hi {{first_name}},\n\nBuilding a SaaS is hard enough without manually chasing leads and invoicing customers.\n\nI've built an autonomous system that handles the operational side:\n- Lead discovery and qualification\n- Automated outreach sequences\n- Revenue tracking and invoice generation\n\nIt integrates with what you already use and runs 24/7.\n\nWorth 15 minutes to see if it fits {{company_name}}?\n\n- {{sender_name}}\"\"\",\n            \"\"\"{{first_name}},\n\nWhen you're scaling a SaaS, every hour spent on admin work is an hour not spent on product or customers.\n\nMy system automates the repetitive stuff:\n- Finding and contacting potential customers\n- Tracking deals and tasks\n- Billing and revenue reporting\n\nIt's designed to run autonomously - you set the parameters, it does the work.\n\nInterested in a quick demo for {{company_name}}?\n\n- {{sender_name}}\"\"\"\n        ]\n    },\n    \"consulting\": {\n        \"subject_lines\": [\n            \"{{company_name}} - your invisible associate\",\n            \"Consultants who hate admin work, read this\",\n            \"Quick idea for {{first_name}}\",\n            \"What if your practice ran itself?\"\n        ],\n        \"body_templates\": [\n            \"\"\"Hi {{first_name}},\n\nMost consultants I know are brilliant at their craft but drowning in the business side - finding clients, sending proposals, tracking hours, chasing payments.\n\nI built an autonomous system that handles all of that:\n- Finds and reaches out to potential clients\n- Tracks engagements and deliverables\n- Generates invoices automatically\n\nYou focus on the consulting. The system handles the business of consulting.\n\nWould you be open to a quick walkthrough for {{company_name}}?\n\n- {{sender_name}}\"\"\",\n            \"\"\"{{first_name}},\n\nHere's the consulting paradox: the more successful you get, the less time you have to find new clients.\n\nMy system breaks that cycle by automating your pipeline:\n- Identifies prospects in your niche\n- Sends personalized outreach\n- Tracks responses and schedules follow-ups\n\nIt's like having a full-time business development person, minus the salary.\n\nInterested?\n\n- {{sender_name}}\"\"\"\n        ]\n    },\n    \"revops\": {\n        \"subject_lines\": [\n            \"{{company_name}} - revenue on autopilot\",\n            \"Your RevOps engine, fully autonomous\",\n            \"Quick revenue idea for {{first_name}}\",\n            \"From RevOps to RevAuto\"\n        ],\n        \"body_templates\": [\n            \"\"\"Hi {{first_name}},\n\nYou know better than most: revenue operations is about removing friction from the money flow.\n\nI built an autonomous system that does exactly that:\n- Pipeline generation (finds and contacts leads automatically)\n- Work tracking with cost/profit analysis\n- Automated invoicing and revenue reporting\n\nNo more spreadsheet gymnastics. No more manual follow-ups.\n\nWorth a look for {{company_name}}?\n\n- {{sender_name}}\"\"\",\n            \"\"\"{{first_name}},\n\nWhat if your entire revenue operation - from lead to invoice - ran autonomously?\n\nThat's what I built:\n- Lead sourcing and outreach on autopilot\n- Task and project management with real-time P&L\n- Invoice generation triggered by work completion\n\nIt's RevOps without the ops.\n\nMind if I show you how it would work for {{company_name}}?\n\n- {{sender_name}}\"\"\"\n        ]\n    }\n}\n\n\ndef get_template_pack_name() -> str:\n    \"\"\"Get the configured template pack name from environment.\"\"\"\n    return os.getenv(\"BIZDEV_NICHE_TEMPLATE\", \"general\").lower()\n\n\ndef get_sender_name() -> str:\n    \"\"\"Get sender name from environment.\"\"\"\n    return os.getenv(\"BIZDEV_SENDER_NAME\", \"HossAgent\")\n\n\ndef get_sender_email() -> Optional[str]:\n    \"\"\"Get sender email from environment (optional).\"\"\"\n    return os.getenv(\"BIZDEV_SENDER_EMAIL\")\n\n\ndef get_offer_description() -> str:\n    \"\"\"Get current offer description from environment.\"\"\"\n    return os.getenv(\"BIZDEV_OFFER\", \"autonomous business operations\")\n\n\ndef get_dashboard_url() -> str:\n    \"\"\"Get dashboard URL from environment.\"\"\"\n    base = os.getenv(\"REPLIT_DEV_DOMAIN\", \"\")\n    if base:\n        return f\"https://{base}\"\n    return os.getenv(\"DASHBOARD_URL\", \"\")\n\n\ndef list_template_packs() -> List[str]:\n    \"\"\"List all available template pack names.\"\"\"\n    return list(TEMPLATE_PACKS.keys())\n\n\ndef get_template_pack(name: str) -> Optional[Dict[str, List[str]]]:\n    \"\"\"Get a specific template pack by name.\"\"\"\n    return TEMPLATE_PACKS.get(name.lower())\n\n\ndef _load_template_log() -> List[Dict[str, Any]]:\n    \"\"\"Load template generation log.\"\"\"\n    try:\n        if TEMPLATE_LOG_FILE.exists():\n            with open(TEMPLATE_LOG_FILE, \"r\") as f:\n                return json.load(f)\n    except Exception:\n        pass\n    return []\n\n\ndef _save_template_log(entries: List[Dict[str, Any]]) -> None:\n    \"\"\"Save template generation log.\"\"\"\n    try:\n        entries = entries[-MAX_TEMPLATE_LOG_ENTRIES:]\n        with open(TEMPLATE_LOG_FILE, \"w\") as f:\n            json.dump(entries, f, indent=2)\n    except Exception as e:\n        print(f\"[BIZDEV] Warning: Could not save template log: {e}\")\n\n\ndef log_template_generation(email: GeneratedEmail, lead_id: int, lead_email: str) -> None:\n    \"\"\"Log a generated email for admin visibility.\"\"\"\n    entries = _load_template_log()\n    entries.append({\n        \"timestamp\": datetime.utcnow().isoformat(),\n        \"lead_id\": lead_id,\n        \"lead_email\": lead_email,\n        \"template_pack\": email.template_pack,\n        \"template_index\": email.template_index,\n        \"subject\": email.subject,\n        \"body_preview\": email.body[:200] + \"...\" if len(email.body) > 200 else email.body\n    })\n    _save_template_log(entries)\n\n\ndef get_template_log(limit: int = 10) -> List[Dict[str, Any]]:\n    \"\"\"Get recent template generations for admin display.\"\"\"\n    entries = _load_template_log()\n    return entries[-limit:]\n\n\ndef generate_email(\n    first_name: str,\n    company_name: str,\n    niche: str = \"\",\n    email: str = \"\",\n    industry: str = \"\"\n) -> GeneratedEmail:\n    \"\"\"\n    Generate a personalized email using the configured template pack.\n    \n    Args:\n        first_name: Contact's first name\n        company_name: Company name\n        niche: Lead's niche/industry (optional)\n        email: Lead's email (for logging)\n        industry: Industry category (optional)\n    \n    Returns:\n        GeneratedEmail with filled-in subject and body\n    \"\"\"\n    pack_name = get_template_pack_name()\n    pack = get_template_pack(pack_name)\n    \n    if not pack:\n        print(f\"[BIZDEV][TEMPLATE] Pack '{pack_name}' not found, using 'general'\")\n        pack_name = \"general\"\n        pack = TEMPLATE_PACKS[\"general\"]\n    \n    subject_template = random.choice(pack[\"subject_lines\"])\n    body_index = random.randint(0, len(pack[\"body_templates\"]) - 1)\n    body_template = pack[\"body_templates\"][body_index]\n    \n    placeholders = {\n        \"first_name\": first_name or \"there\",\n        \"company_name\": company_name or \"your company\",\n        \"niche\": niche or industry or \"your industry\",\n        \"industry\": industry or niche or \"your industry\",\n        \"offer\": get_offer_description(),\n        \"sender_name\": get_sender_name(),\n        \"sender_email\": get_sender_email() or \"\",\n        \"dashboard_url\": get_dashboard_url()\n    }\n    \n    subject = subject_template\n    body = body_template\n    \n    for key, value in placeholders.items():\n        subject = subject.replace(f\"{{{{{key}}}}}\", value)\n        body = body.replace(f\"{{{{{key}}}}}\", value)\n    \n    return GeneratedEmail(\n        subject=subject,\n        body=body,\n        template_pack=pack_name,\n        template_index=body_index,\n        placeholders_used=placeholders\n    )\n\n\ndef get_template_status() -> Dict[str, Any]:\n    \"\"\"Get current template configuration status for admin display.\"\"\"\n    pack_name = get_template_pack_name()\n    pack = get_template_pack(pack_name)\n    \n    return {\n        \"active_pack\": pack_name,\n        \"pack_exists\": pack is not None,\n        \"available_packs\": list_template_packs(),\n        \"sender_name\": get_sender_name(),\n        \"sender_email\": get_sender_email(),\n        \"offer\": get_offer_description(),\n        \"subject_count\": len(pack[\"subject_lines\"]) if pack else 0,\n        \"body_count\": len(pack[\"body_templates\"]) if pack else 0\n    }\n","path":null,"size_bytes":12912,"size_tokens":null},"stripe_utils.py":{"content":"\"\"\"\nStripe Billing Integration for HossAgent.\nHandles payment link creation and webhook processing.\n\nEnvironment Variables:\n  ENABLE_STRIPE = TRUE/FALSE (default: FALSE)\n  STRIPE_API_KEY - Stripe secret key (sk_...)\n  STRIPE_WEBHOOK_SECRET - Webhook signing secret (whsec_...)\n  STRIPE_DEFAULT_CURRENCY - Currency code (default: usd)\n\nSafety:\n  - Falls back to DRY_RUN if credentials missing\n  - Invoice amount safety clamp: $1-$500 by default\n  - All errors are caught and logged without crashing\n\"\"\"\nimport os\nimport json\nimport hmac\nimport hashlib\nfrom datetime import datetime\nfrom typing import Dict, Any, Optional, Tuple\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\n\n@dataclass\nclass PaymentLinkResult:\n    success: bool\n    payment_url: Optional[str]\n    stripe_id: Optional[str]\n    error: Optional[str]\n    mode: str  # \"stripe\" or \"dry_run\"\n\n\nSTRIPE_LOG_FILE = Path(\"stripe_events.json\")\nMAX_STRIPE_LOG_ENTRIES = 5000  # Capped for log rotation\n\n\ndef get_min_invoice_cents() -> int:\n    \"\"\"Get minimum invoice amount from env or default ($1.00).\"\"\"\n    try:\n        return int(os.getenv(\"STRIPE_MIN_AMOUNT_CENTS\", \"100\"))\n    except ValueError:\n        return 100\n\n\ndef get_max_invoice_cents() -> int:\n    \"\"\"Get maximum invoice amount from env or default ($500.00).\"\"\"\n    try:\n        return int(os.getenv(\"STRIPE_MAX_AMOUNT_CENTS\", \"50000\"))\n    except ValueError:\n        return 50000\n\n\ndef is_stripe_enabled() -> bool:\n    \"\"\"Check if Stripe is enabled via environment variable.\"\"\"\n    return os.getenv(\"ENABLE_STRIPE\", \"FALSE\").upper() == \"TRUE\"\n\n\ndef get_stripe_api_key() -> Optional[str]:\n    \"\"\"Get Stripe API key from environment.\"\"\"\n    return os.getenv(\"STRIPE_API_KEY\")\n\n\ndef get_stripe_webhook_secret() -> Optional[str]:\n    \"\"\"Get Stripe webhook secret from environment.\"\"\"\n    return os.getenv(\"STRIPE_WEBHOOK_SECRET\")\n\n\ndef get_default_currency() -> str:\n    \"\"\"Get default currency from environment.\"\"\"\n    return os.getenv(\"STRIPE_DEFAULT_CURRENCY\", \"usd\").lower()\n\n\ndef validate_stripe_config() -> Tuple[bool, str]:\n    \"\"\"\n    Validate Stripe configuration.\n    \n    Returns:\n        (is_valid, message)\n    \"\"\"\n    if not is_stripe_enabled():\n        return False, \"Stripe disabled (ENABLE_STRIPE != TRUE)\"\n    \n    api_key = get_stripe_api_key()\n    if not api_key:\n        return False, \"STRIPE_API_KEY not set\"\n    \n    if not api_key.startswith(\"sk_\"):\n        return False, \"STRIPE_API_KEY should start with 'sk_'\"\n    \n    return True, \"Stripe configured\"\n\n\ndef _load_stripe_log() -> list:\n    \"\"\"Load Stripe event log.\"\"\"\n    try:\n        if STRIPE_LOG_FILE.exists():\n            with open(STRIPE_LOG_FILE, \"r\") as f:\n                return json.load(f)\n    except Exception:\n        pass\n    return []\n\n\ndef _save_stripe_log(entries: list) -> None:\n    \"\"\"Save Stripe event log.\"\"\"\n    try:\n        entries = entries[-MAX_STRIPE_LOG_ENTRIES:]\n        with open(STRIPE_LOG_FILE, \"w\") as f:\n            json.dump(entries, f, indent=2)\n    except Exception as e:\n        print(f\"[STRIPE] Warning: Could not save event log: {e}\")\n\n\ndef log_stripe_event(event_type: str, data: Dict[str, Any]) -> None:\n    \"\"\"Log a Stripe event for admin visibility.\"\"\"\n    entries = _load_stripe_log()\n    entries.append({\n        \"timestamp\": datetime.utcnow().isoformat(),\n        \"event_type\": event_type,\n        \"data\": data\n    })\n    _save_stripe_log(entries)\n\n\ndef get_stripe_log(limit: int = 20) -> list:\n    \"\"\"Get recent Stripe events for admin display.\"\"\"\n    entries = _load_stripe_log()\n    return entries[-limit:]\n\n\ndef check_invoice_amount(amount_cents: int) -> Tuple[bool, str]:\n    \"\"\"\n    Check if invoice amount is within safety bounds.\n    \n    Uses env variables STRIPE_MIN_AMOUNT_CENTS and STRIPE_MAX_AMOUNT_CENTS\n    for configurable limits. Defaults to $1.00 - $500.00.\n    \n    Returns:\n        (is_valid, message)\n    \"\"\"\n    min_cents = get_min_invoice_cents()\n    max_cents = get_max_invoice_cents()\n    \n    if amount_cents < min_cents:\n        msg = f\"Amount ${amount_cents/100:.2f} below minimum ${min_cents/100:.2f}\"\n        print(f\"[STRIPE][AMOUNT_OUT_OF_RANGE] {msg}\")\n        return False, msg\n    \n    if amount_cents > max_cents:\n        msg = f\"Amount ${amount_cents/100:.2f} above maximum ${max_cents/100:.2f}\"\n        print(f\"[STRIPE][AMOUNT_OUT_OF_RANGE] {msg}\")\n        return False, msg\n    \n    return True, \"Amount within bounds\"\n\n\ndef create_payment_link(\n    amount_cents: int,\n    customer_id: int,\n    customer_email: str,\n    description: str,\n    invoice_id: int\n) -> PaymentLinkResult:\n    \"\"\"\n    Create a Stripe payment link for an invoice.\n    \n    Falls back to DRY_RUN mode if Stripe is not configured.\n    Enforces amount safety bounds.\n    \n    Args:\n        amount_cents: Amount in cents\n        customer_id: HossAgent customer ID\n        customer_email: Customer email for Stripe\n        description: Line item description\n        invoice_id: HossAgent invoice ID\n    \n    Returns:\n        PaymentLinkResult with success status and payment URL\n    \"\"\"\n    is_valid_config, config_msg = validate_stripe_config()\n    \n    if not is_valid_config:\n        print(f\"[STRIPE][DRY_RUN] {config_msg}\")\n        log_stripe_event(\"payment_link_dry_run\", {\n            \"reason\": config_msg,\n            \"invoice_id\": invoice_id,\n            \"amount_cents\": amount_cents\n        })\n        return PaymentLinkResult(\n            success=False,\n            payment_url=None,\n            stripe_id=None,\n            error=f\"DRY_RUN: {config_msg}\",\n            mode=\"dry_run\"\n        )\n    \n    is_valid_amount, amount_msg = check_invoice_amount(amount_cents)\n    if not is_valid_amount:\n        print(f\"[STRIPE][SKIP] {amount_msg}\")\n        log_stripe_event(\"payment_link_skipped\", {\n            \"reason\": amount_msg,\n            \"invoice_id\": invoice_id,\n            \"amount_cents\": amount_cents\n        })\n        return PaymentLinkResult(\n            success=False,\n            payment_url=None,\n            stripe_id=None,\n            error=amount_msg,\n            mode=\"stripe\"\n        )\n    \n    try:\n        import requests\n        \n        api_key = get_stripe_api_key()\n        if not api_key:\n            return PaymentLinkResult(\n                success=False,\n                payment_url=None,\n                stripe_id=None,\n                error=\"STRIPE_API_KEY not set\",\n                mode=\"stripe\"\n            )\n        \n        currency = get_default_currency()\n        \n        price_response = requests.post(\n            \"https://api.stripe.com/v1/prices\",\n            auth=(str(api_key), \"\"),\n            data={\n                \"currency\": currency,\n                \"unit_amount\": amount_cents,\n                \"product_data[name]\": description[:200]\n            },\n            timeout=30\n        )\n        \n        if price_response.status_code != 200:\n            error_msg = f\"Price creation failed: {price_response.text[:200]}\"\n            print(f\"[STRIPE][ERROR] {error_msg}\")\n            log_stripe_event(\"price_creation_failed\", {\n                \"invoice_id\": invoice_id,\n                \"error\": error_msg\n            })\n            return PaymentLinkResult(\n                success=False,\n                payment_url=None,\n                stripe_id=None,\n                error=error_msg,\n                mode=\"stripe\"\n            )\n        \n        price_data = price_response.json()\n        price_id = price_data[\"id\"]\n        \n        link_response = requests.post(\n            \"https://api.stripe.com/v1/payment_links\",\n            auth=(str(api_key), \"\"),\n            data={\n                \"line_items[0][price]\": price_id,\n                \"line_items[0][quantity]\": 1,\n                \"metadata[invoice_id]\": str(invoice_id),\n                \"metadata[customer_id]\": str(customer_id)\n            },\n            timeout=30\n        )\n        \n        if link_response.status_code != 200:\n            error_msg = f\"Payment link creation failed: {link_response.text[:200]}\"\n            print(f\"[STRIPE][ERROR] {error_msg}\")\n            log_stripe_event(\"payment_link_failed\", {\n                \"invoice_id\": invoice_id,\n                \"error\": error_msg\n            })\n            return PaymentLinkResult(\n                success=False,\n                payment_url=None,\n                stripe_id=None,\n                error=error_msg,\n                mode=\"stripe\"\n            )\n        \n        link_data = link_response.json()\n        payment_url = link_data[\"url\"]\n        stripe_id = link_data[\"id\"]\n        \n        print(f\"[STRIPE][PAYMENT_LINK] invoice_id={invoice_id} url={payment_url}\")\n        log_stripe_event(\"payment_link_created\", {\n            \"invoice_id\": invoice_id,\n            \"customer_id\": customer_id,\n            \"amount_cents\": amount_cents,\n            \"payment_url\": payment_url,\n            \"stripe_id\": stripe_id\n        })\n        \n        return PaymentLinkResult(\n            success=True,\n            payment_url=payment_url,\n            stripe_id=stripe_id,\n            error=None,\n            mode=\"stripe\"\n        )\n        \n    except ImportError:\n        error_msg = \"requests library not available\"\n        print(f\"[STRIPE][ERROR] {error_msg}\")\n        return PaymentLinkResult(\n            success=False,\n            payment_url=None,\n            stripe_id=None,\n            error=error_msg,\n            mode=\"stripe\"\n        )\n    except Exception as e:\n        error_msg = str(e)\n        print(f\"[STRIPE][ERROR] Exception: {error_msg}\")\n        log_stripe_event(\"payment_link_exception\", {\n            \"invoice_id\": invoice_id,\n            \"error\": error_msg\n        })\n        return PaymentLinkResult(\n            success=False,\n            payment_url=None,\n            stripe_id=None,\n            error=error_msg,\n            mode=\"stripe\"\n        )\n\n\ndef verify_webhook_signature(payload: bytes, signature: str) -> bool:\n    \"\"\"\n    Verify Stripe webhook signature using raw bytes.\n    \n    Stripe computes HMAC on raw bytes, so we must not decode to UTF-8 before verification.\n    Handles multiple v1 signatures (Stripe may send multiple for key rotation).\n    \n    Args:\n        payload: Raw request body as bytes (NOT decoded)\n        signature: Stripe-Signature header value\n    \n    Returns:\n        True if signature is valid\n    \"\"\"\n    webhook_secret = get_stripe_webhook_secret()\n    if not webhook_secret:\n        print(\"[STRIPE][WEBHOOK] No webhook secret configured - cannot verify\")\n        return False\n    \n    try:\n        parts = {}\n        v1_signatures = []\n        \n        for part in signature.split(\",\"):\n            if \"=\" not in part:\n                continue\n            key, value = part.split(\"=\", 1)\n            key = key.strip()\n            value = value.strip()\n            if key == \"t\":\n                parts[\"t\"] = value\n            elif key == \"v1\":\n                v1_signatures.append(value)\n        \n        timestamp = parts.get(\"t\")\n        if not timestamp or not v1_signatures:\n            print(\"[STRIPE][WEBHOOK] Missing timestamp or v1 signature in header\")\n            return False\n        \n        signed_payload_bytes = f\"{timestamp}.\".encode('utf-8') + payload\n        expected_signature = hmac.new(\n            webhook_secret.encode('utf-8'),\n            signed_payload_bytes,\n            hashlib.sha256\n        ).hexdigest()\n        \n        for v1_sig in v1_signatures:\n            if hmac.compare_digest(expected_signature, v1_sig):\n                return True\n        \n        print(\"[STRIPE][WEBHOOK] Signature mismatch - none of the v1 signatures matched\")\n        return False\n        \n    except Exception as e:\n        print(f\"[STRIPE][WEBHOOK] Signature verification error: {e}\")\n        return False\n\n\ndef get_stripe_status() -> Dict[str, Any]:\n    \"\"\"Get current Stripe configuration status for admin display.\"\"\"\n    is_enabled = is_stripe_enabled()\n    is_valid, message = validate_stripe_config()\n    api_key = get_stripe_api_key()\n    webhook_secret = get_stripe_webhook_secret()\n    min_cents = get_min_invoice_cents()\n    max_cents = get_max_invoice_cents()\n    \n    recent_log = get_stripe_log(5)\n    last_webhook_event = None\n    last_error = None\n    for entry in reversed(recent_log):\n        if \"webhook\" in entry.get(\"event_type\", \"\"):\n            last_webhook_event = entry.get(\"timestamp\")\n            break\n        if \"error\" in entry.get(\"event_type\", \"\") or \"failed\" in entry.get(\"event_type\", \"\"):\n            if not last_error:\n                last_error = entry.get(\"data\", {}).get(\"error\")\n    \n    return {\n        \"enabled\": is_enabled,\n        \"configured\": is_valid,\n        \"message\": message,\n        \"currency\": get_default_currency(),\n        \"api_key_present\": api_key is not None and len(api_key) > 0,\n        \"webhook_configured\": webhook_secret is not None and len(webhook_secret) > 0,\n        \"min_amount\": f\"${min_cents/100:.2f}\",\n        \"max_amount\": f\"${max_cents/100:.2f}\",\n        \"last_webhook_event\": last_webhook_event,\n        \"last_error\": last_error\n    }\n\n\ndef validate_stripe_at_startup() -> None:\n    \"\"\"\n    Validate Stripe configuration at startup and print status banner.\n    Called from main.py during app initialization.\n    \n    Logs presence/absence of keys without exposing values.\n    Warns if webhook secret is missing (important for payment verification).\n    \"\"\"\n    is_enabled = is_stripe_enabled()\n    api_key = get_stripe_api_key()\n    webhook_secret = get_stripe_webhook_secret()\n    \n    api_key_present = api_key is not None and len(api_key) > 0\n    webhook_secret_present = webhook_secret is not None and len(webhook_secret) > 0\n    \n    if is_enabled:\n        if api_key_present:\n            print(f\"[STRIPE][STARTUP] Stripe ENABLED - API key present, webhook secret {'present' if webhook_secret_present else 'NOT SET'}\")\n            print(f\"[STRIPE][STARTUP] Currency: {get_default_currency().upper()}, Limits: ${get_min_invoice_cents()/100:.2f}-${get_max_invoice_cents()/100:.2f}\")\n            if not webhook_secret_present:\n                print(f\"[STRIPE][STARTUP][WARNING] STRIPE_WEBHOOK_SECRET not set - webhook events will not be verified!\")\n                print(f\"[STRIPE][STARTUP][WARNING] This is a SECURITY RISK in production. Set STRIPE_WEBHOOK_SECRET in Secrets.\")\n        else:\n            print(f\"[STRIPE][STARTUP][WARNING] ENABLE_STRIPE=TRUE but STRIPE_API_KEY is missing - falling back to DRY_RUN\")\n    else:\n        print(f\"[STRIPE][STARTUP] Stripe disabled (ENABLE_STRIPE != TRUE)\")\n\n\ndef ensure_invoice_payment_url(\n    invoice_id: int,\n    amount_cents: int,\n    customer_id: int,\n    customer_email: str,\n    customer_company: str,\n    invoice_status: str,\n    existing_payment_url: Optional[str]\n) -> PaymentLinkResult:\n    \"\"\"\n    Ensure an invoice has a Stripe payment link if Stripe is enabled.\n    \n    Used for retroactive payment link generation on existing invoices.\n    \n    Conditions to skip:\n    - ENABLE_STRIPE is FALSE\n    - Stripe credentials are invalid\n    - Invoice status is 'paid'\n    - Invoice already has a payment_url\n    \n    Args:\n        invoice_id: HossAgent invoice ID\n        amount_cents: Invoice amount in cents\n        customer_id: Customer ID\n        customer_email: Customer email for Stripe\n        customer_company: Customer company name for description\n        invoice_status: Current invoice status\n        existing_payment_url: Current payment_url (if any)\n    \n    Returns:\n        PaymentLinkResult with success status and payment URL\n    \"\"\"\n    if not is_stripe_enabled():\n        return PaymentLinkResult(\n            success=False,\n            payment_url=None,\n            stripe_id=None,\n            error=\"Stripe disabled\",\n            mode=\"dry_run\"\n        )\n    \n    is_valid, config_msg = validate_stripe_config()\n    if not is_valid:\n        print(f\"[STRIPE][DRY_RUN_FALLBACK] {config_msg}\")\n        log_stripe_event(\"ensure_link_dry_run\", {\n            \"reason\": config_msg,\n            \"invoice_id\": invoice_id\n        })\n        return PaymentLinkResult(\n            success=False,\n            payment_url=None,\n            stripe_id=None,\n            error=f\"DRY_RUN_FALLBACK: {config_msg}\",\n            mode=\"dry_run\"\n        )\n    \n    if invoice_status == \"paid\":\n        return PaymentLinkResult(\n            success=False,\n            payment_url=None,\n            stripe_id=None,\n            error=\"Invoice already paid\",\n            mode=\"stripe\"\n        )\n    \n    if existing_payment_url and len(existing_payment_url) > 10:\n        return PaymentLinkResult(\n            success=False,\n            payment_url=existing_payment_url,\n            stripe_id=None,\n            error=\"Payment URL already exists\",\n            mode=\"stripe\"\n        )\n    \n    result = create_payment_link(\n        amount_cents=amount_cents,\n        customer_id=customer_id,\n        customer_email=customer_email,\n        description=f\"Invoice #{invoice_id} - {customer_company}\",\n        invoice_id=invoice_id\n    )\n    \n    if result.success:\n        print(f\"[STRIPE][LINK_CREATED] Invoice {invoice_id} amount=${amount_cents/100:.2f}\")\n        log_stripe_event(\"retroactive_link_created\", {\n            \"invoice_id\": invoice_id,\n            \"amount_cents\": amount_cents,\n            \"payment_url\": result.payment_url\n        })\n    \n    return result\n\n\ndef get_invoice_payment_stats(invoices: list) -> Dict[str, Any]:\n    \"\"\"\n    Get payment link statistics for a list of invoices.\n    \n    Args:\n        invoices: List of invoice objects with payment_url and status attributes\n    \n    Returns:\n        Dict with counts of invoices with/without payment links\n    \"\"\"\n    total = len(invoices)\n    with_payment_url = 0\n    without_payment_url = 0\n    paid = 0\n    unpaid_without_link = 0\n    \n    for inv in invoices:\n        status = getattr(inv, 'status', 'draft')\n        payment_url = getattr(inv, 'payment_url', None)\n        \n        if status == 'paid':\n            paid += 1\n        elif payment_url and len(payment_url) > 10:\n            with_payment_url += 1\n        else:\n            without_payment_url += 1\n            if status in ('draft', 'sent'):\n                unpaid_without_link += 1\n    \n    return {\n        \"total\": total,\n        \"paid\": paid,\n        \"with_payment_url\": with_payment_url,\n        \"without_payment_url\": without_payment_url,\n        \"unpaid_without_link\": unpaid_without_link\n    }\n\n\ndef get_stripe_payment_mode_status() -> Dict[str, Any]:\n    \"\"\"\n    Get comprehensive Stripe payment mode status for templates.\n    \n    Returns:\n        Dict with:\n        - payments_enabled: True if Stripe is fully configured\n        - payments_available: True if payments can be processed\n        - status_message: Human-readable status message\n        - show_pay_buttons: Whether to show PAY NOW buttons\n    \"\"\"\n    is_enabled = is_stripe_enabled()\n    is_valid, message = validate_stripe_config()\n    \n    if not is_enabled:\n        return {\n            \"payments_enabled\": False,\n            \"payments_available\": False,\n            \"status_message\": \"Online payments are currently disabled. Contact your operator for payment options.\",\n            \"show_pay_buttons\": False\n        }\n    \n    if not is_valid:\n        return {\n            \"payments_enabled\": True,\n            \"payments_available\": False,\n            \"status_message\": \"Online payments temporarily unavailable.\",\n            \"show_pay_buttons\": False\n        }\n    \n    return {\n        \"payments_enabled\": True,\n        \"payments_available\": True,\n        \"status_message\": \"\",\n        \"show_pay_buttons\": True\n    }\n\n\n@dataclass\nclass SubscriptionWebhookResult:\n    \"\"\"Result of processing a subscription webhook.\"\"\"\n    success: bool\n    action: str  # subscription_activated, subscription_updated, subscription_canceled, payment_succeeded, unknown\n    customer_id: Optional[int]\n    subscription_id: Optional[str]\n    new_status: Optional[str]\n    error: Optional[str]\n\n\ndef process_subscription_webhook(event_type: str, event_data: Dict[str, Any]) -> SubscriptionWebhookResult:\n    \"\"\"\n    Process a Stripe subscription webhook event.\n    \n    Handles:\n    - checkout.session.completed -> subscription activation via Checkout\n    - invoice.payment_succeeded -> subscription_status = \"active\"\n    - customer.subscription.updated -> sync status\n    - customer.subscription.deleted -> subscription_status = \"canceled\", plan = \"trial_expired\"\n    \n    Args:\n        event_type: Stripe event type\n        event_data: Stripe event data object\n    \n    Returns:\n        SubscriptionWebhookResult with action taken\n    \"\"\"\n    try:\n        if event_type == \"checkout.session.completed\":\n            mode = event_data.get(\"mode\")\n            if mode != \"subscription\":\n                return SubscriptionWebhookResult(\n                    success=True,\n                    action=\"unknown\",\n                    customer_id=None,\n                    subscription_id=None,\n                    new_status=None,\n                    error=\"Checkout session is not for subscription\"\n                )\n            \n            subscription_id = event_data.get(\"subscription\")\n            customer_stripe_id = event_data.get(\"customer\")\n            metadata = event_data.get(\"metadata\", {})\n            hossagent_customer_id = metadata.get(\"hossagent_customer_id\")\n            \n            log_stripe_event(\"checkout_session_completed\", {\n                \"subscription_id\": subscription_id,\n                \"stripe_customer_id\": customer_stripe_id,\n                \"hossagent_customer_id\": hossagent_customer_id\n            })\n            \n            print(f\"[STRIPE][SUBSCRIPTION] Checkout completed for customer {hossagent_customer_id}, subscription ...{subscription_id[-4:] if subscription_id else 'unknown'}\")\n            \n            return SubscriptionWebhookResult(\n                success=True,\n                action=\"subscription_activated\",\n                customer_id=int(hossagent_customer_id) if hossagent_customer_id else None,\n                subscription_id=subscription_id,\n                new_status=\"active\",\n                error=None\n            )\n        \n        elif event_type == \"invoice.payment_succeeded\":\n            subscription_id = event_data.get(\"subscription\")\n            customer_stripe_id = event_data.get(\"customer\")\n            \n            if not subscription_id:\n                return SubscriptionWebhookResult(\n                    success=False,\n                    action=\"unknown\",\n                    customer_id=None,\n                    subscription_id=None,\n                    new_status=None,\n                    error=\"No subscription ID in invoice.payment_succeeded event\"\n                )\n            \n            log_stripe_event(\"subscription_payment_succeeded\", {\n                \"stripe_customer_id\": customer_stripe_id,\n                \"subscription_id\": subscription_id\n            })\n            \n            print(f\"[STRIPE][SUBSCRIPTION] Payment succeeded for subscription ...{subscription_id[-4:]}\")\n            \n            return SubscriptionWebhookResult(\n                success=True,\n                action=\"payment_succeeded\",\n                customer_id=None,\n                subscription_id=subscription_id,\n                new_status=\"active\",\n                error=None\n            )\n        \n        elif event_type == \"customer.subscription.updated\":\n            subscription = event_data\n            subscription_id = subscription.get(\"id\")\n            status = subscription.get(\"status\")  # active, past_due, canceled, etc.\n            customer_stripe_id = subscription.get(\"customer\")\n            metadata = subscription.get(\"metadata\", {})\n            hossagent_customer_id = metadata.get(\"hossagent_customer_id\")\n            \n            log_stripe_event(\"subscription_updated\", {\n                \"subscription_id\": subscription_id,\n                \"status\": status,\n                \"customer_id\": hossagent_customer_id\n            })\n            \n            print(f\"[STRIPE][SUBSCRIPTION] Subscription ...{subscription_id[-4:] if subscription_id else 'unknown'} updated to status: {status}\")\n            \n            return SubscriptionWebhookResult(\n                success=True,\n                action=\"subscription_updated\",\n                customer_id=int(hossagent_customer_id) if hossagent_customer_id else None,\n                subscription_id=subscription_id,\n                new_status=status,\n                error=None\n            )\n        \n        elif event_type == \"customer.subscription.deleted\":\n            subscription = event_data\n            subscription_id = subscription.get(\"id\")\n            customer_stripe_id = subscription.get(\"customer\")\n            metadata = subscription.get(\"metadata\", {})\n            hossagent_customer_id = metadata.get(\"hossagent_customer_id\")\n            \n            log_stripe_event(\"subscription_deleted\", {\n                \"subscription_id\": subscription_id,\n                \"customer_id\": hossagent_customer_id\n            })\n            \n            print(f\"[STRIPE][SUBSCRIPTION] Subscription ...{subscription_id[-4:] if subscription_id else 'unknown'} CANCELED\")\n            \n            return SubscriptionWebhookResult(\n                success=True,\n                action=\"subscription_canceled\",\n                customer_id=int(hossagent_customer_id) if hossagent_customer_id else None,\n                subscription_id=subscription_id,\n                new_status=\"canceled\",\n                error=None\n            )\n        \n        else:\n            return SubscriptionWebhookResult(\n                success=False,\n                action=\"unknown\",\n                customer_id=None,\n                subscription_id=None,\n                new_status=None,\n                error=f\"Unhandled subscription event type: {event_type}\"\n            )\n    \n    except Exception as e:\n        error_msg = str(e)\n        print(f\"[STRIPE][SUBSCRIPTION][ERROR] Error processing webhook: {error_msg}\")\n        log_stripe_event(\"subscription_webhook_error\", {\n            \"event_type\": event_type,\n            \"error\": error_msg\n        })\n        return SubscriptionWebhookResult(\n            success=False,\n            action=\"error\",\n            customer_id=None,\n            subscription_id=None,\n            new_status=None,\n            error=error_msg\n        )\n","path":null,"size_bytes":26350,"size_tokens":null},"main.py":{"content":"\"\"\"\nHossAgent: Autonomous AI Business Engine\nFastAPI backend with autopilot-driven autonomous agents.\n\nRoutes:\n- /                     Customer Dashboard (public-facing read-only)\n- /admin                Admin Console (operator controls + autopilot toggle)\n- /portal/<token>       Customer Portal (client self-service)\n- /api/leads            List all leads (GET)\n- /api/customers        List all customers (GET)\n- /api/tasks            List all tasks (GET)\n- /api/invoices         List all invoices (GET)\n- /api/run/*            Admin endpoints to manually trigger agent cycles\n- /admin/summary        Daily/weekly summary for operators\n- /admin/send-test-email  Test email configuration\n- /stripe/webhook       Stripe payment webhook\n- /stripe/subscription-webhook  Stripe subscription webhook\n- /upgrade              Upgrade customer to paid plan\n- /api/subscription/status  Get subscription configuration status\n\nSubscription Model:\n- Trial: 7 days, 15 tasks, 20 leads, DRY_RUN email, no billing\n- Paid: $99/month, unlimited access\n\"\"\"\nimport asyncio\nimport json\nimport os\nimport secrets\nimport time\nfrom datetime import datetime, timedelta\nfrom zoneinfo import ZoneInfo\nfrom pathlib import Path\nfrom typing import Optional\nfrom fastapi import FastAPI, Depends, Request, HTTPException, Query, Form, Response\nfrom fastapi.responses import HTMLResponse, RedirectResponse, JSONResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom sqlmodel import Session, select, func\nfrom database import create_db_and_tables, get_session, engine\nfrom models import (\n    Lead, Customer, Task, Invoice, SystemSettings, TrialIdentity, \n    Signal, LeadEvent, PasswordResetToken, PendingOutbound, BusinessProfile, Report,\n    Thread, Message, Suppression, ConversationMetrics,\n    THREAD_STATUS_OPEN, THREAD_STATUS_HUMAN_OWNED, THREAD_STATUS_AUTO, THREAD_STATUS_CLOSED,\n    MESSAGE_DIRECTION_INBOUND, MESSAGE_DIRECTION_OUTBOUND,\n    MESSAGE_STATUS_QUEUED, MESSAGE_STATUS_SENT, MESSAGE_STATUS_DRAFT, MESSAGE_STATUS_FAILED, MESSAGE_STATUS_APPROVED,\n    MESSAGE_GENERATED_AI, MESSAGE_GENERATED_HUMAN, MESSAGE_GENERATED_SYSTEM,\n    ENRICHMENT_STATUS_UNENRICHED,\n    ENRICHMENT_STATUS_WITH_DOMAIN_NO_EMAIL,\n    ENRICHMENT_STATUS_WITH_PHONE_ONLY,\n    ENRICHMENT_STATUS_ENRICHED_NO_OUTBOUND,\n    ENRICHMENT_STATUS_OUTBOUND_SENT,\n    ENRICHMENT_STATUS_ARCHIVED_UNENRICHABLE,\n    UNENRICHABLE_REASON_NO_OSINT_PRESENCE,\n    OUTREACH_MODE_REVIEW,\n)\nfrom agents import (\n    run_bizdev_cycle,\n    run_onboarding_cycle,\n    run_ops_cycle,\n    run_billing_cycle,\n    run_event_driven_bizdev_cycle,\n)\nfrom signals_agent import (\n    run_signals_agent, \n    get_signals_summary, \n    get_lead_events_summary, \n    get_todays_opportunities,\n    get_lead_events_by_enrichment_status,\n    get_lead_events_counts_by_status\n)\nfrom email_utils import send_email, get_email_status, get_email_log, get_sendgrid_stats\nfrom lead_service import generate_new_leads_from_source, get_lead_source_log\nfrom lead_sources import get_lead_source_status\nimport signal_sources\nfrom signal_sources import get_signal_status, run_signal_pipeline, get_registry, get_signal_mode\nfrom release_mode import is_release_mode, print_startup_banners, get_release_mode_status\nfrom analytics import (\n    track_page_view, track_funnel_event, track_event,\n    EventType, get_analytics_summary, get_page_view_stats, get_funnel_stats\n)\n\ndef get_ga_script() -> str:\n    \"\"\"Generate Google Analytics 4 script tag if measurement ID is configured.\"\"\"\n    ga_id = os.environ.get(\"GA_MEASUREMENT_ID\", \"\")\n    if not ga_id:\n        return \"\"\n    return f'''<script async src=\"https://www.googletagmanager.com/gtag/js?id={ga_id}\"></script>\n    <script>\n        window.dataLayer = window.dataLayer || [];\n        function gtag(){{dataLayer.push(arguments);}}\n        gtag('js', new Date());\n        gtag('config', '{ga_id}');\n    </script>'''\nfrom stripe_utils import (\n    validate_stripe_at_startup,\n    is_stripe_enabled,\n    get_stripe_payment_mode_status,\n    ensure_invoice_payment_url,\n    get_invoice_payment_stats,\n    process_subscription_webhook,\n    verify_webhook_signature,\n    log_stripe_event,\n    get_stripe_webhook_secret\n)\nfrom subscription_utils import (\n    get_customer_plan_status,\n    get_subscription_status,\n    bootstrap_stripe_subscription_product,\n    create_stripe_customer,\n    create_subscription,\n    upgrade_to_paid,\n    expire_trial,\n    check_trial_abuse,\n    record_trial_identity,\n    initialize_trial,\n    get_or_create_subscription_checkout_link,\n    create_billing_portal_link\n)\nfrom auth_utils import (\n    hash_password,\n    verify_password,\n    create_customer_session,\n    verify_customer_session,\n    create_admin_session,\n    verify_admin_session,\n    authenticate_customer,\n    generate_public_token,\n    get_customer_from_session,\n    get_customer_from_token,\n    get_admin_password,\n    SESSION_COOKIE_NAME,\n    ADMIN_COOKIE_NAME,\n    SESSION_MAX_AGE,\n    ADMIN_SESSION_MAX_AGE\n)\n\napp = FastAPI(title=\"HossAgent Control Engine\")\n\nDEFAULT_TIMEZONE = \"America/New_York\"\n\ndef format_local_time(utc_dt: datetime, user_time_zone: Optional[str] = None) -> str:\n    \"\"\"\n    Convert UTC datetime to user's local time zone and format for display.\n    \n    Args:\n        utc_dt: Datetime in UTC\n        user_time_zone: IANA timezone string (e.g., \"America/New_York\")\n        \n    Returns:\n        Formatted string like \"Dec 5, 2025  5:42 PM\" or with \"(ET default)\" if using default\n    \"\"\"\n    if utc_dt is None:\n        return \"-\"\n    \n    use_default = False\n    tz_str = user_time_zone\n    \n    if not tz_str:\n        tz_str = DEFAULT_TIMEZONE\n        use_default = True\n    \n    try:\n        tz = ZoneInfo(tz_str)\n    except Exception:\n        tz = ZoneInfo(DEFAULT_TIMEZONE)\n        use_default = True\n    \n    if utc_dt.tzinfo is None:\n        utc_dt = utc_dt.replace(tzinfo=ZoneInfo(\"UTC\"))\n    \n    local_dt = utc_dt.astimezone(tz)\n    \n    formatted = local_dt.strftime(\"%b %d, %Y  %I:%M %p\").replace(\" 0\", \" \")\n    \n    if use_default:\n        formatted += \" (ET default)\"\n    \n    return formatted\n\n\n# ============================================================================\n# STARTUP & BACKGROUND AUTOPILOT\n# ============================================================================\n\n\ndef run_retroactive_payment_links(max_invoices: int = 50) -> int:\n    \"\"\"\n    Generate payment links for existing unpaid invoices that don't have them.\n    \n    Called on startup when ENABLE_STRIPE=TRUE to ensure all draft/sent invoices\n    have payment links.\n    \n    Args:\n        max_invoices: Maximum number of invoices to process per run (bounded)\n    \n    Returns:\n        Number of payment links created\n    \"\"\"\n    if not is_stripe_enabled():\n        return 0\n    \n    from sqlmodel import Session\n    \n    links_created = 0\n    \n    try:\n        with Session(engine) as session:\n            invoices_needing_links = session.exec(\n                select(Invoice).where(\n                    (Invoice.status.in_([\"draft\", \"sent\"])) &\n                    ((Invoice.payment_url == None) | (Invoice.payment_url == \"\"))\n                ).limit(max_invoices)\n            ).all()\n            \n            if not invoices_needing_links:\n                print(f\"[STRIPE][RETROACTIVE] No invoices need payment links\")\n                return 0\n            \n            print(f\"[STRIPE][RETROACTIVE] Processing {len(invoices_needing_links)} invoices for payment links\")\n            \n            for invoice in invoices_needing_links:\n                customer = session.exec(\n                    select(Customer).where(Customer.id == invoice.customer_id)\n                ).first()\n                \n                if not customer:\n                    print(f\"[STRIPE][RETROACTIVE] Invoice {invoice.id} has no customer, skipping\")\n                    continue\n                \n                result = ensure_invoice_payment_url(\n                    invoice_id=invoice.id,\n                    amount_cents=invoice.amount_cents,\n                    customer_id=customer.id,\n                    customer_email=customer.contact_email,\n                    customer_company=customer.company,\n                    invoice_status=invoice.status,\n                    existing_payment_url=invoice.payment_url\n                )\n                \n                if result.success and result.payment_url:\n                    invoice.payment_url = result.payment_url\n                    if result.stripe_id:\n                        invoice.stripe_payment_id = result.stripe_id\n                    session.add(invoice)\n                    links_created += 1\n            \n            if links_created > 0:\n                session.commit()\n                print(f\"[STRIPE][RETROACTIVE] Created {links_created} payment links\")\n            \n    except Exception as e:\n        print(f\"[STRIPE][RETROACTIVE] Error: {e}\")\n    \n    return links_created\n\n\ndef bootstrap_business_profiles(session: Session) -> int:\n    \"\"\"\n    Ensure all customers have a BusinessProfile with sensible defaults.\n    \n    Creates placeholder profiles for customers missing them, logging warnings.\n    This enables the Contextual Opportunity Engine to function for all customers.\n    \n    Returns:\n        Number of profiles created.\n    \"\"\"\n    from models import BusinessProfile\n    \n    customers_without_profiles = session.exec(\n        select(Customer).where(\n            ~Customer.id.in_(\n                select(BusinessProfile.customer_id)\n            )\n        )\n    ).all()\n    \n    created = 0\n    for customer in customers_without_profiles:\n        default_profile = BusinessProfile(\n            customer_id=customer.id,\n            short_description=f\"{customer.company} - Professional services\",\n            services=\"General business services\",\n            ideal_customer=\"Small to medium businesses\",\n            voice_tone=\"professional\",\n            communication_style=\"conversational\",\n            primary_contact_name=customer.contact_name or \"Team\",\n            primary_contact_email=customer.contact_email\n        )\n        \n        if customer.niche:\n            default_profile.services = customer.niche\n        if customer.geography:\n            default_profile.ideal_customer = f\"Businesses in {customer.geography}\"\n        \n        session.add(default_profile)\n        created += 1\n        print(f\"[BOOTSTRAP][WARNING] Created default BusinessProfile for customer {customer.id} ({customer.company}) - please configure in portal settings\")\n    \n    if created > 0:\n        session.commit()\n        print(f\"[BOOTSTRAP] Created {created} default BusinessProfiles\")\n    \n    return created\n\n\ndef run_production_cleanup(session: Session, owner_email_domain: str = \"\", purge_all_signals: bool = True) -> dict:\n    \"\"\"\n    One-time production database cleanup.\n    \n    Removes dev/test/demo data while preserving real production customers.\n    This should only be run ONCE during production initialization.\n    \n    Args:\n        session: Database session\n        owner_email_domain: Domain to identify real customers (e.g., \"mycompany.com\")\n        purge_all_signals: If True, deletes ALL signals/lead_events (fresh start)\n    \n    Returns:\n        Summary of cleanup actions taken.\n    \"\"\"\n    from datetime import datetime\n    import re\n    \n    results = {\n        \"signals_deleted\": 0,\n        \"lead_events_deleted\": 0,\n        \"pending_outbound_deleted\": 0,\n        \"reports_deleted\": 0,\n        \"invoices_deleted\": 0,\n        \"tasks_deleted\": 0,\n        \"leads_deleted\": 0,\n        \"customers_deleted\": 0,\n        \"counters_reset\": 0,\n        \"purged_at\": datetime.utcnow().isoformat(),\n        \"already_run\": False,\n        \"audit_log\": []\n    }\n    \n    cleanup_flag_file = Path(\"production_cleanup_completed.flag\")\n    if cleanup_flag_file.exists():\n        results[\"already_run\"] = True\n        print(\"[CLEANUP] Production cleanup already completed. Skipping.\")\n        return results\n    \n    print(\"[CLEANUP] Starting one-time production database cleanup...\")\n    \n    real_customer_ids = []\n    fake_customer_ids = []\n    all_customers = session.exec(select(Customer)).all()\n    \n    fake_company_patterns = [\n        r\"^Test\\s\", r\"^Demo\\s\", r\"^Fake\\s\", r\"^Sample\\s\",\n        r\"Quantum\\s*Dynamics\", r\"Apex\\s*Ventures\", r\"Stratton\\s*Industries\",\n        r\"Atlas\\s*Enterprise\", r\"Nexus\\s*Capital\", r\"Titan\\s*Logistics\",\n        r\"Meridian\\s*Solutions\", r\"Catalyst\\s*Growth\", r\"Vanguard\\s*Consulting\",\n        r\"Precision\\s*Demand\", r\"Sterling\\s*Strategy\", r\"Forge\\s*Strategic\",\n        r\"Atlas\\s*Revenue\", r\"Momentum\\s*Marketing\", r\"Quantum\\s*Lead\",\n        r\"Elevate\\s*Agency\", r\"Keystone\\s*Advisory\", r\"Summit\\s*Digital\"\n    ]\n    \n    for customer in all_customers:\n        is_real = False\n        is_fake = False\n        \n        if owner_email_domain and customer.contact_email and customer.contact_email.endswith(owner_email_domain):\n            is_real = True\n        \n        if customer.plan == \"paid\" and customer.subscription_status == \"active\":\n            is_real = True\n        \n        if customer.stripe_customer_id or customer.stripe_subscription_id:\n            is_real = True\n        \n        if hasattr(customer, 'notes') and customer.notes and \"ADMIN\" in customer.notes.upper():\n            is_real = True\n        \n        if customer.contact_email:\n            fake_email_patterns = [\"@example\", \"@test\", \"@fake\", \"@demo\", \"@localhost\", \"@dummy\"]\n            if any(p in customer.contact_email.lower() for p in fake_email_patterns):\n                is_fake = True\n        \n        if customer.company:\n            for pattern in fake_company_patterns:\n                if re.search(pattern, customer.company, re.IGNORECASE):\n                    is_fake = True\n                    break\n        \n        if is_real and not is_fake:\n            real_customer_ids.append(customer.id)\n            print(f\"[CLEANUP] Keeping real customer: {customer.id} - {customer.company} ({customer.contact_email})\")\n        elif is_fake and not is_real:\n            fake_customer_ids.append(customer.id)\n            results[\"audit_log\"].append(f\"CUSTOMER_MARKED_FAKE: {customer.id} - {customer.company}\")\n    \n    if not real_customer_ids:\n        print(\"[CLEANUP][WARNING] No real customers identified by domain. Checking for trial customers with real signups...\")\n        for customer in all_customers:\n            if customer.plan == \"trial\" and customer.trial_start_at and customer.id not in fake_customer_ids:\n                real_customer_ids.append(customer.id)\n                print(f\"[CLEANUP] Keeping trial customer: {customer.id} - {customer.company}\")\n    \n    if not real_customer_ids:\n        print(\"[CLEANUP][SAFETY] No real customers identified. Keeping all customers.\")\n        real_customer_ids = [c.id for c in all_customers]\n    \n    from models import Signal, LeadEvent, PendingOutbound, Report, Task, Invoice, Lead\n    \n    if purge_all_signals:\n        all_signals = session.exec(select(Signal)).all()\n        for s in all_signals:\n            session.delete(s)\n            results[\"signals_deleted\"] += 1\n        results[\"audit_log\"].append(f\"SIGNALS_PURGED_ALL: {results['signals_deleted']}\")\n        \n        all_events = session.exec(select(LeadEvent)).all()\n        for le in all_events:\n            session.delete(le)\n            results[\"lead_events_deleted\"] += 1\n        results[\"audit_log\"].append(f\"LEAD_EVENTS_PURGED_ALL: {results['lead_events_deleted']}\")\n    else:\n        signals = session.exec(select(Signal)).all()\n        for s in signals:\n            if s.company_id and s.company_id not in real_customer_ids:\n                session.delete(s)\n                results[\"signals_deleted\"] += 1\n        \n        lead_events = session.exec(select(LeadEvent)).all()\n        for le in lead_events:\n            if le.company_id and le.company_id not in real_customer_ids:\n                session.delete(le)\n                results[\"lead_events_deleted\"] += 1\n    \n    pending = session.exec(select(PendingOutbound)).all()\n    for p in pending:\n        if p.customer_id not in real_customer_ids:\n            session.delete(p)\n            results[\"pending_outbound_deleted\"] += 1\n    results[\"audit_log\"].append(f\"PENDING_OUTBOUND_DELETED: {results['pending_outbound_deleted']}\")\n    \n    reports = session.exec(select(Report)).all()\n    for r in reports:\n        if r.customer_id not in real_customer_ids:\n            session.delete(r)\n            results[\"reports_deleted\"] += 1\n    results[\"audit_log\"].append(f\"REPORTS_DELETED: {results['reports_deleted']}\")\n    \n    tasks = session.exec(select(Task)).all()\n    for t in tasks:\n        if t.customer_id not in real_customer_ids:\n            session.delete(t)\n            results[\"tasks_deleted\"] += 1\n    results[\"audit_log\"].append(f\"TASKS_DELETED: {results['tasks_deleted']}\")\n    \n    invoices = session.exec(select(Invoice)).all()\n    for i in invoices:\n        should_delete = False\n        if i.customer_id not in real_customer_ids:\n            should_delete = True\n        elif i.amount_cents == 0 and i.status == \"draft\":\n            should_delete = True\n        elif i.amount_cents == 0 and not i.stripe_invoice_id:\n            should_delete = True\n        \n        if should_delete:\n            session.delete(i)\n            results[\"invoices_deleted\"] += 1\n    results[\"audit_log\"].append(f\"INVOICES_DELETED: {results['invoices_deleted']}\")\n    \n    leads = session.exec(select(Lead)).all()\n    fake_lead_patterns = [\n        r\"^Lead_\\d+\",\n        r\"@example\\.\", r\"@test\\.\", r\"@fake\\.\", r\"@demo\\.\",\n        r\"@localhost\", r\"@mailinator\", r\"@dummy\",\n        r\"^contact@(quantum|apex|stratton|atlas|nexus|titan|meridian|catalyst|vanguard)\",\n        r\"@atlasrevenue\", r\"@stratton\", r\"@apex\", r\"@quantum\", r\"@nexus\",\n        r\"@titan\", r\"@meridian\", r\"@catalyst\", r\"@vanguard\", r\"@blackstone\",\n        r\"@vector\", r\"@summit\", r\"@horizon\", r\"@precision\", r\"@sterling\",\n        r\"@forge\", r\"@momentum\", r\"@elevate\", r\"@keystone\", r\"@pioneer\",\n    ]\n    \n    fake_company_names = [\n        \"atlas\", \"stratton\", \"apex\", \"quantum\", \"nexus\", \"titan\", \"meridian\",\n        \"catalyst\", \"vanguard\", \"blackstone\", \"vector\", \"summit\", \"horizon\",\n        \"precision\", \"sterling\", \"forge\", \"momentum\", \"elevate\", \"keystone\", \"pioneer\"\n    ]\n    \n    for lead in leads:\n        is_fake_lead = False\n        \n        if hasattr(lead, 'source') and lead.source in (\"dummy_seed\", \"test\", \"demo\"):\n            is_fake_lead = True\n        \n        if lead.email:\n            for pattern in fake_lead_patterns:\n                if re.search(pattern, lead.email, re.IGNORECASE):\n                    is_fake_lead = True\n                    break\n        \n        if lead.name and re.match(r\"^Lead_\\d+$\", lead.name):\n            is_fake_lead = True\n        \n        if hasattr(lead, 'company') and lead.company:\n            company_lower = lead.company.lower()\n            for fake_name in fake_company_names:\n                if fake_name in company_lower:\n                    is_fake_lead = True\n                    break\n        \n        if is_fake_lead:\n            session.delete(lead)\n            results[\"leads_deleted\"] += 1\n    results[\"audit_log\"].append(f\"LEADS_DELETED: {results['leads_deleted']}\")\n    \n    for cid in fake_customer_ids:\n        if cid not in real_customer_ids:\n            customer = session.get(Customer, cid)\n            if customer:\n                session.delete(customer)\n                results[\"customers_deleted\"] += 1\n                results[\"audit_log\"].append(f\"CUSTOMER_DELETED: {cid} - {customer.company}\")\n    \n    for customer in all_customers:\n        if customer.id in real_customer_ids:\n            customer.tasks_this_period = 0\n            customer.leads_this_period = 0\n            session.add(customer)\n            results[\"counters_reset\"] += 1\n    results[\"audit_log\"].append(f\"COUNTERS_RESET: {results['counters_reset']}\")\n    \n    session.commit()\n    \n    audit_content = f\"\"\"Production Cleanup Audit Log\n============================\nTimestamp: {results['purged_at']}\nOwner Domain Filter: {owner_email_domain or 'None'}\nPurge All Signals: {purge_all_signals}\n\nSummary:\n--------\nSignals Deleted: {results['signals_deleted']}\nLead Events Deleted: {results['lead_events_deleted']}\nPending Outbound Deleted: {results['pending_outbound_deleted']}\nReports Deleted: {results['reports_deleted']}\nTasks Deleted: {results['tasks_deleted']}\nInvoices Deleted: {results['invoices_deleted']}\nLeads Deleted: {results['leads_deleted']}\nCustomers Deleted: {results['customers_deleted']}\nCounters Reset: {results['counters_reset']}\n\nAudit Trail:\n------------\n\"\"\" + \"\\n\".join(results[\"audit_log\"])\n    \n    cleanup_flag_file.write_text(audit_content)\n    \n    print(f\"[CLEANUP] Complete. Signals: {results['signals_deleted']}, Events: {results['lead_events_deleted']}, \"\n          f\"Outbound: {results['pending_outbound_deleted']}, Reports: {results['reports_deleted']}, \"\n          f\"Tasks: {results['tasks_deleted']}, Invoices: {results['invoices_deleted']}, \"\n          f\"Leads: {results['leads_deleted']}, Customers: {results['customers_deleted']}, \"\n          f\"Counters reset: {results['counters_reset']}\")\n    \n    return results\n\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"\"\"Initialize database, validate configuration, and start autopilot background loop.\"\"\"\n    import os\n    print_startup_banners()\n    \n    create_db_and_tables()\n    \n    validate_stripe_at_startup()\n    \n    bootstrap_result = bootstrap_stripe_subscription_product()\n    if bootstrap_result[\"success\"]:\n        print(f\"[STARTUP] Subscription product ready: {bootstrap_result['message']}\")\n    elif is_stripe_enabled():\n        print(f\"[STARTUP] Subscription bootstrap: {bootstrap_result['message']}\")\n    \n    run_retroactive_payment_links()\n    \n    print(\"[LEADS][STARTUP] HossNative (Autonomous Discovery) active\")\n    print(\"[LEADS][STARTUP] Lead discovery via SignalNet + web scraping - no external APIs\")\n    \n    asyncio.create_task(autopilot_loop())\n    print(\"[STARTUP] HossAgent initialized. Autopilot loop active.\")\n\n\nasync def autopilot_loop():\n    \"\"\"\n    Background task: Runs agent cycles automatically when autopilot is enabled.\n    \n    Checks SystemSettings.autopilot_enabled every 15 minutes (production cycle).\n    If enabled, runs the full SignalNet-first pipeline:\n      1. Lead Generation - Fetch new leads from configured source\n      2. SignalNet Pipeline - Fetch signals, score, convert to LeadEvents\n      3. Enrichment - Enrich unenriched LeadEvents with contact data\n      4. BizDev - Send outreach emails to NEW leads\n      5. Event-Driven BizDev - Process ENRICHED LeadEvents for contextual outreach\n      6. Onboarding - Convert qualified leads to customers\n      7. Ops - Execute pending tasks and generate reports\n      8. Billing - Generate invoices for completed work\n    \n    Pipeline: SignalNet  Score  LeadEvents  Enrich  BizDev  Email\n    \n    Per-customer autopilot settings override global behavior.\n    Safe: Catches and logs exceptions without crashing the loop.\n    Idempotent: Prevents duplicate LeadEvents, outbound, and reports.\n    \"\"\"\n    # Allow the server to fully start before running the first cycle\n    await asyncio.sleep(5)\n    \n    # Log email mode at startup\n    from email_utils import get_email_status\n    email_status = get_email_status()\n    print(\"[AUTOPILOT][STARTUP] Pipeline: SignalNet  Score  LeadEvents  Enrich  BizDev  Email\")\n    print(f\"[AUTOPILOT][STARTUP] Email mode: {email_status['mode']} (configured: {email_status['configured_mode']})\")\n    if not email_status['is_valid']:\n        print(f\"[AUTOPILOT][STARTUP] Email fallback reason: {email_status['message']}\")\n    \n    while True:\n        try:\n            with Session(engine) as session:\n                settings = session.exec(\n                    select(SystemSettings).where(SystemSettings.id == 1)\n                ).first()\n\n                if settings and settings.autopilot_enabled:\n                    print(\"\\n[AUTOPILOT] Starting cycle...\")\n                    \n                    # Bootstrap business profiles for customers without them\n                    bootstrap_business_profiles(session)\n                    \n                    # Step 1: Generate new leads from configured source\n                    generate_new_leads_from_source(session)\n                    \n                    # Step 2-3: Run SignalNet pipeline (fetches, scores, converts to LeadEvents)\n                    run_signals_agent(session)\n                    \n                    # Step 4: Enrich unenriched LeadEvents (batch of up to 15 per cycle)\n                    try:\n                        from lead_enrichment import run_enrichment_pipeline\n                        enrichment_results = await run_enrichment_pipeline(session)\n                        pending_info = f\", Pending: {enrichment_results.get('pending', 0)}\" if enrichment_results.get('pending', 0) > 0 else \"\"\n                        print(f\"[ENRICHMENT] Processed: {enrichment_results.get('processed', 0)}, \"\n                              f\"Enriched: {enrichment_results.get('enriched', 0)}, \"\n                              f\"Failed: {enrichment_results.get('failed', 0)}{pending_info}\")\n                    except Exception as e:\n                        print(f\"[ENRICHMENT][ERROR] {e}\")\n                    \n                    # Step 4b: MacroStorm - SEC EDGAR ingestion (every 4 hours)\n                    try:\n                        from sec_edgar_connector import run_sec_edgar_ingestion\n                        from forcecast_engine import run_forcecast_for_new_macro_events\n                        \n                        last_edgar_run = getattr(autopilot_loop, '_last_edgar_run', 0)\n                        edgar_cooldown = 14400\n                        \n                        if time.time() - last_edgar_run >= edgar_cooldown:\n                            print(\"[MACROSTORM][SEC_EDGAR] Running scheduled SEC filing ingestion...\")\n                            edgar_result = run_sec_edgar_ingestion(session, filing_types=[\"10-K\", \"10-Q\", \"8-K\"])\n                            print(f\"[MACROSTORM][SEC_EDGAR] Filings: {edgar_result.get('filings_found', 0)}, MacroEvents: {edgar_result.get('macro_events_created', 0)}\")\n                            \n                            forcecast_result = run_forcecast_for_new_macro_events(session)\n                            print(f\"[MACROSTORM][FORCECAST] MacroEvents mapped: {forcecast_result.get('macro_events_processed', 0)}, LeadEvents: {forcecast_result.get('lead_events_created', 0)}\")\n                            \n                            autopilot_loop._last_edgar_run = time.time()\n                    except Exception as e:\n                        print(f\"[MACROSTORM][ERROR] {e}\")\n                    \n                    # Step 5-6: BizDev outreach (now prefers enriched LeadEvents)\n                    await run_bizdev_cycle(session)\n                    await run_event_driven_bizdev_cycle(session)\n                    \n                    # Step 7: Onboarding, Ops, and Billing\n                    await run_onboarding_cycle(session)\n                    await run_ops_cycle(session)\n                    await run_billing_cycle(session)\n                    print(\"[AUTOPILOT] Cycle complete.\\n\")\n                else:\n                    pass  # Silent when disabled to reduce log noise\n\n        except Exception as e:\n            import traceback\n            print(f\"[AUTOPILOT][ERROR] {e}\")\n            print(f\"[AUTOPILOT][TRACEBACK] {traceback.format_exc()}\")\n\n        # Sleep 15 minutes between cycles (production cadence)\n        await asyncio.sleep(900)\n\n\n# ============================================================================\n# PAGES\n# ============================================================================\n\n\n@app.get(\"/\", response_class=HTMLResponse)\ndef serve_marketing_landing(request: Request):\n    \"\"\"Marketing Landing Page: Public-facing marketing page for new visitors.\"\"\"\n    track_page_view(\n        path=\"/\",\n        referrer=request.headers.get(\"referer\"),\n        user_agent=request.headers.get(\"user-agent\"),\n        ip_address=request.client.host if request.client else None\n    )\n    with open(\"templates/marketing_landing.html\", \"r\") as f:\n        template = f.read()\n    return template.replace(\"{ga_script}\", get_ga_script())\n\n\n@app.get(\"/about\", response_class=HTMLResponse)\ndef serve_about_page():\n    \"\"\"About Page: Information about HossAgent.\"\"\"\n    with open(\"templates/about.html\", \"r\") as f:\n        return f.read()\n\n\n@app.get(\"/how-it-works\", response_class=HTMLResponse)\ndef serve_how_it_works_page():\n    \"\"\"How It Works Page: Guide for using HossAgent.\"\"\"\n    with open(\"templates/how_it_works.html\", \"r\") as f:\n        return f.read()\n\n\n# ============================================================================\n# AUTHENTICATION ROUTES\n# ============================================================================\n\n\n@app.get(\"/signup\", response_class=HTMLResponse)\ndef signup_get(request: Request):\n    \"\"\"Render signup form.\"\"\"\n    track_funnel_event(\n        EventType.SIGNUP_STARTED,\n        ip_address=request.client.host if request.client else None\n    )\n    \n    with open(\"templates/auth_signup.html\", \"r\") as f:\n        template = f.read()\n    \n    html = template.format(\n        error_html=\"\",\n        company=\"\",\n        contact_name=\"\",\n        email=\"\",\n        niche=\"\",\n        geography=\"\"\n    )\n    return html\n\n\n@app.post(\"/signup\", response_class=HTMLResponse)\ndef signup_post(\n    request: Request,\n    company: str = Form(...),\n    contact_name: str = Form(...),\n    email: str = Form(...),\n    password: str = Form(...),\n    password_confirm: str = Form(...),\n    niche: str = Form(\"\"),\n    geography: str = Form(\"\"),\n    session: Session = Depends(get_session)\n):\n    \"\"\"Process signup form.\"\"\"\n    with open(\"templates/auth_signup.html\", \"r\") as f:\n        template = f.read()\n    \n    def render_error(error_msg: str) -> str:\n        error_html = f'<div class=\"error-message\">{error_msg}</div>'\n        return template.format(\n            error_html=error_html,\n            company=company,\n            contact_name=contact_name,\n            email=email,\n            niche=niche,\n            geography=geography\n        )\n    \n    if password != password_confirm:\n        return HTMLResponse(content=render_error(\"Passwords do not match\"))\n    \n    if len(password) < 8:\n        return HTMLResponse(content=render_error(\"Password must be at least 8 characters\"))\n    \n    ip_address = request.client.host if request.client else None\n    user_agent = request.headers.get(\"User-Agent\")\n    \n    is_allowed, block_reason = check_trial_abuse(\n        session=session,\n        email=email.lower().strip(),\n        ip_address=ip_address,\n        user_agent=user_agent\n    )\n    \n    if not is_allowed:\n        return HTMLResponse(content=render_error(f\"Unable to create trial: {block_reason}\"))\n    \n    existing = session.exec(\n        select(Customer).where(Customer.contact_email == email.lower().strip())\n    ).first()\n    if existing:\n        return HTMLResponse(content=render_error(\"An account with this email already exists. Please log in.\"))\n    \n    customer = Customer(\n        company=company.strip(),\n        contact_name=contact_name.strip(),\n        contact_email=email.lower().strip(),\n        password_hash=hash_password(password),\n        public_token=generate_public_token(),\n        niche=niche.strip() if niche else None,\n        geography=geography.strip() if geography else None\n    )\n    \n    customer = initialize_trial(customer)\n    \n    session.add(customer)\n    session.flush()\n    \n    record_trial_identity(\n        session=session,\n        customer_id=customer.id,\n        email=email.lower().strip(),\n        ip_address=ip_address,\n        user_agent=user_agent\n    )\n    \n    session.commit()\n    \n    track_funnel_event(\n        EventType.SIGNUP_COMPLETED,\n        customer_id=customer.id,\n        ip_address=ip_address\n    )\n    \n    print(f\"[SIGNUP] New customer created: {customer.company} ({customer.contact_email})\")\n    \n    session_token = create_customer_session(customer.id)\n    response = RedirectResponse(url=\"/portal\", status_code=303)\n    response.set_cookie(\n        key=SESSION_COOKIE_NAME,\n        value=session_token,\n        max_age=SESSION_MAX_AGE,\n        httponly=True,\n        samesite=\"lax\"\n    )\n    return response\n\n\n@app.get(\"/login\", response_class=HTMLResponse)\ndef login_get(request: Request):\n    \"\"\"Render login form.\"\"\"\n    with open(\"templates/auth_login.html\", \"r\") as f:\n        template = f.read()\n    \n    message_html = \"\"\n    if request.query_params.get(\"registered\") == \"true\":\n        message_html = '<div class=\"success-message\">Account created successfully. Please log in.</div>'\n    elif request.query_params.get(\"logout\") == \"true\":\n        message_html = '<div class=\"success-message\">You have been logged out.</div>'\n    \n    html = template.format(\n        message_html=message_html,\n        email=\"\"\n    )\n    return html\n\n\n@app.post(\"/login\", response_class=HTMLResponse)\ndef login_post(\n    email: str = Form(...),\n    password: str = Form(...),\n    session: Session = Depends(get_session)\n):\n    \"\"\"Process login form.\"\"\"\n    with open(\"templates/auth_login.html\", \"r\") as f:\n        template = f.read()\n    \n    customer, error = authenticate_customer(session, email, password)\n    \n    if error:\n        error_html = f'<div class=\"error-message\">{error}</div>'\n        html = template.format(\n            message_html=error_html,\n            email=email\n        )\n        return HTMLResponse(content=html)\n    \n    track_funnel_event(EventType.LOGIN, customer_id=customer.id)\n    \n    session_token = create_customer_session(customer.id)\n    response = RedirectResponse(url=\"/portal\", status_code=303)\n    response.set_cookie(\n        key=SESSION_COOKIE_NAME,\n        value=session_token,\n        max_age=SESSION_MAX_AGE,\n        httponly=True,\n        samesite=\"lax\"\n    )\n    \n    print(f\"[LOGIN] Customer logged in: {customer.contact_email}\")\n    return response\n\n\n@app.get(\"/logout\")\ndef logout():\n    \"\"\"Clear session and redirect to home.\"\"\"\n    response = RedirectResponse(url=\"/?logout=true\", status_code=303)\n    response.delete_cookie(key=SESSION_COOKIE_NAME)\n    return response\n\n\n# ============================================================================\n# FORGOT PASSWORD ROUTES\n# ============================================================================\n\n\n@app.get(\"/forgot-password\", response_class=HTMLResponse)\ndef forgot_password_get(request: Request):\n    \"\"\"Render forgot password form.\"\"\"\n    with open(\"templates/forgot_password.html\", \"r\") as f:\n        template = f.read()\n    \n    message_html = \"\"\n    if request.query_params.get(\"sent\") == \"true\":\n        message_html = '<div class=\"success-message\">If an account with that email exists, a reset link has been sent.</div>'\n    \n    html = template.format(\n        message_html=message_html,\n        email=\"\"\n    )\n    return html\n\n\n@app.post(\"/forgot-password\", response_class=HTMLResponse)\ndef forgot_password_post(\n    request: Request,\n    email: str = Form(...),\n    session: Session = Depends(get_session)\n):\n    \"\"\"Process forgot password form - generate reset token and send email.\"\"\"\n    customer = session.exec(\n        select(Customer).where(Customer.contact_email == email.lower().strip())\n    ).first()\n    \n    if customer:\n        token = secrets.token_urlsafe(32)\n        expires_at = datetime.utcnow() + timedelta(hours=1)\n        \n        reset_token = PasswordResetToken(\n            customer_id=customer.id,\n            token=token,\n            expires_at=expires_at\n        )\n        session.add(reset_token)\n        session.commit()\n        \n        host = request.headers.get(\"host\", \"localhost:5000\")\n        scheme = \"https\" if \"https\" in request.url.scheme or host.endswith(\".repl.co\") or host.endswith(\".replit.dev\") or host.endswith(\".replit.app\") else \"http\"\n        reset_url = f\"{scheme}://{host}/reset-password?token={token}\"\n        \n        reset_email_subject = \"Reset Your HossAgent Password\"\n        reset_email_body = f\"\"\"Hello {customer.contact_name or 'there'},\n\nYou requested a password reset for your HossAgent account.\n\nClick the link below to reset your password:\n{reset_url}\n\nThis link will expire in 1 hour.\n\nIf you didn't request this reset, you can safely ignore this email.\n\nBest,\nThe HossAgent Team\n\"\"\"\n        \n        send_email(\n            to_email=customer.contact_email,\n            subject=reset_email_subject,\n            body=reset_email_body,\n            lead_name=customer.contact_name or \"\",\n            company=customer.company\n        )\n        \n        print(f\"[FORGOT_PASSWORD] Reset token generated for: {customer.contact_email}\")\n    else:\n        print(f\"[FORGOT_PASSWORD] No account found for: {email}\")\n    \n    return RedirectResponse(url=\"/forgot-password?sent=true\", status_code=303)\n\n\n@app.get(\"/reset-password\", response_class=HTMLResponse)\ndef reset_password_get(\n    token: str = Query(None),\n    session: Session = Depends(get_session)\n):\n    \"\"\"Render reset password form.\"\"\"\n    with open(\"templates/reset_password.html\", \"r\") as f:\n        template = f.read()\n    \n    if not token:\n        html = template.format(\n            message_html='<div class=\"error-message\">Invalid reset link. Please request a new password reset.</div>',\n            form_html='<p style=\"text-align: center; color: #888;\">No valid reset token provided.</p>'\n        )\n        return html\n    \n    reset_token = session.exec(\n        select(PasswordResetToken).where(PasswordResetToken.token == token)\n    ).first()\n    \n    if not reset_token:\n        html = template.format(\n            message_html='<div class=\"error-message\">Invalid reset link. Please request a new password reset.</div>',\n            form_html='<p style=\"text-align: center; color: #888;\"><a href=\"/forgot-password\" style=\"color: #999;\">Request a new reset link</a></p>'\n        )\n        return html\n    \n    if reset_token.used:\n        html = template.format(\n            message_html='<div class=\"error-message\">This reset link has already been used. Please request a new password reset.</div>',\n            form_html='<p style=\"text-align: center; color: #888;\"><a href=\"/forgot-password\" style=\"color: #999;\">Request a new reset link</a></p>'\n        )\n        return html\n    \n    if datetime.utcnow() > reset_token.expires_at:\n        html = template.format(\n            message_html='<div class=\"error-message\">This reset link has expired. Please request a new password reset.</div>',\n            form_html='<p style=\"text-align: center; color: #888;\"><a href=\"/forgot-password\" style=\"color: #999;\">Request a new reset link</a></p>'\n        )\n        return html\n    \n    form_html = f'''\n            <form method=\"POST\" action=\"/reset-password\">\n                <input type=\"hidden\" name=\"token\" value=\"{token}\">\n                \n                <div class=\"form-group\">\n                    <label>New Password</label>\n                    <input type=\"password\" name=\"password\" placeholder=\"Enter new password\" required minlength=\"8\">\n                </div>\n                \n                <div class=\"form-group\">\n                    <label>Confirm Password</label>\n                    <input type=\"password\" name=\"password_confirm\" placeholder=\"Confirm new password\" required minlength=\"8\">\n                </div>\n                \n                <button type=\"submit\" class=\"btn-submit\">Reset Password</button>\n            </form>\n    '''\n    \n    html = template.format(\n        message_html=\"\",\n        form_html=form_html\n    )\n    return html\n\n\n@app.post(\"/reset-password\", response_class=HTMLResponse)\ndef reset_password_post(\n    token: str = Form(...),\n    password: str = Form(...),\n    password_confirm: str = Form(...),\n    session: Session = Depends(get_session)\n):\n    \"\"\"Process reset password form - validate token and set new password.\"\"\"\n    with open(\"templates/reset_password.html\", \"r\") as f:\n        template = f.read()\n    \n    def render_error(error_msg: str, show_form: bool = False) -> str:\n        error_html = f'<div class=\"error-message\">{error_msg}</div>'\n        if show_form:\n            form_html = f'''\n            <form method=\"POST\" action=\"/reset-password\">\n                <input type=\"hidden\" name=\"token\" value=\"{token}\">\n                \n                <div class=\"form-group\">\n                    <label>New Password</label>\n                    <input type=\"password\" name=\"password\" placeholder=\"Enter new password\" required minlength=\"8\">\n                </div>\n                \n                <div class=\"form-group\">\n                    <label>Confirm Password</label>\n                    <input type=\"password\" name=\"password_confirm\" placeholder=\"Confirm new password\" required minlength=\"8\">\n                </div>\n                \n                <button type=\"submit\" class=\"btn-submit\">Reset Password</button>\n            </form>\n            '''\n        else:\n            form_html = '<p style=\"text-align: center; color: #888;\"><a href=\"/forgot-password\" style=\"color: #999;\">Request a new reset link</a></p>'\n        return template.format(message_html=error_html, form_html=form_html)\n    \n    reset_token = session.exec(\n        select(PasswordResetToken).where(PasswordResetToken.token == token)\n    ).first()\n    \n    if not reset_token:\n        return HTMLResponse(content=render_error(\"Invalid reset link. Please request a new password reset.\"))\n    \n    if reset_token.used:\n        return HTMLResponse(content=render_error(\"This reset link has already been used. Please request a new password reset.\"))\n    \n    if datetime.utcnow() > reset_token.expires_at:\n        return HTMLResponse(content=render_error(\"This reset link has expired. Please request a new password reset.\"))\n    \n    if password != password_confirm:\n        return HTMLResponse(content=render_error(\"Passwords do not match.\", show_form=True))\n    \n    if len(password) < 8:\n        return HTMLResponse(content=render_error(\"Password must be at least 8 characters.\", show_form=True))\n    \n    customer = session.exec(\n        select(Customer).where(Customer.id == reset_token.customer_id)\n    ).first()\n    \n    if not customer:\n        return HTMLResponse(content=render_error(\"Account not found. Please contact support.\"))\n    \n    customer.password_hash = hash_password(password)\n    reset_token.used = True\n    session.add(customer)\n    session.add(reset_token)\n    session.commit()\n    \n    print(f\"[RESET_PASSWORD] Password reset for: {customer.contact_email}\")\n    \n    session_token = create_customer_session(customer.id)\n    response = RedirectResponse(url=\"/portal\", status_code=303)\n    response.set_cookie(\n        key=SESSION_COOKIE_NAME,\n        value=session_token,\n        max_age=SESSION_MAX_AGE,\n        httponly=True,\n        samesite=\"lax\"\n    )\n    \n    return response\n\n\n# ============================================================================\n# ADMIN AUTHENTICATION ROUTES\n# ============================================================================\n\n\n@app.get(\"/admin\", response_class=HTMLResponse)\ndef serve_admin_console(request: Request, session: Session = Depends(get_session)):\n    \"\"\"Admin Console: Operator controls for system management (requires authentication).\"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    \n    if not verify_admin_session(admin_token):\n        return RedirectResponse(url=\"/admin/login\", status_code=303)\n    \n    with open(\"templates/admin_console_new.html\", \"r\") as f:\n        return f.read()\n\n\n@app.get(\"/admin/diagnostics\", response_class=HTMLResponse)\ndef serve_diagnostics(request: Request, session: Session = Depends(get_session)):\n    \"\"\"Lead funnel diagnostics dashboard (requires authentication).\"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    \n    if not verify_admin_session(admin_token):\n        return RedirectResponse(url=\"/admin/login\", status_code=303)\n    \n    with open(\"templates/admin_diagnostics.html\", \"r\") as f:\n        return f.read()\n\n\n@app.get(\"/admin/login\", response_class=HTMLResponse)\ndef admin_login_get():\n    \"\"\"Render admin login form.\"\"\"\n    with open(\"templates/admin_login.html\", \"r\") as f:\n        template = f.read()\n    \n    html = template.format(error_html=\"\")\n    return html\n\n\n@app.post(\"/admin/login\", response_class=HTMLResponse)\ndef admin_login_post(\n    password: str = Form(...)\n):\n    \"\"\"Process admin login form.\"\"\"\n    with open(\"templates/admin_login.html\", \"r\") as f:\n        template = f.read()\n    \n    admin_password = get_admin_password()\n    \n    if password != admin_password:\n        error_html = '<div class=\"error-message\">Invalid password</div>'\n        html = template.format(error_html=error_html)\n        return HTMLResponse(content=html)\n    \n    admin_token = create_admin_session()\n    response = RedirectResponse(url=\"/admin\", status_code=303)\n    response.set_cookie(\n        key=ADMIN_COOKIE_NAME,\n        value=admin_token,\n        max_age=ADMIN_SESSION_MAX_AGE,\n        httponly=True,\n        samesite=\"lax\"\n    )\n    \n    print(\"[ADMIN] Admin logged in\")\n    return response\n\n\n# ============================================================================\n# SESSION-BASED PORTAL ROUTE\n# ============================================================================\n\n\n@app.get(\"/portal\", response_class=HTMLResponse)\ndef portal_session_based(request: Request, session: Session = Depends(get_session)):\n    \"\"\"\n    Session-based customer portal for logged-in customers.\n    \n    Requires a valid session cookie. Redirects to login if not authenticated.\n    \"\"\"\n    session_token = request.cookies.get(SESSION_COOKIE_NAME)\n    customer = get_customer_from_session(session, session_token)\n    \n    if not customer:\n        return RedirectResponse(url=\"/login\", status_code=303)\n    \n    track_funnel_event(EventType.PORTAL_VIEW, customer_id=customer.id)\n    \n    return render_customer_portal(customer, request, session)\n\n\n@app.get(\"/portal/settings\", response_class=HTMLResponse)\ndef portal_settings_get(request: Request, session: Session = Depends(get_session)):\n    \"\"\"Display business profile / settings form.\"\"\"\n    session_token = request.cookies.get(SESSION_COOKIE_NAME)\n    customer = get_customer_from_session(session, session_token)\n    \n    if not customer:\n        return RedirectResponse(url=\"/login\", status_code=303)\n    \n    track_funnel_event(EventType.SETTINGS_VIEW, customer_id=customer.id)\n    \n    profile = session.exec(\n        select(BusinessProfile).where(BusinessProfile.customer_id == customer.id)\n    ).first()\n    \n    with open(\"templates/portal_settings.html\", \"r\") as f:\n        template = f.read()\n    \n    def selected(value, check):\n        return 'selected=\"selected\"' if value == check else ''\n    \n    html = template.format(\n        message_html=\"\",\n        short_description=profile.short_description or \"\" if profile else \"\",\n        services=profile.services or \"\" if profile else \"\",\n        pricing_notes=profile.pricing_notes or \"\" if profile else \"\",\n        ideal_customer=profile.ideal_customer or \"\" if profile else \"\",\n        excluded_customers=profile.excluded_customers or \"\" if profile else \"\",\n        voice_tone_professional=selected(profile.voice_tone if profile else \"\", \"professional\"),\n        voice_tone_friendly=selected(profile.voice_tone if profile else \"\", \"friendly\"),\n        voice_tone_casual=selected(profile.voice_tone if profile else \"\", \"casual\"),\n        voice_tone_formal=selected(profile.voice_tone if profile else \"\", \"formal\"),\n        voice_tone_confident=selected(profile.voice_tone if profile else \"\", \"confident\"),\n        comm_style_direct=selected(profile.communication_style if profile else \"\", \"direct\"),\n        comm_style_conversational=selected(profile.communication_style if profile else \"\", \"conversational\"),\n        comm_style_storytelling=selected(profile.communication_style if profile else \"\", \"storytelling\"),\n        comm_style_data=selected(profile.communication_style if profile else \"\", \"data-driven\"),\n        constraints=profile.constraints or \"\" if profile else \"\",\n        primary_contact_name=profile.primary_contact_name or customer.contact_name or \"\" if profile else customer.contact_name or \"\",\n        primary_contact_email=profile.primary_contact_email or customer.contact_email or \"\" if profile else customer.contact_email or \"\",\n        outreach_mode_auto='selected=\"selected\"' if customer.outreach_mode == \"AUTO\" else \"\",\n        outreach_mode_review='selected=\"selected\"' if customer.outreach_mode == \"REVIEW\" else \"\",\n        autopilot_enabled_true='selected=\"selected\"' if customer.autopilot_enabled else \"\",\n        autopilot_enabled_false='selected=\"selected\"' if not customer.autopilot_enabled else \"\",\n        do_not_contact_list=profile.do_not_contact_list or \"\" if profile else \"\"\n    )\n    \n    return HTMLResponse(content=html)\n\n\n@app.post(\"/portal/settings\", response_class=HTMLResponse)\ndef portal_settings_post(\n    request: Request,\n    short_description: str = Form(\"\"),\n    services: str = Form(\"\"),\n    pricing_notes: str = Form(\"\"),\n    ideal_customer: str = Form(\"\"),\n    excluded_customers: str = Form(\"\"),\n    voice_tone: str = Form(\"\"),\n    communication_style: str = Form(\"\"),\n    constraints: str = Form(\"\"),\n    primary_contact_name: str = Form(\"\"),\n    primary_contact_email: str = Form(\"\"),\n    outreach_mode: str = Form(\"AUTO\"),\n    autopilot_enabled: str = Form(\"true\"),\n    do_not_contact_list: str = Form(\"\"),\n    session: Session = Depends(get_session)\n):\n    \"\"\"Save business profile / settings.\"\"\"\n    session_token = request.cookies.get(SESSION_COOKIE_NAME)\n    customer = get_customer_from_session(session, session_token)\n    \n    if not customer:\n        return RedirectResponse(url=\"/login\", status_code=303)\n    \n    profile = session.exec(\n        select(BusinessProfile).where(BusinessProfile.customer_id == customer.id)\n    ).first()\n    \n    if not profile:\n        profile = BusinessProfile(customer_id=customer.id)\n    \n    profile.short_description = short_description.strip() or None\n    profile.services = services.strip() or None\n    profile.pricing_notes = pricing_notes.strip() or None\n    profile.ideal_customer = ideal_customer.strip() or None\n    profile.excluded_customers = excluded_customers.strip() or None\n    profile.voice_tone = voice_tone.strip() or None\n    profile.communication_style = communication_style.strip() or None\n    profile.constraints = constraints.strip() or None\n    profile.primary_contact_name = primary_contact_name.strip() or None\n    profile.primary_contact_email = primary_contact_email.strip() or None\n    profile.do_not_contact_list = do_not_contact_list.strip() or None\n    profile.updated_at = datetime.utcnow()\n    \n    customer.outreach_mode = outreach_mode if outreach_mode in [\"AUTO\", \"REVIEW\"] else \"AUTO\"\n    customer.autopilot_enabled = autopilot_enabled.lower() == \"true\"\n    \n    session.add(profile)\n    session.add(customer)\n    session.commit()\n    \n    print(f\"[PORTAL] Settings saved for customer {customer.id}: {customer.company} (autopilot={'ON' if customer.autopilot_enabled else 'OFF'})\")\n    \n    with open(\"templates/portal_settings.html\", \"r\") as f:\n        template = f.read()\n    \n    def selected(value, check):\n        return 'selected=\"selected\"' if value == check else ''\n    \n    html = template.format(\n        message_html='<div class=\"success-message\">Settings saved successfully!</div>',\n        short_description=profile.short_description or \"\",\n        services=profile.services or \"\",\n        pricing_notes=profile.pricing_notes or \"\",\n        ideal_customer=profile.ideal_customer or \"\",\n        excluded_customers=profile.excluded_customers or \"\",\n        voice_tone_professional=selected(profile.voice_tone, \"professional\"),\n        voice_tone_friendly=selected(profile.voice_tone, \"friendly\"),\n        voice_tone_casual=selected(profile.voice_tone, \"casual\"),\n        voice_tone_formal=selected(profile.voice_tone, \"formal\"),\n        voice_tone_confident=selected(profile.voice_tone, \"confident\"),\n        comm_style_direct=selected(profile.communication_style, \"direct\"),\n        comm_style_conversational=selected(profile.communication_style, \"conversational\"),\n        comm_style_storytelling=selected(profile.communication_style, \"storytelling\"),\n        comm_style_data=selected(profile.communication_style, \"data-driven\"),\n        constraints=profile.constraints or \"\",\n        primary_contact_name=profile.primary_contact_name or \"\",\n        primary_contact_email=profile.primary_contact_email or \"\",\n        outreach_mode_auto='selected=\"selected\"' if customer.outreach_mode == \"AUTO\" else \"\",\n        outreach_mode_review='selected=\"selected\"' if customer.outreach_mode == \"REVIEW\" else \"\",\n        autopilot_enabled_true='selected=\"selected\"' if customer.autopilot_enabled else \"\",\n        autopilot_enabled_false='selected=\"selected\"' if not customer.autopilot_enabled else \"\",\n        do_not_contact_list=profile.do_not_contact_list or \"\"\n    )\n    \n    return HTMLResponse(content=html)\n\n\n@app.post(\"/portal/cancel\")\ndef portal_cancel_subscription(request: Request, session: Session = Depends(get_session)):\n    \"\"\"\n    Cancel subscription at end of billing period.\n    \n    Only available for paid users with active subscription.\n    Sets cancelled_at_period_end = True but keeps subscription_status as \"active\"\n    so they can continue using the service until the billing period ends.\n    \"\"\"\n    session_token = request.cookies.get(SESSION_COOKIE_NAME)\n    customer = get_customer_from_session(session, session_token)\n    \n    if not customer:\n        return RedirectResponse(url=\"/login\", status_code=303)\n    \n    plan_status = get_customer_plan_status(customer)\n    \n    if not plan_status.is_paid:\n        return RedirectResponse(url=\"/portal?error=not_subscribed\", status_code=303)\n    \n    customer.cancelled_at_period_end = True\n    session.add(customer)\n    session.commit()\n    \n    track_funnel_event(EventType.CANCELLATION, customer_id=customer.id)\n    \n    print(f\"[PORTAL] Subscription cancellation scheduled for customer {customer.id}: {customer.company}\")\n    \n    return RedirectResponse(url=\"/portal?cancelled=true\", status_code=303)\n\n\n@app.post(\"/portal/reactivate\")\ndef portal_reactivate_subscription(request: Request, session: Session = Depends(get_session)):\n    \"\"\"\n    Reactivate a subscription that was scheduled for cancellation.\n    \n    Sets cancelled_at_period_end = False to resume the subscription.\n    \"\"\"\n    session_token = request.cookies.get(SESSION_COOKIE_NAME)\n    customer = get_customer_from_session(session, session_token)\n    \n    if not customer:\n        return RedirectResponse(url=\"/login\", status_code=303)\n    \n    plan_status = get_customer_plan_status(customer)\n    \n    if not plan_status.is_paid:\n        return RedirectResponse(url=\"/portal?error=not_subscribed\", status_code=303)\n    \n    customer.cancelled_at_period_end = False\n    customer.cancellation_effective_at = None\n    session.add(customer)\n    session.commit()\n    \n    print(f\"[PORTAL] Subscription reactivated for customer {customer.id}: {customer.company}\")\n    \n    return RedirectResponse(url=\"/portal?reactivated=true\", status_code=303)\n\n\n@app.post(\"/api/user/timezone\")\ndef save_user_timezone(request: Request, session: Session = Depends(get_session)):\n    \"\"\"\n    Save user's detected timezone from browser.\n    Called automatically on first page load to set timezone for localized timestamp display.\n    \"\"\"\n    import json as json_module\n    \n    session_token = request.cookies.get(SESSION_COOKIE_NAME)\n    customer = get_customer_from_session(session, session_token)\n    \n    if not customer:\n        return JSONResponse({\"status\": \"unauthenticated\"}, status_code=401)\n    \n    import asyncio\n    body = asyncio.get_event_loop().run_until_complete(request.body())\n    try:\n        data = json_module.loads(body)\n        timezone = data.get(\"timezone\")\n    except Exception:\n        return JSONResponse({\"status\": \"error\", \"message\": \"Invalid JSON\"}, status_code=400)\n    \n    if not timezone:\n        return JSONResponse({\"status\": \"error\", \"message\": \"No timezone provided\"}, status_code=400)\n    \n    try:\n        ZoneInfo(timezone)\n    except Exception:\n        return JSONResponse({\"status\": \"error\", \"message\": \"Invalid timezone\"}, status_code=400)\n    \n    customer.time_zone = timezone\n    session.add(customer)\n    session.commit()\n    \n    print(f\"[TIMEZONE] Saved timezone {timezone} for customer {customer.id}\")\n    \n    return JSONResponse({\"status\": \"ok\", \"timezone\": timezone})\n\n\n@app.post(\"/api/admin/timezone\")\ndef save_admin_timezone(request: Request):\n    \"\"\"\n    Save admin's detected timezone to a cookie (admin doesn't have a user record).\n    \"\"\"\n    import json as json_module\n    \n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        return JSONResponse({\"status\": \"unauthenticated\"}, status_code=401)\n    \n    import asyncio\n    body = asyncio.get_event_loop().run_until_complete(request.body())\n    try:\n        data = json_module.loads(body)\n        timezone = data.get(\"timezone\")\n    except Exception:\n        return JSONResponse({\"status\": \"error\", \"message\": \"Invalid JSON\"}, status_code=400)\n    \n    if not timezone:\n        return JSONResponse({\"status\": \"error\", \"message\": \"No timezone provided\"}, status_code=400)\n    \n    try:\n        ZoneInfo(timezone)\n    except Exception:\n        return JSONResponse({\"status\": \"error\", \"message\": \"Invalid timezone\"}, status_code=400)\n    \n    response = JSONResponse({\"status\": \"ok\", \"timezone\": timezone})\n    response.set_cookie(\n        key=\"admin_timezone\",\n        value=timezone,\n        max_age=60*60*24*365,\n        httponly=False,\n        samesite=\"lax\"\n    )\n    \n    print(f\"[TIMEZONE] Saved admin timezone {timezone}\")\n    \n    return response\n\n\n@app.post(\"/api/outreach/{outreach_id}/{action}\")\ndef handle_outreach_action(\n    outreach_id: int,\n    action: str,\n    request: Request,\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Handle pending outreach actions: approve, edit, or skip.\n    \n    Actions:\n    - approve: Send the email immediately\n    - edit: Mark for editing (redirect to edit page - for now just approve)\n    - skip: Mark as skipped\n    \"\"\"\n    session_token = request.cookies.get(SESSION_COOKIE_NAME)\n    customer = get_customer_from_session(session, session_token)\n    \n    if not customer:\n        return JSONResponse(status_code=401, content={\"success\": False, \"error\": \"Not authenticated\"})\n    \n    outreach = session.exec(\n        select(PendingOutbound).where(\n            PendingOutbound.id == outreach_id,\n            PendingOutbound.customer_id == customer.id\n        )\n    ).first()\n    \n    if not outreach:\n        return JSONResponse(status_code=404, content={\"success\": False, \"error\": \"Outreach not found\"})\n    \n    if outreach.status != \"PENDING\":\n        return JSONResponse(content={\"success\": False, \"error\": \"Already processed\"})\n    \n    if action == \"approve\" or action == \"edit\":\n        outreach.status = \"APPROVED\"\n        outreach.approved_at = datetime.utcnow()\n        \n        email_success = send_email(\n            to_email=outreach.to_email,\n            subject=outreach.subject,\n            body=outreach.body,\n            lead_name=outreach.to_name or \"\",\n            company=\"\"\n        )\n        \n        if email_success:\n            outreach.status = \"SENT\"\n            outreach.sent_at = datetime.utcnow()\n            print(f\"[OUTREACH] Email sent: {outreach.id} to {outreach.to_email}\")\n        else:\n            print(f\"[OUTREACH] Email queued (dry-run): {outreach.id}\")\n        \n        session.add(outreach)\n        session.commit()\n        return JSONResponse(content={\"success\": True, \"action\": \"sent\" if email_success else \"approved\"})\n    \n    elif action == \"skip\":\n        outreach.status = \"SKIPPED\"\n        outreach.skipped_reason = \"Skipped by customer\"\n        session.add(outreach)\n        session.commit()\n        print(f\"[OUTREACH] Skipped: {outreach.id}\")\n        return JSONResponse(content={\"success\": True, \"action\": \"skipped\"})\n    \n    return JSONResponse(status_code=400, content={\"success\": False, \"error\": \"Invalid action\"})\n\n\n@app.post(\"/api/message/{message_id}/{action}\")\ndef handle_message_draft_action(\n    message_id: int,\n    action: str,\n    request: Request,\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Handle message draft actions: approve or discard.\n    \n    Actions:\n    - approve: Send the AI-generated draft email\n    - discard: Delete the draft message\n    \"\"\"\n    session_token = request.cookies.get(SESSION_COOKIE_NAME)\n    customer = get_customer_from_session(session, session_token)\n    \n    if not customer:\n        return JSONResponse(status_code=401, content={\"success\": False, \"error\": \"Not authenticated\"})\n    \n    message = session.exec(\n        select(Message).where(\n            Message.id == message_id,\n            Message.customer_id == customer.id\n        )\n    ).first()\n    \n    if not message:\n        return JSONResponse(status_code=404, content={\"success\": False, \"error\": \"Message not found\"})\n    \n    if message.status != \"DRAFT\" or message.direction != \"OUTBOUND\":\n        return JSONResponse(content={\"success\": False, \"error\": \"Not a draft message\"})\n    \n    if action == \"approve\":\n        from email_utils import send_email\n        \n        email_success = send_email(\n            to_email=message.to_email,\n            subject=message.subject,\n            body=message.body,\n            lead_name=\"\",\n            company=\"\"\n        )\n        \n        if email_success:\n            message.status = \"SENT\"\n            message.sent_at = datetime.utcnow()\n            print(f\"[MESSAGE] Draft approved and sent: {message.id} to {message.to_email}\")\n        else:\n            message.status = \"QUEUED\"\n            print(f\"[MESSAGE] Draft approved (queued): {message.id}\")\n        \n        if message.thread_id:\n            thread = session.exec(select(Thread).where(Thread.id == message.thread_id)).first()\n            if thread:\n                thread.last_message_at = datetime.utcnow()\n                thread.last_direction = \"OUTBOUND\"\n                thread.last_summary = (message.subject or message.body[:100] if message.body else \"\")[:100]\n                thread.outbound_count += 1\n                thread.message_count += 1\n                session.add(thread)\n        \n        session.add(message)\n        session.commit()\n        return JSONResponse(content={\"success\": True, \"action\": \"sent\" if email_success else \"queued\"})\n    \n    elif action == \"discard\":\n        session.delete(message)\n        session.commit()\n        print(f\"[MESSAGE] Draft discarded: {message_id}\")\n        return JSONResponse(content={\"success\": True, \"action\": \"discarded\"})\n    \n    return JSONResponse(status_code=400, content={\"success\": False, \"error\": \"Invalid action\"})\n\n\n@app.post(\"/api/upgrade\")\ndef admin_upgrade_customer(\n    customer_id: int = Query(..., description=\"Customer ID to upgrade\"),\n    request: Request = None,\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Admin endpoint to upgrade a customer to paid plan via admin override.\n    \n    Sets:\n    - customer.plan = \"paid\"\n    - customer.subscription_status = \"active\"\n    - customer.billing_method = \"ADMIN_OVERRIDE\"\n    - Clears trial_end_at\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME) if request else None\n    if not verify_admin_session(admin_token):\n        return JSONResponse(status_code=403, content={\"success\": False, \"error\": \"Admin access required\"})\n    \n    customer = session.exec(\n        select(Customer).where(Customer.id == customer_id)\n    ).first()\n    \n    if not customer:\n        return JSONResponse(status_code=404, content={\"success\": False, \"error\": \"Customer not found\"})\n    \n    customer.plan = \"paid\"\n    customer.subscription_status = \"active\"\n    customer.billing_method = \"ADMIN_OVERRIDE\"\n    customer.trial_end_at = None\n    \n    session.add(customer)\n    session.commit()\n    \n    print(f\"[ADMIN] Customer {customer.id} ({customer.company}) upgraded to PAID via admin override\")\n    \n    return JSONResponse(content={\n        \"success\": True,\n        \"customer_id\": customer.id,\n        \"company\": customer.company,\n        \"plan\": customer.plan,\n        \"subscription_status\": customer.subscription_status,\n        \"billing_method\": customer.billing_method\n    })\n\n\n@app.get(\"/api/pending-outreach\")\ndef get_all_pending_outreach(\n    request: Request,\n    limit: int = Query(default=50, le=100),\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Get all pending outreach records across all customers (admin only).\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    outreach_records = session.exec(\n        select(PendingOutbound).order_by(PendingOutbound.created_at.desc()).limit(limit)\n    ).all()\n    \n    result = []\n    for po in outreach_records:\n        customer = session.exec(\n            select(Customer).where(Customer.id == po.customer_id)\n        ).first()\n        \n        result.append({\n            \"id\": po.id,\n            \"customer_id\": po.customer_id,\n            \"customer_company\": customer.company if customer else \"Unknown\",\n            \"to_email\": po.to_email,\n            \"subject\": po.subject,\n            \"status\": po.status,\n            \"created_at\": po.created_at.isoformat() if po.created_at else None\n        })\n    \n    return result\n\n\n@app.get(\"/customers/{customer_id}\", response_class=HTMLResponse)\ndef customer_detail(customer_id: int, session: Session = Depends(get_session)):\n    \"\"\"Customer detail page.\"\"\"\n    customer = session.exec(\n        select(Customer).where(Customer.id == customer_id)\n    ).first()\n    if not customer:\n        raise HTTPException(status_code=404, detail=\"Customer not found\")\n\n    leads = session.exec(\n        select(Lead).where(Lead.email == customer.contact_email)\n    ).all()\n    tasks = session.exec(\n        select(Task).where(Task.customer_id == customer_id)\n    ).all()\n    invoices = session.exec(\n        select(Invoice).where(Invoice.customer_id == customer_id)\n    ).all()\n\n    html = f\"\"\"\n    <!DOCTYPE html>\n    <html><head><meta charset=\"utf-8\"><title>{customer.company} - HossAgent</title>\n    <style>body{{background:#0a0a0a;color:#fff;font-family:Georgia,serif;padding:2rem}}</style>\n    </head><body>\n    <a href=\"/\"> Back to Dashboard</a>\n    <h1>{customer.company}</h1>\n    <p><strong>Email:</strong> {customer.contact_email}</p>\n    <p><strong>Plan:</strong> {customer.billing_plan}</p>\n    <p><strong>Status:</strong> {customer.status}</p>\n    <h2>Tasks ({len(tasks)})</h2>\n    <ul>\n    {''.join(f\"<li>Task {t.id}: {t.description} ({t.status}) - ${t.profit_cents/100:.2f}</li>\" for t in tasks)}\n    </ul>\n    <h2>Invoices ({len(invoices)})</h2>\n    <ul>\n    {''.join(f\"<li>Invoice {i.id}: ${i.amount_cents/100:.2f} ({i.status})</li>\" for i in invoices)}\n    </ul>\n    </body></html>\n    \"\"\"\n    return html\n\n\n@app.get(\"/leads/{lead_id}\", response_class=HTMLResponse)\ndef lead_detail(lead_id: int, session: Session = Depends(get_session)):\n    \"\"\"Lead detail page.\"\"\"\n    lead = session.exec(select(Lead).where(Lead.id == lead_id)).first()\n    if not lead:\n        raise HTTPException(status_code=404, detail=\"Lead not found\")\n\n    customer = session.exec(\n        select(Customer).where(Customer.contact_email == lead.email)\n    ).first()\n    tasks = []\n    if customer:\n        tasks = session.exec(\n            select(Task).where(Task.customer_id == customer.id)\n        ).all()\n\n    html = f\"\"\"\n    <!DOCTYPE html>\n    <html><head><meta charset=\"utf-8\"><title>{lead.company} - Lead</title>\n    <style>body{{background:#0a0a0a;color:#fff;font-family:Georgia,serif;padding:2rem}}</style>\n    </head><body>\n    <a href=\"/\"> Back to Dashboard</a>\n    <h1>{lead.company}</h1>\n    <p><strong>Contact:</strong> {lead.name} ({lead.email})</p>\n    <p><strong>Niche:</strong> {lead.niche}</p>\n    <p><strong>Status:</strong> {lead.status}</p>\n    <p><strong>Last Contacted:</strong> {lead.last_contacted_at.strftime(\"%Y-%m-%d %H:%M\") if lead.last_contacted_at else \"Never\"}</p>\n    {'<h2>Customer</h2><p>Company: ' + customer.company + ' (ID: ' + str(customer.id) + ')</p>' if customer else '<p><em>Not yet converted to customer.</em></p>'}\n    <h2>Tasks</h2>\n    <ul>\n    {''.join(f\"<li>Task {t.id}: {t.description} ({t.status})</li>\" for t in tasks) or '<li><em>None</em></li>'}\n    </ul>\n    </body></html>\n    \"\"\"\n    return html\n\n\n@app.get(\"/invoices/{invoice_id}\", response_class=HTMLResponse)\ndef invoice_detail(invoice_id: int, session: Session = Depends(get_session)):\n    \"\"\"Invoice detail page.\"\"\"\n    invoice = session.exec(\n        select(Invoice).where(Invoice.id == invoice_id)\n    ).first()\n    if not invoice:\n        raise HTTPException(status_code=404, detail=\"Invoice not found\")\n\n    customer = session.exec(\n        select(Customer).where(Customer.id == invoice.customer_id)\n    ).first()\n    tasks = session.exec(\n        select(Task).where(Task.customer_id == invoice.customer_id)\n    ).all()\n\n    html = f\"\"\"\n    <!DOCTYPE html>\n    <html><head><meta charset=\"utf-8\"><title>Invoice {invoice.id}</title>\n    <style>body{{background:#0a0a0a;color:#fff;font-family:Georgia,serif;padding:2rem}}</style>\n    </head><body>\n    <a href=\"/\"> Back to Dashboard</a>\n    <h1>Invoice {invoice.id}</h1>\n    <p><strong>Customer:</strong> {customer.company if customer else 'Unknown'}</p>\n    <p><strong>Amount:</strong> ${invoice.amount_cents/100:.2f}</p>\n    <p><strong>Status:</strong> {invoice.status}</p>\n    <p><strong>Created:</strong> {invoice.created_at.strftime(\"%Y-%m-%d %H:%M\")}</p>\n    <p><strong>Paid:</strong> {invoice.paid_at.strftime(\"%Y-%m-%d %H:%M\") if invoice.paid_at else \"\"}</p>\n    <p><strong>Notes:</strong> {invoice.notes or 'None'}</p>\n    <h2>Related Tasks</h2>\n    <ul>\n    {''.join(f\"<li>Task {t.id}: {t.description} - ${t.profit_cents/100:.2f}</li>\" for t in tasks) or '<li><em>None</em></li>'}\n    </ul>\n    </body></html>\n    \"\"\"\n    return html\n\n\n# ============================================================================\n# API ENDPOINTS - DATA RETRIEVAL\n# ============================================================================\n\n\n@app.get(\"/api/leads\")\ndef get_leads(session: Session = Depends(get_session)):\n    \"\"\"Get all leads.\"\"\"\n    leads = session.exec(select(Lead)).all()\n    return [\n        {\n            \"id\": l.id,\n            \"name\": l.name,\n            \"email\": l.email,\n            \"company\": l.company,\n            \"niche\": l.niche,\n            \"status\": l.status,\n            \"website\": l.website,\n            \"source\": l.source,\n            \"last_contacted_at\": l.last_contacted_at.isoformat()\n            if l.last_contacted_at\n            else None,\n            \"created_at\": l.created_at.isoformat(),\n        }\n        for l in leads\n    ]\n\n\n@app.get(\"/api/customers\")\ndef get_customers(session: Session = Depends(get_session)):\n    \"\"\"Get all customers.\"\"\"\n    customers = session.exec(select(Customer)).all()\n    return [\n        {\n            \"id\": c.id,\n            \"company\": c.company,\n            \"contact_email\": c.contact_email,\n            \"plan\": c.plan,\n            \"billing_plan\": c.billing_plan,\n            \"status\": c.status,\n            \"stripe_customer_id\": c.stripe_customer_id,\n            \"public_token\": c.public_token,\n            \"notes\": c.notes,\n            \"created_at\": c.created_at.isoformat(),\n        }\n        for c in customers\n    ]\n\n\n@app.get(\"/api/tasks\")\ndef get_tasks(session: Session = Depends(get_session)):\n    \"\"\"Get all tasks.\"\"\"\n    tasks = session.exec(select(Task)).all()\n    return [\n        {\n            \"id\": t.id,\n            \"customer_id\": t.customer_id,\n            \"description\": t.description,\n            \"status\": t.status,\n            \"reward_cents\": t.reward_cents,\n            \"cost_cents\": t.cost_cents,\n            \"profit_cents\": t.profit_cents,\n            \"result_summary\": t.result_summary,\n            \"created_at\": t.created_at.isoformat(),\n            \"completed_at\": t.completed_at.isoformat() if t.completed_at else None,\n        }\n        for t in tasks\n    ]\n\n\n@app.get(\"/api/invoices\")\ndef get_invoices(session: Session = Depends(get_session)):\n    \"\"\"Get all invoices.\"\"\"\n    invoices = session.exec(select(Invoice)).all()\n    return [\n        {\n            \"id\": i.id,\n            \"customer_id\": i.customer_id,\n            \"amount_cents\": i.amount_cents,\n            \"status\": i.status,\n            \"created_at\": i.created_at.isoformat(),\n            \"paid_at\": i.paid_at.isoformat() if i.paid_at else None,\n            \"notes\": i.notes,\n        }\n        for i in invoices\n    ]\n\n\n# ============================================================================\n# API ENDPOINTS - ADMIN CONTROLS\n# ============================================================================\n\n\n@app.post(\"/admin/autopilot\")\nasync def toggle_autopilot(enabled: bool, session: Session = Depends(get_session)):\n    \"\"\"Toggle autopilot mode on/off.\"\"\"\n    settings = session.exec(\n        select(SystemSettings).where(SystemSettings.id == 1)\n    ).first()\n    if settings:\n        settings.autopilot_enabled = enabled\n        session.add(settings)\n        session.commit()\n        status = \"enabled\" if enabled else \"disabled\"\n        print(f\"[ADMIN] Autopilot {status}\")\n        return {\"status\": f\"Autopilot {status}\"}\n    return {\"error\": \"SystemSettings not found\"}\n\n\n@app.post(\"/admin/outbound-autopilot\")\nasync def toggle_outbound_autopilot(enabled: bool, session: Session = Depends(get_session)):\n    \"\"\"Toggle outbound autopilot mode on/off. When OFF, BizDev stops auto-sending and leads wait for manual approval.\"\"\"\n    settings = session.exec(\n        select(SystemSettings).where(SystemSettings.id == 1)\n    ).first()\n    if settings:\n        settings.outbound_autopilot_enabled = enabled\n        session.add(settings)\n        session.commit()\n        status = \"enabled\" if enabled else \"disabled\"\n        print(f\"[ADMIN] Outbound Autopilot {status}\")\n        return {\"status\": f\"Outbound Autopilot {status}\", \"outbound_autopilot_enabled\": enabled}\n    return {\"error\": \"SystemSettings not found\"}\n\n\n@app.get(\"/api/settings\")\ndef get_settings(session: Session = Depends(get_session)):\n    \"\"\"Get current system settings including email and release mode configuration.\"\"\"\n    settings = session.exec(\n        select(SystemSettings).where(SystemSettings.id == 1)\n    ).first()\n    email_status = get_email_status()\n    release_status = get_release_mode_status()\n    \n    if settings:\n        return {\n            \"autopilot_enabled\": settings.autopilot_enabled,\n            \"outbound_autopilot_enabled\": getattr(settings, 'outbound_autopilot_enabled', True),\n            \"email\": email_status,\n            \"release_mode\": release_status\n        }\n    return {\"error\": \"Settings not found\", \"email\": email_status, \"release_mode\": release_status, \"outbound_autopilot_enabled\": True}\n\n\n@app.get(\"/api/email-log\")\ndef get_email_log_endpoint(limit: int = Query(default=10, le=50)):\n    \"\"\"Get recent email attempts for admin console display.\"\"\"\n    return {\"entries\": get_email_log(limit)}\n\n\n@app.get(\"/api/lead-source\")\ndef get_lead_source_endpoint():\n    \"\"\"\n    Get current lead source configuration and status.\n    \n    Returns:\n        - niche: Target ICP description\n        - geography: Geographic constraint (if any)\n        - provider: Current provider (HossNative)\n        - max_new_leads_per_cycle: Lead generation cap\n        - last_run: Timestamp of last lead generation run\n        - last_created_count: Number of leads created in last run\n    \"\"\"\n    status = get_lead_source_status()\n    log = get_lead_source_log()\n    \n    return {\n        **status,\n        \"last_run\": log.get(\"last_run\"),\n        \"last_created_count\": log.get(\"last_created_count\", 0),\n        \"runs\": log.get(\"runs\", [])[-10:],\n        \"recent_leads\": log.get(\"recent_leads\", [])[-10:]\n    }\n\n\n@app.post(\"/api/run/lead-source\")\ndef run_lead_source_manual(session: Session = Depends(get_session)):\n    \"\"\"Manually trigger lead source generation cycle.\"\"\"\n    message = generate_new_leads_from_source(session)\n    return {\"message\": message}\n\n\n# ============================================================================\n# API ENDPOINTS - AGENT EXECUTION (MANUAL TRIGGERS)\n# ============================================================================\n\n\n@app.post(\"/api/run/bizdev\")\nasync def run_bizdev(session: Session = Depends(get_session)):\n    \"\"\"Manually trigger BizDev cycle.\"\"\"\n    message = await run_bizdev_cycle(session)\n    return {\"message\": message}\n\n\n@app.post(\"/api/run/onboarding\")\nasync def run_onboarding(session: Session = Depends(get_session)):\n    \"\"\"Manually trigger Onboarding cycle.\"\"\"\n    message = await run_onboarding_cycle(session)\n    return {\"message\": message}\n\n\n@app.post(\"/api/run/ops\")\nasync def run_ops(session: Session = Depends(get_session)):\n    \"\"\"Manually trigger Ops cycle.\"\"\"\n    message = await run_ops_cycle(session)\n    return {\"message\": message}\n\n\n@app.post(\"/api/run/billing\")\nasync def run_billing(session: Session = Depends(get_session)):\n    \"\"\"Manually trigger Billing cycle.\"\"\"\n    message = await run_billing_cycle(session)\n    return {\"message\": message}\n\n\n@app.post(\"/api/run/signals\")\ndef run_signals_manual(request: Request, session: Session = Depends(get_session)):\n    \"\"\"\n    Manually trigger Signals Agent cycle (admin only).\n    \n    The Signals Agent:\n    - Monitors external context signals about companies\n    - Generates LeadEvents for moment-aware outreach\n    - Miami-tuned heuristics for South Florida market\n    \n    Returns:\n        - signals_created: Number of new signals generated\n        - events_created: Number of new lead events created\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    result = run_signals_agent(session)\n    return {\n        \"message\": f\"Signals: Created {result['signals_created']} signals, {result['events_created']} events\",\n        **result\n    }\n\n\n@app.post(\"/api/run/event-bizdev\")\nasync def run_event_bizdev(request: Request, session: Session = Depends(get_session)):\n    \"\"\"\n    Manually trigger Event-Driven BizDev cycle (admin only).\n    \n    Processes LeadEvents with status='new' and sends contextual Miami-style outreach.\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    message = await run_event_driven_bizdev_cycle(session)\n    return {\"message\": message}\n\n\n@app.get(\"/api/signals\")\ndef get_signals_endpoint(\n    request: Request,\n    limit: int = Query(default=20, le=100),\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Get all signals (admin only).\n    \n    Returns recent signals from the Signals Engine ordered by creation date.\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    signals = get_signals_summary(session, limit)\n    return [\n        {\n            \"id\": s.id,\n            \"company_id\": s.company_id,\n            \"lead_id\": s.lead_id,\n            \"source_type\": s.source_type,\n            \"context_summary\": s.context_summary,\n            \"geography\": s.geography,\n            \"created_at\": s.created_at.isoformat() if s.created_at else None\n        }\n        for s in signals\n    ]\n\n\n@app.get(\"/api/lead_events\")\ndef get_lead_events_endpoint(\n    request: Request,\n    limit: int = Query(default=20, le=100),\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Get all lead events (admin only).\n    \n    Returns recent lead events ordered by creation date.\n    Includes urgency_score, category, and status for filtering.\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    events = get_lead_events_summary(session, limit)\n    return [\n        {\n            \"id\": e.id,\n            \"company_id\": e.company_id,\n            \"lead_id\": e.lead_id,\n            \"signal_id\": e.signal_id,\n            \"summary\": e.summary,\n            \"category\": e.category,\n            \"urgency_score\": e.urgency_score,\n            \"status\": e.status,\n            \"enrichment_status\": e.enrichment_status,\n            \"enrichment_attempts\": e.enrichment_attempts,\n            \"max_enrichment_attempts\": getattr(e, 'max_enrichment_attempts', 3),\n            \"unenrichable_reason\": getattr(e, 'unenrichable_reason', None),\n            \"lead_company\": e.lead_company,\n            \"lead_domain\": e.lead_domain,\n            \"lead_email\": e.lead_email,\n            \"lead_phone_e164\": getattr(e, 'lead_phone_e164', None),\n            \"domain_confidence\": getattr(e, 'domain_confidence', 0),\n            \"email_confidence\": getattr(e, 'email_confidence', 0),\n            \"phone_confidence\": getattr(e, 'phone_confidence', 0),\n            \"recommended_action\": e.recommended_action,\n            \"outbound_message\": e.outbound_message,\n            \"created_at\": e.created_at.isoformat() if e.created_at else None\n        }\n        for e in events\n    ]\n\n\n@app.get(\"/api/enrichment/metrics\")\ndef get_enrichment_metrics_endpoint(\n    request: Request,\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Get enrichment metrics by source type (admin only).\n    \n    Returns enrichment success rates, domain/email/phone discovery counts,\n    and unenrichable breakdown by source.\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    from models import EnrichmentMetrics\n    \n    metrics = session.exec(\n        select(EnrichmentMetrics).order_by(EnrichmentMetrics.period_start.desc())\n    ).all()\n    \n    return [\n        {\n            \"id\": m.id,\n            \"source_type\": m.source_type,\n            \"total_leads\": m.total_leads or 0,\n            \"enriched_leads\": m.enriched_leads or 0,\n            \"enrichment_rate\": round(m.enrichment_rate or 0.0, 2),\n            \"domains_discovered\": m.domains_discovered or 0,\n            \"emails_discovered\": m.emails_discovered or 0,\n            \"phones_discovered\": m.phones_discovered or 0,\n            \"unenrichable_no_domain\": m.unenrichable_no_domain or 0,\n            \"unenrichable_no_contact\": m.unenrichable_no_contact or 0,\n            \"unenrichable_no_osint\": m.unenrichable_no_osint or 0,\n            \"outbound_sent\": m.outbound_sent or 0,\n            \"replies_received\": m.replies_received or 0,\n            \"reply_rate\": round(m.reply_rate or 0.0, 2),\n            \"period_start\": m.period_start.isoformat() if m.period_start else None,\n            \"last_updated_at\": m.last_updated_at.isoformat() if m.last_updated_at else None\n        }\n        for m in metrics\n    ]\n\n\n@app.get(\"/api/companies\")\ndef get_companies_endpoint(\n    request: Request,\n    limit: int = Query(default=50, le=200),\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Get all companies in the Company table (admin only).\n    \n    Returns canonical company entities with their enrichment status.\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    from models import Company\n    \n    companies = session.exec(\n        select(Company).order_by(Company.last_seen_at.desc()).limit(limit)\n    ).all()\n    \n    return [\n        {\n            \"id\": c.id,\n            \"name\": c.name,\n            \"normalized_name\": c.normalized_name,\n            \"domain\": c.domain,\n            \"geography\": c.geography,\n            \"niche\": c.niche,\n            \"source_type\": c.source_type,\n            \"source_confidence\": round(c.source_confidence or 0.0, 2),\n            \"enrichment_complete\": c.enrichment_complete or False,\n            \"enrichment_attempts\": c.enrichment_attempts or 0,\n            \"first_seen_at\": c.first_seen_at.isoformat() if c.first_seen_at else None,\n            \"last_seen_at\": c.last_seen_at.isoformat() if c.last_seen_at else None\n        }\n        for c in companies\n    ]\n\n\n@app.post(\"/api/invoices/{invoice_id}/mark-paid\")\ndef mark_invoice_paid(invoice_id: int, session: Session = Depends(get_session)):\n    \"\"\"Mark an invoice as paid (for testing).\"\"\"\n    invoice = session.exec(\n        select(Invoice).where(Invoice.id == invoice_id)\n    ).first()\n    if not invoice:\n        raise HTTPException(status_code=404, detail=\"Invoice not found\")\n    \n    invoice.status = \"paid\"\n    invoice.paid_at = datetime.utcnow()\n    session.add(invoice)\n    session.commit()\n    print(f\"[ADMIN] Invoice {invoice_id} marked as paid\")\n    return {\"status\": \"paid\", \"invoice_id\": invoice_id}\n\n\n@app.post(\"/admin/send-test-email\")\ndef send_test_email(\n    to_email: str = Query(..., description=\"Recipient email address\"),\n    subject: Optional[str] = Query(default=\"HossAgent Test Email\", description=\"Email subject\"),\n    body: Optional[str] = Query(default=None, description=\"Email body\")\n):\n    \"\"\"\n    Send a test email to verify configuration.\n    \n    Usage: POST /admin/send-test-email?to_email=your@email.com\n    \n    Returns JSON with:\n        - mode: Current email mode (DRY_RUN, SENDGRID, SMTP)\n        - to: Recipient address\n        - success: Whether email was actually sent\n        - message: Human-readable status\n    \"\"\"\n    email_status = get_email_status()\n    mode = email_status[\"mode\"]\n    \n    if body is None:\n        body = f\"\"\"This is a test email from HossAgent.\n\nYour outbound email system is configured and working.\n\nMode: {mode}\nSent at: {datetime.utcnow().isoformat()}\n\n- HossAgent\"\"\"\n    \n    success = send_email(\n        to_email=to_email,\n        subject=subject or \"HossAgent Test Email\",\n        body=body,\n        lead_name=\"Test\",\n        company=\"Test Email\"\n    )\n    \n    return {\n        \"success\": success,\n        \"mode\": mode,\n        \"to\": to_email,\n        \"message\": f\"Email {'sent successfully' if success else 'logged (dry-run mode)'} via {mode}\"\n    }\n\n\n@app.post(\"/admin/production-cleanup\")\ndef admin_production_cleanup(\n    request: Request,\n    owner_email_domain: str = Query(default=\"\", description=\"Domain to identify real customers (e.g., 'hossagent.net')\"),\n    purge_all_signals: bool = Query(default=True, description=\"If true, delete ALL signals and lead_events (fresh start)\"),\n    confirm: bool = Query(default=False, description=\"Set to true to actually run cleanup\"),\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    One-time production database cleanup.\n    \n    IMPORTANT: This should only be run ONCE during production initialization.\n    It removes all dev/test/demo data while preserving real production customers.\n    \n    Safety:\n    - Requires confirm=true to actually run\n    - Creates a flag file to prevent re-running\n    - Logs all deletions for auditability\n    - Produces an audit log file for compliance\n    \n    Usage:\n        POST /admin/production-cleanup?owner_email_domain=hossagent.net&purge_all_signals=true&confirm=true\n    \n    What it deletes:\n    - ALL signals and lead_events (if purge_all_signals=true)\n    - Signals/events from non-real customers\n    - Pending outbound from non-real customers\n    - Reports from non-real customers\n    - Tasks from non-real customers\n    - Invoices with $0 amounts or from non-real customers\n    - Leads with fake emails (Lead_*, @example, @test, etc.)\n    - Customers with demo company names\n    \n    What it preserves:\n    - Customers matching owner_email_domain\n    - Customers with ADMIN in notes\n    - Customers with Stripe integration\n    - Paid customers with active subscriptions\n    \n    Returns JSON summary of cleanup actions taken.\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token or \"\"):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    if not confirm:\n        cleanup_flag = Path(\"production_cleanup_completed.flag\")\n        already_run = cleanup_flag.exists()\n        \n        return {\n            \"success\": False,\n            \"message\": \"Dry run - set confirm=true to actually run cleanup\",\n            \"already_run\": already_run,\n            \"warning\": \"This will permanently delete dev/test data. Make sure you have a backup.\",\n            \"instructions\": {\n                \"owner_email_domain\": \"Set this to your email domain to identify real customers\",\n                \"purge_all_signals\": \"Set to true to delete ALL signals/lead_events (recommended for fresh start)\",\n                \"confirm\": \"Set to true to execute the cleanup\"\n            }\n        }\n    \n    results = run_production_cleanup(session, owner_email_domain, purge_all_signals)\n    \n    return {\n        \"success\": True,\n        \"message\": \"Production cleanup completed\" if not results[\"already_run\"] else \"Cleanup already run previously\",\n        **results\n    }\n\n\n@app.get(\"/admin/production-status\")\ndef admin_production_status(\n    request: Request,\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Get production readiness status.\n    \n    Returns current configuration and readiness for production operation.\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token or \"\"):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    email_status = get_email_status()\n    cleanup_flag = Path(\"production_cleanup_completed.flag\")\n    \n    settings = session.exec(select(SystemSettings).where(SystemSettings.id == 1)).first()\n    \n    customers = session.exec(select(Customer)).all()\n    paid_customers = [c for c in customers if c.plan == \"paid\"]\n    trial_customers = [c for c in customers if c.plan == \"trial\"]\n    \n    return {\n        \"email\": {\n            \"mode\": email_status[\"mode\"],\n            \"configured_mode\": email_status[\"configured_mode\"],\n            \"is_valid\": email_status[\"is_valid\"],\n            \"message\": email_status[\"message\"],\n            \"hourly_status\": email_status[\"hourly\"]\n        },\n        \"autopilot\": {\n            \"global_enabled\": settings.autopilot_enabled if settings else False,\n            \"cycle_interval\": \"15 minutes\"\n        },\n        \"cleanup\": {\n            \"completed\": cleanup_flag.exists(),\n            \"flag_file\": str(cleanup_flag)\n        },\n        \"customers\": {\n            \"total\": len(customers),\n            \"paid\": len(paid_customers),\n            \"trial\": len(trial_customers)\n        },\n        \"production_ready\": (\n            email_status[\"is_valid\"] and \n            email_status[\"mode\"] != \"DRY_RUN\" and\n            cleanup_flag.exists()\n        ),\n        \"recommendations\": []\n    }\n\n\n@app.post(\"/admin/regenerate-payment-links\")\ndef admin_regenerate_payment_links(\n    max_invoices: int = Query(default=100, description=\"Maximum invoices to process\"),\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Regenerate Stripe payment links for all unpaid invoices missing them.\n    \n    Use this endpoint to:\n    - Fix invoices created before Stripe was enabled\n    - Regenerate links after Stripe configuration changes\n    - Bulk update invoices missing payment links\n    \n    Returns JSON summary:\n        - invoices_processed: Number of invoices checked\n        - links_created: Number of new payment links generated\n        - links_failed: Number of link creation failures\n        - invoices_skipped: Number of invoices skipped (already have links or are paid)\n        - details: Per-invoice breakdown\n    \"\"\"\n    from stripe_utils import ensure_invoice_payment_url, is_stripe_enabled, validate_stripe_config\n    \n    is_valid, config_msg = validate_stripe_config()\n    if not is_valid:\n        return {\n            \"success\": False,\n            \"error\": config_msg,\n            \"invoices_processed\": 0,\n            \"links_created\": 0,\n            \"links_failed\": 0,\n            \"invoices_skipped\": 0,\n            \"details\": []\n        }\n    \n    invoices = session.exec(\n        select(Invoice).where(\n            Invoice.status.in_([\"draft\", \"sent\"])\n        ).limit(max_invoices)\n    ).all()\n    \n    results = {\n        \"success\": True,\n        \"invoices_processed\": len(invoices),\n        \"links_created\": 0,\n        \"links_failed\": 0,\n        \"invoices_skipped\": 0,\n        \"details\": []\n    }\n    \n    for invoice in invoices:\n        if invoice.payment_url and len(invoice.payment_url) > 10:\n            results[\"invoices_skipped\"] += 1\n            results[\"details\"].append({\n                \"invoice_id\": invoice.id,\n                \"status\": \"skipped\",\n                \"reason\": \"Already has payment link\"\n            })\n            continue\n        \n        customer = session.exec(\n            select(Customer).where(Customer.id == invoice.customer_id)\n        ).first()\n        \n        if not customer:\n            results[\"invoices_skipped\"] += 1\n            results[\"details\"].append({\n                \"invoice_id\": invoice.id,\n                \"status\": \"skipped\",\n                \"reason\": \"No customer found\"\n            })\n            continue\n        \n        try:\n            result = ensure_invoice_payment_url(\n                invoice_id=invoice.id,\n                amount_cents=invoice.amount_cents,\n                customer_id=customer.id,\n                customer_email=customer.contact_email,\n                customer_company=customer.company,\n                invoice_status=invoice.status,\n                existing_payment_url=invoice.payment_url\n            )\n            \n            if result.success and result.payment_url:\n                invoice.payment_url = result.payment_url\n                stripe_id = getattr(result, 'stripe_id', None)\n                if stripe_id:\n                    invoice.stripe_payment_id = stripe_id\n                session.add(invoice)\n                results[\"links_created\"] += 1\n                url_preview = result.payment_url[:50] + \"...\" if len(result.payment_url) > 50 else result.payment_url\n                results[\"details\"].append({\n                    \"invoice_id\": invoice.id,\n                    \"status\": \"created\",\n                    \"payment_url\": url_preview\n                })\n            elif result.mode == \"dry_run\":\n                results[\"invoices_skipped\"] += 1\n                results[\"details\"].append({\n                    \"invoice_id\": invoice.id,\n                    \"status\": \"skipped\",\n                    \"reason\": f\"DRY_RUN: {result.error or 'Stripe not available'}\"\n                })\n            else:\n                results[\"links_failed\"] += 1\n                results[\"details\"].append({\n                    \"invoice_id\": invoice.id,\n                    \"status\": \"failed\",\n                    \"error\": result.error or \"Unknown error\"\n                })\n        except Exception as e:\n            results[\"links_failed\"] += 1\n            results[\"details\"].append({\n                \"invoice_id\": invoice.id,\n                \"status\": \"failed\",\n                \"error\": str(e)\n            })\n            print(f\"[ADMIN][REGENERATE][ERROR] Invoice {invoice.id}: {e}\")\n    \n    session.commit()\n    \n    print(f\"[ADMIN][REGENERATE] Processed {results['invoices_processed']} invoices: \"\n          f\"{results['links_created']} created, {results['links_failed']} failed, \"\n          f\"{results['invoices_skipped']} skipped\")\n    \n    return results\n\n\n# ============================================================================\n# CONVERSATION ENGINE ENDPOINTS\n# ============================================================================\n\n\nfrom conversation_engine import (\n    validate_inbound_secret, parse_sendgrid_inbound, process_inbound_email,\n    send_queued_messages, approve_draft, edit_and_approve_draft, discard_draft,\n    set_thread_status, get_thread_summary, get_customer_threads, calculate_customer_metrics,\n    THREAD_STATUS_OPEN, THREAD_STATUS_HUMAN_OWNED, THREAD_STATUS_AUTO, THREAD_STATUS_CLOSED,\n    InboundEmailData\n)\n\n\n@app.post(\"/email/inbound\")\nasync def inbound_email_webhook(request: Request, session: Session = Depends(get_session)):\n    \"\"\"\n    Handle inbound email from SendGrid Inbound Parse webhook.\n    \n    Parses incoming email, matches to thread/customer, stores message,\n    generates AI draft reply if applicable.\n    \n    Requires INBOUND_EMAIL_SECRET env var for validation (optional but recommended).\n    \"\"\"\n    try:\n        form_data = await request.form()\n        request_data = {key: value for key, value in form_data.items()}\n        \n        provided_secret = request.headers.get(\"X-Inbound-Secret\", \"\")\n        if not provided_secret:\n            provided_secret = request_data.get(\"secret\", \"\")\n        \n        if not validate_inbound_secret(provided_secret):\n            print(\"[INBOUND][WEBHOOK] Invalid secret\")\n            raise HTTPException(status_code=401, detail=\"Invalid secret\")\n        \n        email_data = parse_sendgrid_inbound(request_data)\n        \n        print(f\"[INBOUND][WEBHOOK] Received from {email_data.from_email} to {email_data.to_email}\")\n        print(f\"[INBOUND][WEBHOOK] Subject: {email_data.subject}\")\n        \n        result = process_inbound_email(session, email_data)\n        \n        if result[\"success\"]:\n            print(f\"[INBOUND][WEBHOOK] Processed: thread={result['thread_id']}, message={result['message_id']}, actions={result['actions']}\")\n            return JSONResponse({\n                \"status\": \"processed\",\n                \"thread_id\": result[\"thread_id\"],\n                \"message_id\": result[\"message_id\"],\n                \"actions\": result[\"actions\"]\n            })\n        else:\n            print(f\"[INBOUND][WEBHOOK] Failed: {result['error']}\")\n            return JSONResponse({\n                \"status\": \"failed\",\n                \"error\": result[\"error\"],\n                \"actions\": result.get(\"actions\", [])\n            }, status_code=200)\n            \n    except HTTPException:\n        raise\n    except Exception as e:\n        print(f\"[INBOUND][WEBHOOK] Error: {e}\")\n        return JSONResponse({\n            \"status\": \"error\",\n            \"error\": str(e)\n        }, status_code=500)\n\n\n@app.get(\"/api/conversations/threads\")\ndef api_get_threads(\n    request: Request,\n    customer_id: int = Query(None),\n    status: str = Query(None),\n    limit: int = Query(50),\n    session: Session = Depends(get_session)\n):\n    \"\"\"Get conversation threads, optionally filtered by customer and status.\"\"\"\n    session_token = request.cookies.get(SESSION_COOKIE_NAME)\n    customer = get_customer_from_session(session, session_token) if session_token else None\n    \n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    is_admin = verify_admin_session(admin_token) if admin_token else False\n    \n    if not customer and not is_admin:\n        raise HTTPException(status_code=401, detail=\"Authentication required\")\n    \n    if customer and not is_admin:\n        customer_id = customer.id\n    \n    if customer_id:\n        threads = get_customer_threads(session, customer_id, status, limit)\n    else:\n        query = select(Thread).order_by(Thread.updated_at.desc()).limit(limit)\n        if status:\n            query = query.where(Thread.status == status)\n        all_threads = session.exec(query).all()\n        threads = [\n            {\n                \"id\": t.id,\n                \"customer_id\": t.customer_id,\n                \"lead_email\": t.lead_email,\n                \"lead_name\": t.lead_name,\n                \"status\": t.status,\n                \"message_count\": t.message_count,\n                \"last_message_at\": t.last_message_at.isoformat() if t.last_message_at else None,\n                \"last_direction\": t.last_direction,\n                \"last_summary\": t.last_summary\n            }\n            for t in all_threads\n        ]\n    \n    return {\"threads\": threads, \"count\": len(threads)}\n\n\n@app.get(\"/api/conversations/thread/{thread_id}\")\ndef api_get_thread(request: Request, thread_id: int, session: Session = Depends(get_session)):\n    \"\"\"Get thread details with all messages.\"\"\"\n    session_token = request.cookies.get(SESSION_COOKIE_NAME)\n    customer = get_customer_from_session(session, session_token) if session_token else None\n    \n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    is_admin = verify_admin_session(admin_token) if admin_token else False\n    \n    if not customer and not is_admin:\n        raise HTTPException(status_code=401, detail=\"Authentication required\")\n    \n    summary = get_thread_summary(session, thread_id)\n    if not summary:\n        raise HTTPException(status_code=404, detail=\"Thread not found\")\n    \n    if customer and not is_admin and summary.get(\"customer_id\") != customer.id:\n        raise HTTPException(status_code=403, detail=\"Access denied\")\n    \n    return summary\n\n\n@app.post(\"/api/conversations/thread/{thread_id}/status\")\ndef api_set_thread_status(\n    request: Request,\n    thread_id: int,\n    status: str = Query(..., description=\"OPEN, HUMAN_OWNED, AUTO, or CLOSED\"),\n    session: Session = Depends(get_session)\n):\n    \"\"\"Update thread status.\"\"\"\n    session_token = request.cookies.get(SESSION_COOKIE_NAME)\n    customer = get_customer_from_session(session, session_token) if session_token else None\n    \n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    is_admin = verify_admin_session(admin_token) if admin_token else False\n    \n    if not customer and not is_admin:\n        raise HTTPException(status_code=401, detail=\"Authentication required\")\n    \n    if status not in [THREAD_STATUS_OPEN, THREAD_STATUS_HUMAN_OWNED, THREAD_STATUS_AUTO, THREAD_STATUS_CLOSED]:\n        raise HTTPException(status_code=400, detail=f\"Invalid status: {status}\")\n    \n    thread = session.exec(select(Thread).where(Thread.id == thread_id)).first()\n    if not thread:\n        raise HTTPException(status_code=404, detail=\"Thread not found\")\n    \n    if customer and not is_admin and thread.customer_id != customer.id:\n        raise HTTPException(status_code=403, detail=\"Access denied\")\n    \n    success = set_thread_status(session, thread_id, status)\n    if not success:\n        raise HTTPException(status_code=404, detail=\"Thread not found\")\n    \n    return {\"status\": \"updated\", \"thread_id\": thread_id, \"new_status\": status}\n\n\n@app.get(\"/api/conversations/drafts\")\ndef api_get_drafts(\n    request: Request,\n    customer_id: int = Query(None),\n    limit: int = Query(50),\n    session: Session = Depends(get_session)\n):\n    \"\"\"Get pending draft messages awaiting approval.\"\"\"\n    session_token = request.cookies.get(SESSION_COOKIE_NAME)\n    customer = get_customer_from_session(session, session_token) if session_token else None\n    \n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    is_admin = verify_admin_session(admin_token) if admin_token else False\n    \n    if not customer and not is_admin:\n        raise HTTPException(status_code=401, detail=\"Authentication required\")\n    \n    if customer and not is_admin:\n        customer_id = customer.id\n    \n    query = select(Message).where(Message.status == MESSAGE_STATUS_DRAFT)\n    if customer_id:\n        query = query.where(Message.customer_id == customer_id)\n    query = query.order_by(Message.created_at.desc()).limit(limit)\n    \n    drafts = session.exec(query).all()\n    \n    return {\n        \"drafts\": [\n            {\n                \"id\": m.id,\n                \"thread_id\": m.thread_id,\n                \"customer_id\": m.customer_id,\n                \"to_email\": m.to_email,\n                \"subject\": m.subject,\n                \"body_text\": m.body_text,\n                \"generated_by\": m.generated_by,\n                \"guardrail_flags\": json.loads(m.guardrail_flags) if m.guardrail_flags else None,\n                \"created_at\": m.created_at.isoformat() if m.created_at else None\n            }\n            for m in drafts\n        ],\n        \"count\": len(drafts)\n    }\n\n\n@app.post(\"/api/conversations/draft/{message_id}/approve\")\ndef api_approve_draft(request: Request, message_id: int, session: Session = Depends(get_session)):\n    \"\"\"Approve a draft message for sending.\"\"\"\n    session_token = request.cookies.get(SESSION_COOKIE_NAME)\n    customer = get_customer_from_session(session, session_token) if session_token else None\n    \n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    is_admin = verify_admin_session(admin_token) if admin_token else False\n    \n    if not customer and not is_admin:\n        raise HTTPException(status_code=401, detail=\"Authentication required\")\n    \n    msg = session.exec(select(Message).where(Message.id == message_id)).first()\n    if not msg:\n        raise HTTPException(status_code=404, detail=\"Draft not found\")\n    \n    if customer and not is_admin and msg.customer_id != customer.id:\n        raise HTTPException(status_code=403, detail=\"Access denied\")\n    \n    success = approve_draft(session, message_id)\n    if not success:\n        raise HTTPException(status_code=404, detail=\"Draft not found or already processed\")\n    return {\"status\": \"approved\", \"message_id\": message_id}\n\n\n@app.post(\"/api/conversations/draft/{message_id}/edit\")\ndef api_edit_draft(\n    request: Request,\n    message_id: int,\n    body_text: str = Form(...),\n    subject: str = Form(None),\n    session: Session = Depends(get_session)\n):\n    \"\"\"Edit and approve a draft message.\"\"\"\n    session_token = request.cookies.get(SESSION_COOKIE_NAME)\n    customer = get_customer_from_session(session, session_token) if session_token else None\n    \n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    is_admin = verify_admin_session(admin_token) if admin_token else False\n    \n    if not customer and not is_admin:\n        raise HTTPException(status_code=401, detail=\"Authentication required\")\n    \n    msg = session.exec(select(Message).where(Message.id == message_id)).first()\n    if not msg:\n        raise HTTPException(status_code=404, detail=\"Draft not found\")\n    \n    if customer and not is_admin and msg.customer_id != customer.id:\n        raise HTTPException(status_code=403, detail=\"Access denied\")\n    \n    success = edit_and_approve_draft(session, message_id, body_text, subject)\n    if not success:\n        raise HTTPException(status_code=404, detail=\"Draft not found or already processed\")\n    return {\"status\": \"edited_and_approved\", \"message_id\": message_id}\n\n\n@app.post(\"/api/conversations/draft/{message_id}/discard\")\ndef api_discard_draft(request: Request, message_id: int, session: Session = Depends(get_session)):\n    \"\"\"Discard a draft message.\"\"\"\n    session_token = request.cookies.get(SESSION_COOKIE_NAME)\n    customer = get_customer_from_session(session, session_token) if session_token else None\n    \n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    is_admin = verify_admin_session(admin_token) if admin_token else False\n    \n    if not customer and not is_admin:\n        raise HTTPException(status_code=401, detail=\"Authentication required\")\n    \n    msg = session.exec(select(Message).where(Message.id == message_id)).first()\n    if not msg:\n        raise HTTPException(status_code=404, detail=\"Draft not found\")\n    \n    if customer and not is_admin and msg.customer_id != customer.id:\n        raise HTTPException(status_code=403, detail=\"Access denied\")\n    \n    success = discard_draft(session, message_id)\n    if not success:\n        raise HTTPException(status_code=404, detail=\"Draft not found or already processed\")\n    return {\"status\": \"discarded\", \"message_id\": message_id}\n\n\n@app.post(\"/api/conversations/send-queued\")\ndef api_send_queued(\n    request: Request,\n    max_messages: int = Query(10),\n    session: Session = Depends(get_session)\n):\n    \"\"\"Send queued messages (admin only).\"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    results = send_queued_messages(session, max_messages)\n    return {\n        \"sent\": len([r for r in results if r.get(\"status\") == \"sent\"]),\n        \"failed\": len([r for r in results if r.get(\"status\") == \"failed\"]),\n        \"results\": results\n    }\n\n\n@app.get(\"/api/conversations/metrics/{customer_id}\")\ndef api_get_metrics(request: Request, customer_id: int, session: Session = Depends(get_session)):\n    \"\"\"Get conversation metrics for a customer.\"\"\"\n    session_token = request.cookies.get(SESSION_COOKIE_NAME)\n    customer = get_customer_from_session(session, session_token) if session_token else None\n    \n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    is_admin = verify_admin_session(admin_token) if admin_token else False\n    \n    if not customer and not is_admin:\n        raise HTTPException(status_code=401, detail=\"Authentication required\")\n    \n    if customer and not is_admin and customer.id != customer_id:\n        raise HTTPException(status_code=403, detail=\"Access denied\")\n    \n    metrics = calculate_customer_metrics(session, customer_id)\n    return {\n        \"customer_id\": customer_id,\n        \"total_lead_events\": metrics.total_lead_events,\n        \"total_threads\": metrics.total_threads,\n        \"leads_contacted\": metrics.leads_contacted,\n        \"leads_replied\": metrics.leads_replied,\n        \"reply_rate_pct\": round(metrics.reply_rate_pct, 1),\n        \"avg_response_time_seconds\": metrics.avg_response_time_seconds,\n        \"total_outbound\": metrics.total_outbound,\n        \"total_inbound\": metrics.total_inbound,\n        \"messages_ai_drafted\": metrics.messages_ai_drafted,\n        \"messages_human_sent\": metrics.messages_human_sent,\n        \"avg_thread_depth\": round(metrics.avg_thread_depth, 1),\n        \"last_calculated_at\": metrics.last_calculated_at.isoformat() if metrics.last_calculated_at else None\n    }\n\n\n# ============================================================================\n# STRIPE WEBHOOK\n# ============================================================================\n\n\n@app.post(\"/stripe/webhook\")\nasync def stripe_webhook(request: Request, session: Session = Depends(get_session)):\n    \"\"\"\n    Handle Stripe webhook events.\n    \n    Validates webhook signature and processes payment events.\n    Updates invoice status when payment is completed.\n    \n    Supported events:\n      - checkout.session.completed: Payment link checkout completed\n      - payment_intent.succeeded: Direct payment succeeded\n      - invoice.paid: Stripe invoice marked paid (if using Stripe invoices)\n    \n    Returns:\n      200 with status on success\n      400 on invalid signature\n      500 on processing error (but doesn't crash app)\n    \"\"\"\n    from stripe_utils import verify_webhook_signature, log_stripe_event, get_stripe_webhook_secret\n    \n    try:\n        payload = await request.body()\n    except Exception as e:\n        print(f\"[STRIPE][WEBHOOK] Failed to read request body: {e}\")\n        raise HTTPException(status_code=400, detail=\"Invalid request body\")\n    \n    signature = request.headers.get(\"Stripe-Signature\", \"\")\n    \n    if not signature:\n        print(\"[STRIPE][WEBHOOK] Missing Stripe-Signature header\")\n        log_stripe_event(\"webhook_missing_signature\", {})\n        raise HTTPException(status_code=400, detail=\"Missing signature header\")\n    \n    webhook_secret = get_stripe_webhook_secret()\n    if not webhook_secret:\n        print(\"[STRIPE][WEBHOOK] No webhook secret configured - accepting event without verification\")\n        log_stripe_event(\"webhook_received_no_secret\", {\"warning\": \"Unverified - no secret configured\"})\n    elif not verify_webhook_signature(payload, signature):\n        print(\"[STRIPE][WEBHOOK] Invalid signature - rejecting event\")\n        log_stripe_event(\"webhook_invalid_signature\", {})\n        raise HTTPException(status_code=400, detail=\"Invalid signature\")\n    \n    import json\n    try:\n        event = json.loads(payload)\n        event_type = event.get(\"type\", \"unknown\")\n        event_id = event.get(\"id\", \"unknown\")\n        event_data = event.get(\"data\", {}).get(\"object\", {})\n        \n        print(f\"[STRIPE][WEBHOOK] Received event: {event_type} (id={event_id})\")\n        log_stripe_event(f\"webhook_{event_type}\", {\n            \"event_id\": event_id,\n            \"type\": event_type\n        })\n        \n        invoice_updated = False\n        invoice_id = None\n        \n        if event_type in [\"checkout.session.completed\", \"payment_intent.succeeded\", \"invoice.paid\"]:\n            metadata = event_data.get(\"metadata\", {})\n            invoice_id = metadata.get(\"invoice_id\")\n            \n            if not invoice_id and event_type == \"checkout.session.completed\":\n                invoice_id = event_data.get(\"client_reference_id\")\n            \n            stripe_amount = event_data.get(\"amount_total\") or event_data.get(\"amount\") or event_data.get(\"amount_paid\")\n            stripe_currency = (event_data.get(\"currency\") or \"\").lower()\n            stripe_status = event_data.get(\"status\") or event_data.get(\"payment_status\")\n            \n            from stripe_utils import get_default_currency\n            expected_currency = get_default_currency()\n            \n            payment_successful = False\n            if event_type == \"checkout.session.completed\":\n                payment_successful = stripe_status in [\"complete\", \"paid\"] or event_data.get(\"payment_status\") == \"paid\"\n            elif event_type == \"payment_intent.succeeded\":\n                payment_successful = stripe_status == \"succeeded\" or event_type == \"payment_intent.succeeded\"\n            elif event_type == \"invoice.paid\":\n                payment_successful = stripe_status == \"paid\" or event_data.get(\"paid\") == True\n            \n            if not invoice_id:\n                print(f\"[STRIPE][WEBHOOK] No invoice_id in event metadata - cannot process\")\n                log_stripe_event(\"webhook_missing_invoice_id\", {\"event_type\": event_type})\n            elif not payment_successful:\n                print(f\"[STRIPE][WEBHOOK] Payment not confirmed (status={stripe_status}) - not marking as paid\")\n                log_stripe_event(\"webhook_payment_not_confirmed\", {\n                    \"invoice_id\": invoice_id,\n                    \"status\": stripe_status,\n                    \"event_type\": event_type\n                })\n            else:\n                try:\n                    invoice = session.exec(\n                        select(Invoice).where(Invoice.id == int(invoice_id))\n                    ).first()\n                    \n                    if not invoice:\n                        print(f\"[STRIPE][WEBHOOK] Invoice {invoice_id} not found in database\")\n                        log_stripe_event(\"webhook_invoice_not_found\", {\"invoice_id\": invoice_id})\n                    elif invoice.status == \"paid\":\n                        print(f\"[STRIPE][WEBHOOK] Invoice {invoice_id} already paid - no action\")\n                    elif stripe_amount is not None and stripe_amount != invoice.amount_cents:\n                        print(f\"[STRIPE][WEBHOOK][SECURITY] Amount mismatch for invoice {invoice_id}: expected {invoice.amount_cents}, got {stripe_amount}\")\n                        log_stripe_event(\"webhook_amount_mismatch\", {\n                            \"invoice_id\": invoice_id,\n                            \"expected_amount\": invoice.amount_cents,\n                            \"received_amount\": stripe_amount\n                        })\n                    elif stripe_currency and stripe_currency != expected_currency:\n                        print(f\"[STRIPE][WEBHOOK][SECURITY] Currency mismatch for invoice {invoice_id}: expected {expected_currency}, got {stripe_currency}\")\n                        log_stripe_event(\"webhook_currency_mismatch\", {\n                            \"invoice_id\": invoice_id,\n                            \"expected_currency\": expected_currency,\n                            \"received_currency\": stripe_currency\n                        })\n                    else:\n                        invoice.status = \"paid\"\n                        invoice.paid_at = datetime.utcnow()\n                        session.add(invoice)\n                        session.commit()\n                        invoice_updated = True\n                        print(f\"[STRIPE][WEBHOOK] Invoice {invoice_id} marked as PAID (amount=${invoice.amount_cents/100:.2f}, currency={stripe_currency or expected_currency})\")\n                        log_stripe_event(\"invoice_paid\", {\n                            \"invoice_id\": invoice_id,\n                            \"amount_cents\": invoice.amount_cents,\n                            \"stripe_amount\": stripe_amount,\n                            \"stripe_currency\": stripe_currency,\n                            \"event_type\": event_type\n                        })\n                except ValueError:\n                    print(f\"[STRIPE][WEBHOOK] Invalid invoice_id format: {invoice_id}\")\n        \n        return {\n            \"status\": \"processed\",\n            \"event_type\": event_type,\n            \"event_id\": event_id,\n            \"invoice_updated\": invoice_updated,\n            \"invoice_id\": invoice_id\n        }\n        \n    except json.JSONDecodeError as e:\n        print(f\"[STRIPE][WEBHOOK] Invalid JSON payload: {e}\")\n        log_stripe_event(\"webhook_invalid_json\", {\"error\": str(e)})\n        raise HTTPException(status_code=400, detail=\"Invalid JSON payload\")\n    except Exception as e:\n        print(f\"[STRIPE][WEBHOOK] Error processing event: {e}\")\n        log_stripe_event(\"webhook_error\", {\"error\": str(e)})\n        return JSONResponse(\n            status_code=200,\n            content={\"status\": \"error\", \"message\": \"Processing failed but acknowledged\"}\n        )\n\n\n# ============================================================================\n# STRIPE STATUS API\n# ============================================================================\n\n\n@app.get(\"/api/stripe/status\")\ndef get_stripe_status_endpoint(session: Session = Depends(get_session)):\n    \"\"\"Get current Stripe configuration status including payment link stats.\"\"\"\n    from stripe_utils import get_stripe_status, get_stripe_log\n    \n    status = get_stripe_status()\n    recent_events = get_stripe_log(10)\n    \n    all_invoices = list(session.exec(select(Invoice)).all())\n    invoice_stats = get_invoice_payment_stats(all_invoices)\n    \n    return {\n        **status,\n        **invoice_stats,\n        \"recent_events\": recent_events\n    }\n\n\n# ============================================================================\n# SUBSCRIPTION MANAGEMENT\n# ============================================================================\n\n\n@app.get(\"/api/subscription/status\")\ndef get_subscription_status_endpoint():\n    \"\"\"Get current subscription configuration status.\"\"\"\n    return get_subscription_status()\n\n\n@app.get(\"/api/customer/{customer_id}/plan\")\ndef get_customer_plan_endpoint(customer_id: int, session: Session = Depends(get_session)):\n    \"\"\"Get plan status for a specific customer.\"\"\"\n    customer = session.exec(\n        select(Customer).where(Customer.id == customer_id)\n    ).first()\n    \n    if not customer:\n        raise HTTPException(status_code=404, detail=\"Customer not found\")\n    \n    plan_status = get_customer_plan_status(customer)\n    \n    return {\n        \"customer_id\": customer_id,\n        \"company\": customer.company,\n        \"plan\": plan_status.plan,\n        \"is_trial\": plan_status.is_trial,\n        \"is_paid\": plan_status.is_paid,\n        \"is_expired\": plan_status.is_expired,\n        \"days_remaining\": plan_status.days_remaining,\n        \"tasks_used\": plan_status.tasks_used,\n        \"tasks_limit\": plan_status.tasks_limit,\n        \"leads_used\": plan_status.leads_used,\n        \"leads_limit\": plan_status.leads_limit,\n        \"can_run_tasks\": plan_status.can_run_tasks,\n        \"can_generate_leads\": plan_status.can_generate_leads,\n        \"can_send_real_email\": plan_status.can_send_real_email,\n        \"can_use_billing\": plan_status.can_use_billing,\n        \"can_use_autopilot\": plan_status.can_use_autopilot,\n        \"upgrade_required\": plan_status.upgrade_required,\n        \"status_message\": plan_status.status_message\n    }\n\n\n@app.post(\"/upgrade\")\ndef upgrade_customer(\n    customer_id: int = Query(..., description=\"Customer ID to upgrade\"),\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Upgrade a customer from trial to paid plan.\n    \n    Creates Stripe customer and subscription, then updates customer plan.\n    \n    Returns:\n        Success/failure with subscription details\n    \"\"\"\n    customer = session.exec(\n        select(Customer).where(Customer.id == customer_id)\n    ).first()\n    \n    if not customer:\n        raise HTTPException(status_code=404, detail=\"Customer not found\")\n    \n    plan_status = get_customer_plan_status(customer)\n    \n    if plan_status.is_paid:\n        return {\n            \"success\": True,\n            \"message\": \"Customer already on paid plan\",\n            \"customer_id\": customer_id,\n            \"plan\": \"paid\"\n        }\n    \n    if not is_stripe_enabled():\n        customer = upgrade_to_paid(customer, stripe_subscription_id=None)\n        session.add(customer)\n        session.commit()\n        \n        print(f\"[UPGRADE] Customer {customer_id} upgraded to paid (Stripe disabled)\")\n        return {\n            \"success\": True,\n            \"message\": \"Upgraded to paid plan (Stripe disabled - manual billing)\",\n            \"customer_id\": customer_id,\n            \"plan\": \"paid\",\n            \"stripe_subscription_id\": None\n        }\n    \n    stripe_customer_id = customer.stripe_customer_id\n    if not stripe_customer_id:\n        stripe_customer_id, error = create_stripe_customer(\n            customer_id=customer.id,\n            email=customer.contact_email,\n            company=customer.company\n        )\n        \n        if error:\n            print(f\"[UPGRADE][ERROR] Failed to create Stripe customer: {error}\")\n            return {\n                \"success\": False,\n                \"error\": f\"Failed to create Stripe customer: {error}\",\n                \"customer_id\": customer_id\n            }\n        \n        customer.stripe_customer_id = stripe_customer_id\n        session.add(customer)\n        session.flush()\n    \n    if not stripe_customer_id:\n        return {\n            \"success\": False,\n            \"error\": \"Stripe customer ID missing\",\n            \"customer_id\": customer_id\n        }\n    \n    subscription_id, error = create_subscription(\n        stripe_customer_id=stripe_customer_id,\n        customer_id=customer.id\n    )\n    \n    if error:\n        print(f\"[UPGRADE][ERROR] Failed to create subscription: {error}\")\n        return {\n            \"success\": False,\n            \"error\": f\"Failed to create subscription: {error}\",\n            \"customer_id\": customer_id,\n            \"stripe_customer_id\": stripe_customer_id\n        }\n    \n    if not subscription_id:\n        return {\n            \"success\": False,\n            \"error\": \"Failed to get subscription ID\",\n            \"customer_id\": customer_id,\n            \"stripe_customer_id\": stripe_customer_id\n        }\n    \n    customer = upgrade_to_paid(customer, stripe_subscription_id=subscription_id)\n    session.add(customer)\n    session.commit()\n    \n    print(f\"[UPGRADE] Customer {customer_id} upgraded to paid plan, subscription ...{subscription_id[-4:]}\")\n    log_stripe_event(\"customer_upgraded\", {\n        \"customer_id\": customer_id,\n        \"stripe_customer_id\": stripe_customer_id,\n        \"subscription_id\": subscription_id\n    })\n    \n    return {\n        \"success\": True,\n        \"message\": \"Upgraded to paid plan - $99/month subscription active\",\n        \"customer_id\": customer_id,\n        \"plan\": \"paid\",\n        \"stripe_customer_id\": stripe_customer_id,\n        \"stripe_subscription_id\": subscription_id\n    }\n\n\n@app.post(\"/stripe/subscription-webhook\")\nasync def stripe_subscription_webhook(request: Request, session: Session = Depends(get_session)):\n    \"\"\"\n    Handle Stripe subscription webhook events.\n    \n    Validates webhook signature and processes subscription events.\n    Updates customer plan status based on subscription changes.\n    \n    Supported events:\n      - checkout.session.completed: Customer completed Stripe Checkout\n      - invoice.payment_succeeded: Subscription payment succeeded\n      - customer.subscription.updated: Subscription status changed\n      - customer.subscription.deleted: Subscription canceled\n    \n    Returns:\n      200 with status on success\n      400 on invalid signature\n    \"\"\"\n    try:\n        payload = await request.body()\n    except Exception as e:\n        print(f\"[STRIPE][SUBSCRIPTION-WEBHOOK] Failed to read request body: {e}\")\n        raise HTTPException(status_code=400, detail=\"Invalid request body\")\n    \n    signature = request.headers.get(\"Stripe-Signature\", \"\")\n    \n    if not signature:\n        print(\"[STRIPE][SUBSCRIPTION-WEBHOOK] Missing Stripe-Signature header\")\n        log_stripe_event(\"subscription_webhook_missing_signature\", {})\n        raise HTTPException(status_code=400, detail=\"Missing signature header\")\n    \n    webhook_secret = get_stripe_webhook_secret()\n    if not webhook_secret:\n        print(\"[STRIPE][SUBSCRIPTION-WEBHOOK] No webhook secret - accepting unverified\")\n    elif not verify_webhook_signature(payload, signature):\n        print(\"[STRIPE][SUBSCRIPTION-WEBHOOK] Invalid signature - rejecting\")\n        log_stripe_event(\"subscription_webhook_invalid_signature\", {})\n        raise HTTPException(status_code=400, detail=\"Invalid signature\")\n    \n    try:\n        event = json.loads(payload)\n        event_type = event.get(\"type\", \"unknown\")\n        event_id = event.get(\"id\", \"unknown\")\n        event_data = event.get(\"data\", {}).get(\"object\", {})\n        \n        print(f\"[STRIPE][SUBSCRIPTION-WEBHOOK] Received: {event_type} (id={event_id})\")\n        \n        result = process_subscription_webhook(event_type, event_data)\n        \n        if result.success and result.customer_id:\n            customer = session.exec(\n                select(Customer).where(Customer.id == result.customer_id)\n            ).first()\n            \n            if customer:\n                if result.action == \"subscription_canceled\":\n                    customer = expire_trial(customer)\n                    print(f\"[STRIPE][SUBSCRIPTION-WEBHOOK] Customer {customer.id} subscription canceled - plan set to trial_expired\")\n                elif result.new_status == \"active\":\n                    customer.subscription_status = \"active\"\n                    if customer.plan != \"paid\":\n                        customer.plan = \"paid\"\n                        print(f\"[STRIPE][SUBSCRIPTION-WEBHOOK] Customer {customer.id} set to paid\")\n                elif result.new_status in [\"past_due\", \"canceled\", \"unpaid\"]:\n                    customer.subscription_status = result.new_status\n                    print(f\"[STRIPE][SUBSCRIPTION-WEBHOOK] Customer {customer.id} subscription status: {result.new_status}\")\n                \n                session.add(customer)\n                session.commit()\n        \n        return {\n            \"status\": \"processed\",\n            \"event_type\": event_type,\n            \"event_id\": event_id,\n            \"action\": result.action,\n            \"customer_id\": result.customer_id,\n            \"new_status\": result.new_status\n        }\n        \n    except json.JSONDecodeError as e:\n        print(f\"[STRIPE][SUBSCRIPTION-WEBHOOK] Invalid JSON: {e}\")\n        raise HTTPException(status_code=400, detail=\"Invalid JSON\")\n    except Exception as e:\n        print(f\"[STRIPE][SUBSCRIPTION-WEBHOOK] Error: {e}\")\n        return JSONResponse(\n            status_code=200,\n            content={\"status\": \"error\", \"message\": \"Processing failed but acknowledged\"}\n        )\n\n\n# ============================================================================\n# RELEASE MODE & SUMMARY\n# ============================================================================\n\n\n@app.get(\"/api/release-mode\")\ndef get_release_mode_endpoint():\n    \"\"\"Get current release mode configuration status.\"\"\"\n    return get_release_mode_status()\n\n\n@app.get(\"/admin/summary\")\ndef get_admin_summary(\n    hours: int = Query(default=24, ge=1, le=168),\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Get summary of system activity for the last N hours.\n    \n    Default 24 hours, max 168 (one week).\n    \n    Returns aggregated stats for leads, emails, tasks, invoices, and payments.\n    \"\"\"\n    cutoff = datetime.utcnow() - timedelta(hours=hours)\n    \n    leads_new = session.exec(\n        select(func.count()).select_from(Lead).where(Lead.created_at >= cutoff)\n    ).one()\n    leads_contacted = session.exec(\n        select(func.count()).select_from(Lead).where(\n            (Lead.status == \"contacted\") & (Lead.created_at >= cutoff)\n        )\n    ).one()\n    leads_converted = session.exec(\n        select(func.count()).select_from(Lead).where(\n            (Lead.status == \"converted\") & (Lead.created_at >= cutoff)\n        )\n    ).one()\n    leads_failed = session.exec(\n        select(func.count()).select_from(Lead).where(\n            (Lead.status == \"email_failed\") & (Lead.created_at >= cutoff)\n        )\n    ).one()\n    \n    tasks_completed = session.exec(\n        select(func.count()).select_from(Task).where(\n            (Task.status == \"completed\") & (Task.created_at >= cutoff)\n        )\n    ).one()\n    tasks_profit = session.exec(\n        select(func.coalesce(func.sum(Task.profit_cents), 0)).select_from(Task).where(\n            (Task.status == \"completed\") & (Task.created_at >= cutoff)\n        )\n    ).one()\n    \n    invoices_generated = session.exec(\n        select(func.count()).select_from(Invoice).where(Invoice.created_at >= cutoff)\n    ).one()\n    invoices_paid = session.exec(\n        select(func.count()).select_from(Invoice).where(\n            (Invoice.status == \"paid\") & (Invoice.paid_at >= cutoff)\n        )\n    ).one()\n    revenue_cents = session.exec(\n        select(func.coalesce(func.sum(Invoice.amount_cents), 0)).select_from(Invoice).where(\n            (Invoice.status == \"paid\") & (Invoice.paid_at >= cutoff)\n        )\n    ).one()\n    \n    email_log = get_email_log(100)\n    emails_in_period = [e for e in email_log if datetime.fromisoformat(e.get(\"timestamp\", \"2000-01-01\")) >= cutoff]\n    emails_sent = len([e for e in emails_in_period if e.get(\"status\") == \"sent\"])\n    emails_failed = len([e for e in emails_in_period if e.get(\"status\") == \"failed\"])\n    emails_dry_run = len([e for e in emails_in_period if e.get(\"mode\") == \"dry_run\"])\n    \n    return {\n        \"period\": {\n            \"hours\": hours,\n            \"start\": cutoff.isoformat(),\n            \"end\": datetime.utcnow().isoformat()\n        },\n        \"leads\": {\n            \"new\": leads_new,\n            \"contacted\": leads_contacted,\n            \"converted\": leads_converted,\n            \"email_failed\": leads_failed\n        },\n        \"emails\": {\n            \"sent\": emails_sent,\n            \"failed\": emails_failed,\n            \"dry_run\": emails_dry_run\n        },\n        \"tasks\": {\n            \"completed\": tasks_completed,\n            \"profit_cents\": tasks_profit\n        },\n        \"invoices\": {\n            \"generated\": invoices_generated,\n            \"paid\": invoices_paid,\n            \"revenue_cents\": revenue_cents\n        },\n        \"release_mode\": is_release_mode(),\n        \"generated_at\": datetime.utcnow().isoformat()\n    }\n\n\n# ============================================================================\n# CUSTOMER PORTAL HELPER\n# ============================================================================\n\n\ndef render_customer_portal(customer: Customer, request: Request, session: Session) -> HTMLResponse:\n    \"\"\"\n    Render customer portal for a given customer.\n    \n    Clean 3-section layout:\n    1. Account Status (top card)\n    2. Recent Opportunities & Outreach (combined view)\n    3. Reports & Deep Dives\n    \"\"\"\n    import html as html_module\n    \n    plan_status = get_customer_plan_status(customer)\n    \n    invoices = session.exec(\n        select(Invoice).where(Invoice.customer_id == customer.id).order_by(Invoice.created_at.desc()).limit(5)\n    ).all()\n    outstanding_invoices = [i for i in invoices if i.status in [\"draft\", \"sent\"]]\n    total_outstanding = sum(i.amount_cents for i in outstanding_invoices)\n    \n    if plan_status.is_paid:\n        plan_name = \"HossAgent Pro\"\n        status_class = \"active\"\n        status_label = \"Active\"\n        billing_info = \"$99/month - Full access\"\n        if customer.cancelled_at_period_end:\n            billing_info = \"Cancels at end of billing period\"\n            account_cta = f'''<a href=\"/portal/reactivate\" class=\"account-cta\" onclick=\"event.preventDefault(); document.getElementById('reactivate-form').submit();\">Reactivate</a>\n            <a href=\"/billing/{customer.public_token}\" class=\"account-cta secondary\" style=\"margin-left: 0.5rem;\">Manage Billing</a>\n            <form id=\"reactivate-form\" action=\"/portal/reactivate\" method=\"POST\" style=\"display:none;\"></form>'''\n        else:\n            if total_outstanding > 0:\n                billing_info = f\"$99/month - ${total_outstanding/100:.0f} outstanding\"\n            account_cta = f'''<a href=\"/billing/{customer.public_token}\" class=\"account-cta secondary\">Manage Billing</a>\n            <a href=\"/portal/cancel\" class=\"account-cta secondary\" style=\"margin-left: 0.5rem; color: var(--accent-red);\" onclick=\"event.preventDefault(); if(confirm('Cancel your subscription? Access continues until end of billing period.')) document.getElementById('cancel-form').submit();\">Cancel</a>\n            <form id=\"cancel-form\" action=\"/portal/cancel\" method=\"POST\" style=\"display:none;\"></form>'''\n    elif plan_status.is_expired:\n        plan_name = \"Trial\"\n        status_class = \"paused\"\n        status_label = \"Expired\"\n        billing_info = f\"Trial ended - {plan_status.tasks_used}/{plan_status.tasks_limit} tasks, {plan_status.leads_used}/{plan_status.leads_limit} leads used\"\n        account_cta = f'''<a href=\"/subscribe/{customer.public_token}\" class=\"account-cta\">Start Subscription - $99/month</a>'''\n    else:\n        plan_name = \"Trial\"\n        status_class = \"trial\"\n        status_label = f\"{plan_status.days_remaining} days left\"\n        limit_text = f\"{plan_status.tasks_used}/{plan_status.tasks_limit} tasks, {plan_status.leads_used}/{plan_status.leads_limit} leads\"\n        billing_info = f\"Trial ends in {plan_status.days_remaining} days ({limit_text})\"\n        account_cta = f'''<a href=\"/subscribe/{customer.public_token}\" class=\"account-cta\">Upgrade to Pro - $99/month</a>'''\n    \n    autopilot_class = \"autopilot-on\" if customer.autopilot_enabled else \"autopilot-off\"\n    autopilot_label = \"ON\" if customer.autopilot_enabled else \"OFF\"\n    \n    payment_banner = \"\"\n    query_params = dict(request.query_params) if hasattr(request, 'query_params') else {}\n    if query_params.get(\"payment\") == \"success\":\n        payment_banner = '<div class=\"payment-success\">Payment successful! Your subscription is now active.</div>'\n    elif query_params.get(\"payment\") == \"cancelled\":\n        payment_banner = '<div class=\"payment-cancelled\">Payment was cancelled. You can try again when ready.</div>'\n    elif query_params.get(\"cancelled\") == \"true\":\n        payment_banner = '<div class=\"payment-cancelled\">Your subscription will remain active until the end of this billing period.</div>'\n    elif query_params.get(\"reactivated\") == \"true\":\n        payment_banner = '<div class=\"payment-success\">Your subscription has been reactivated!</div>'\n    \n    settings = session.exec(select(SystemSettings).where(SystemSettings.id == 1)).first()\n    outbound_autopilot_off = settings and not getattr(settings, 'outbound_autopilot_enabled', True)\n    \n    awaiting_approval_count = 0\n    if outbound_autopilot_off:\n        awaiting_approval_count = session.exec(\n            select(func.count(LeadEvent.id))\n            .where(LeadEvent.company_id == customer.id)\n            .where(LeadEvent.enrichment_status == ENRICHMENT_STATUS_ENRICHED_NO_OUTBOUND)\n        ).one()\n    \n    approval_banner = \"\"\n    if outbound_autopilot_off and awaiting_approval_count > 0:\n        approval_banner = f'''\n        <div class=\"approval-banner\">\n            <div class=\"approval-banner-text\">\n                <span class=\"approval-count\">{awaiting_approval_count}</span>\n                <span>Awaiting Your Approval - Outbound autopilot is OFF. Review leads below to send emails manually.</span>\n            </div>\n        </div>\n        '''\n    \n    business_profile = session.exec(\n        select(BusinessProfile).where(BusinessProfile.customer_id == customer.id)\n    ).first()\n    is_review_mode = customer and customer.outreach_mode == OUTREACH_MODE_REVIEW\n    \n    if is_review_mode:\n        allowed_statuses = [ENRICHMENT_STATUS_OUTBOUND_SENT, ENRICHMENT_STATUS_ENRICHED_NO_OUTBOUND]\n    else:\n        allowed_statuses = [ENRICHMENT_STATUS_OUTBOUND_SENT]\n    \n    total_opportunities = session.exec(\n        select(func.count(LeadEvent.id))\n        .where(LeadEvent.company_id == customer.id)\n        .where(LeadEvent.enrichment_status.in_(allowed_statuses))\n    ).one()\n    \n    opportunities = session.exec(\n        select(LeadEvent)\n        .where(LeadEvent.company_id == customer.id)\n        .where(LeadEvent.enrichment_status.in_(allowed_statuses))\n        .order_by(LeadEvent.created_at.desc())\n        .limit(30)\n    ).all()\n    \n    pending_outreach = session.exec(\n        select(PendingOutbound).where(\n            PendingOutbound.customer_id == customer.id,\n            PendingOutbound.status == \"PENDING\"\n        ).order_by(PendingOutbound.created_at.desc()).limit(10)\n    ).all()\n    pending_map = {po.lead_event_id: po for po in pending_outreach if po.lead_event_id}\n    \n    user_tz = customer.time_zone if customer else None\n    \n    if opportunities:\n        opp_cards = \"\"\n        for opp in opportunities:\n            timestamp = format_local_time(opp.created_at, user_tz) if opp.created_at else \"-\"\n            company_name = html_module.escape(opp.lead_company or opp.summary[:40] or \"Unknown Lead\")\n            signal_summary = html_module.escape(opp.summary[:120] if opp.summary else \"Opportunity identified\")\n            \n            if opp.status.upper() == \"CONTACTED\":\n                status_class_opp = \"sent\"\n                status_text = \"Email Sent\"\n            elif opp.status.upper() == \"RESPONDED\":\n                status_class_opp = \"responded\"\n                status_text = \"Responded\"\n            elif opp.do_not_contact:\n                status_class_opp = \"suppressed\"\n                status_text = \"Suppressed\"\n            elif opp.status.upper() in [\"CLOSED\", \"CLOSED_WON\", \"CLOSED_LOST\"]:\n                status_class_opp = \"closed\"\n                status_text = \"Closed\"\n            else:\n                status_class_opp = \"new\"\n                status_text = \"New\"\n            \n            outbound = session.exec(\n                select(PendingOutbound).where(\n                    PendingOutbound.lead_event_id == opp.id\n                ).order_by(PendingOutbound.created_at.desc()).limit(1)\n            ).first()\n            \n            lead_contact_email = opp.lead_email or opp.enriched_email or \"\"\n            lead_contact_name = opp.lead_name or opp.enriched_contact_name or \"\"\n            lead_contact_company = opp.lead_company or opp.enriched_company_name or \"\"\n            lead_contact_domain = opp.lead_domain or \"\"\n            lead_contact_phone = opp.lead_phone_e164 or \"\"\n            lead_phone_type = opp.phone_type or \"\"\n            \n            contact_info_html = \"\"\n            if lead_contact_email or lead_contact_company or lead_contact_phone:\n                contact_parts = []\n                if lead_contact_name:\n                    contact_parts.append(f\"<strong>{html_module.escape(lead_contact_name)}</strong>\")\n                if lead_contact_company:\n                    contact_parts.append(html_module.escape(lead_contact_company))\n                if lead_contact_email:\n                    contact_parts.append(f'<a href=\"mailto:{html_module.escape(lead_contact_email)}\" style=\"color: var(--accent-green);\">{html_module.escape(lead_contact_email)}</a>')\n                if lead_contact_domain:\n                    contact_parts.append(f'<a href=\"https://{html_module.escape(lead_contact_domain)}\" target=\"_blank\" style=\"color: var(--text-secondary);\">{html_module.escape(lead_contact_domain)}</a>')\n                \n                phone_html = \"\"\n                if lead_contact_phone:\n                    phone_type_badge = \"\"\n                    if lead_phone_type == \"mobile\":\n                        phone_type_badge = '<span style=\"font-size: 0.65rem; background: rgba(34, 197, 94, 0.15); color: var(--accent-green); padding: 0.15rem 0.4rem; border-radius: 4px; margin-left: 0.5rem;\">Mobile</span>'\n                    elif lead_phone_type == \"landline\":\n                        phone_type_badge = '<span style=\"font-size: 0.65rem; background: rgba(59, 130, 246, 0.15); color: var(--accent-blue); padding: 0.15rem 0.4rem; border-radius: 4px; margin-left: 0.5rem;\">Landline</span>'\n                    elif lead_phone_type == \"voip\":\n                        phone_type_badge = '<span style=\"font-size: 0.65rem; background: rgba(168, 85, 247, 0.15); color: #a855f7; padding: 0.15rem 0.4rem; border-radius: 4px; margin-left: 0.5rem;\">VoIP</span>'\n                    elif lead_phone_type == \"tollfree\":\n                        phone_type_badge = '<span style=\"font-size: 0.65rem; background: rgba(245, 158, 11, 0.15); color: var(--accent-orange); padding: 0.15rem 0.4rem; border-radius: 4px; margin-left: 0.5rem;\">Toll-Free</span>'\n                    \n                    phone_html = f'''\n                    <div style=\"margin-top: 0.5rem; padding-top: 0.5rem; border-top: 1px solid var(--border-subtle);\">\n                        <div style=\"display: flex; align-items: center; gap: 0.5rem;\">\n                            <a href=\"tel:{html_module.escape(lead_contact_phone)}\" style=\"color: var(--accent-blue); text-decoration: none; font-weight: 500;\">{html_module.escape(lead_contact_phone)}</a>\n                            {phone_type_badge}\n                        </div>\n                    </div>\n                    '''\n                \n                contact_info_html = f'''\n                <div class=\"lead-contact-info\" style=\"background: var(--bg-tertiary); border-radius: 6px; padding: 0.75rem; margin-bottom: 0.75rem; border-left: 3px solid var(--accent-green);\">\n                    <div style=\"font-size: 0.7rem; color: var(--text-tertiary); margin-bottom: 0.25rem; text-transform: uppercase; letter-spacing: 0.5px;\">Lead Contact</div>\n                    <div style=\"font-size: 0.85rem; color: var(--text-primary); line-height: 1.5;\">{\" | \".join(contact_parts)}</div>\n                    {phone_html}\n                </div>\n                '''\n            \n            source_url = None\n            if opp.signal_id:\n                signal = session.exec(select(Signal).where(Signal.id == opp.signal_id)).first()\n                if signal and signal.raw_payload:\n                    try:\n                        payload = json.loads(signal.raw_payload)\n                        source_url = payload.get(\"url\") or payload.get(\"source_url\") or payload.get(\"link\")\n                    except (json.JSONDecodeError, TypeError):\n                        pass\n            \n            context_html = \"\"\n            if opp.summary:\n                why_relevant = \"This company appeared in the news indicating potential growth or change.\"\n                if \"expand\" in opp.summary.lower():\n                    why_relevant = \"They are expanding operations - a key indicator they may need your services.\"\n                elif \"roofing\" in opp.summary.lower() or \"ac\" in opp.summary.lower() or \"hvac\" in opp.summary.lower():\n                    why_relevant = \"They are in a service industry with potential synergies to your business.\"\n                elif \"competitor\" in opp.summary.lower():\n                    why_relevant = \"A competitor shift has been detected - opportunity to position your services.\"\n                elif \"new\" in opp.summary.lower() and (\"headquarters\" in opp.summary.lower() or \"office\" in opp.summary.lower()):\n                    why_relevant = \"They are opening new locations - a strong signal they may need local services.\"\n                \n                source_link_html = \"\"\n                if source_url:\n                    source_link_html = f'<div style=\"font-size: 0.8rem; margin-top: 0.5rem;\"><a href=\"{html_module.escape(source_url)}\" target=\"_blank\" style=\"color: var(--accent-green); text-decoration: none;\">View Source Story</a></div>'\n                \n                context_html = f'''\n                <div class=\"opportunity-context\" style=\"background: var(--bg-secondary); border-radius: 6px; padding: 0.75rem; margin-bottom: 0.75rem; border: 1px solid var(--border-subtle);\">\n                    <div style=\"font-size: 0.7rem; color: var(--text-tertiary); margin-bottom: 0.5rem; text-transform: uppercase; letter-spacing: 0.5px;\">Why This Opportunity</div>\n                    <div style=\"font-size: 0.85rem; color: var(--text-secondary); line-height: 1.5;\">{html_module.escape(why_relevant)}</div>\n                    <div style=\"font-size: 0.8rem; color: var(--text-tertiary); margin-top: 0.5rem;\"><strong>Category:</strong> {html_module.escape(opp.category or 'general')}</div>\n                    {source_link_html}\n                </div>\n                '''\n            \n            next_steps_html = \"\"\n            if opp.status.upper() == \"CONTACTED\":\n                phone_option = \"\"\n                if lead_contact_phone:\n                    first_name = (lead_contact_name.split()[0] if lead_contact_name else \"there\")\n                    signal_context = opp.summary[:50] if opp.summary else \"your recent activity\"\n                    sms_suggestion = f\"Hey {first_name}, Sam Holliday here in Miami - saw {signal_context}. Wanted to share a quick local insight; is this the right number?\"\n                    phone_option = f'''\n                    <li style=\"margin-top: 0.5rem;\">\n                        <strong>Phone Contact:</strong> <a href=\"tel:{html_module.escape(lead_contact_phone)}\" style=\"color: var(--accent-blue);\">Call directly</a>\n                        <div style=\"margin-top: 0.25rem; padding: 0.5rem; background: var(--bg-tertiary); border-radius: 4px; font-size: 0.75rem; color: var(--text-tertiary);\">\n                            <strong>Suggested SMS:</strong> \"{html_module.escape(sms_suggestion)}\"\n                        </div>\n                    </li>\n                    '''\n                \n                next_steps_html = f'''\n                <div class=\"next-steps\" style=\"background: rgba(34, 197, 94, 0.1); border-radius: 6px; padding: 0.75rem; margin-top: 0.75rem; border: 1px solid rgba(34, 197, 94, 0.2);\">\n                    <div style=\"font-size: 0.7rem; color: var(--accent-green); margin-bottom: 0.5rem; text-transform: uppercase; letter-spacing: 0.5px;\">Recommended Next Steps</div>\n                    <ul style=\"font-size: 0.8rem; color: var(--text-secondary); margin: 0; padding-left: 1.25rem; line-height: 1.6;\">\n                        <li>Wait 2-3 business days for a response</li>\n                        <li>If no reply, a follow-up will be sent automatically</li>\n                        <li>Use the contact info above to reach out directly if urgent</li>\n                        {phone_option}\n                    </ul>\n                </div>\n                '''\n            \n            email_detail = \"\"\n            if outbound and outbound.status == \"SENT\":\n                email_detail = f'''\n                {contact_info_html}\n                {context_html}\n                <div class=\"email-preview\">\n                    <div class=\"email-header\">\n                        <span class=\"email-to\">To: {html_module.escape(outbound.to_email)}</span>\n                        <span class=\"email-sent-badge\">Sent</span>\n                    </div>\n                    <div class=\"email-subject\">Subject: {html_module.escape(outbound.subject or \"\")}</div>\n                    <div class=\"email-body\">{html_module.escape(outbound.body or \"\")}</div>\n                </div>\n                {next_steps_html}\n                '''\n            elif outbound and outbound.status == \"PENDING\" and customer.outreach_mode == \"REVIEW\":\n                email_detail = f'''\n                {contact_info_html}\n                {context_html}\n                <div class=\"email-preview\" style=\"border-left-color: var(--accent-orange);\">\n                    <div class=\"email-header\">\n                        <span class=\"email-to\">To: {html_module.escape(outbound.to_email)}</span>\n                        <span class=\"email-sent-badge\" style=\"background: rgba(245, 158, 11, 0.15); color: var(--accent-orange);\">Awaiting Your Approval</span>\n                    </div>\n                    <div class=\"email-subject\">Subject: {html_module.escape(outbound.subject or \"\")}</div>\n                    <div class=\"email-body\">{html_module.escape(outbound.body or \"\")}</div>\n                    <div class=\"approval-actions\" style=\"margin-top: 1rem; padding-top: 0.75rem; border-top: 1px solid var(--border-subtle); display: flex; gap: 0.5rem; flex-wrap: wrap;\">\n                        <button onclick=\"event.stopPropagation(); handleOutreach({outbound.id}, 'approve')\" style=\"background: var(--accent-green); color: #000; border: none; padding: 0.5rem 1rem; border-radius: 6px; cursor: pointer; font-size: 0.8rem; font-weight: 500;\">Approve & Send</button>\n                        <button onclick=\"event.stopPropagation(); handleOutreach({outbound.id}, 'skip')\" style=\"background: transparent; color: var(--text-secondary); border: 1px solid var(--border-medium); padding: 0.5rem 1rem; border-radius: 6px; cursor: pointer; font-size: 0.8rem;\">Skip</button>\n                    </div>\n                </div>\n                '''\n            elif outbound and outbound.status in [\"APPROVED\", \"PENDING\"]:\n                status_badge = \"Queued\" if outbound.status == \"APPROVED\" else \"Pending\"\n                email_detail = f'''\n                {contact_info_html}\n                {context_html}\n                <div class=\"email-preview\" style=\"border-left-color: var(--accent-orange);\">\n                    <div class=\"email-header\">\n                        <span class=\"email-to\">To: {html_module.escape(outbound.to_email)}</span>\n                        <span class=\"email-sent-badge\" style=\"background: rgba(245, 158, 11, 0.15); color: var(--accent-orange);\">{status_badge}</span>\n                    </div>\n                    <div class=\"email-subject\">Subject: {html_module.escape(outbound.subject or \"\")}</div>\n                    <div class=\"email-body\">{html_module.escape(outbound.body or \"\")}</div>\n                </div>\n                '''\n            elif outbound and outbound.status == \"SKIPPED\":\n                email_detail = f'''\n                {contact_info_html}\n                <div class=\"email-preview\" style=\"border-left-color: var(--text-tertiary); opacity: 0.7;\">\n                    <div class=\"email-header\">\n                        <span class=\"email-to\">To: {html_module.escape(outbound.to_email)}</span>\n                        <span class=\"email-sent-badge\" style=\"background: var(--bg-secondary); color: var(--text-tertiary);\">Skipped</span>\n                    </div>\n                    <div class=\"email-subject\">Subject: {html_module.escape(outbound.subject or \"\")}</div>\n                    <div class=\"email-body\">{html_module.escape(outbound.body or \"\")}</div>\n                </div>\n                '''\n            elif opp.outbound_message and opp.status.upper() == \"CONTACTED\":\n                email_to = lead_contact_email or \"lead\"\n                email_subject = opp.outbound_subject or f\"Regarding {lead_contact_company or 'your company'}\"\n                email_detail = f'''\n                {contact_info_html}\n                {context_html}\n                <div class=\"email-preview\">\n                    <div class=\"email-header\">\n                        <span class=\"email-to\">To: {html_module.escape(email_to)}</span>\n                        <span class=\"email-sent-badge\">Sent</span>\n                    </div>\n                    <div class=\"email-subject\">Subject: {html_module.escape(email_subject)}</div>\n                    <div class=\"email-body\">{html_module.escape(opp.outbound_message)}</div>\n                </div>\n                {next_steps_html}\n                '''\n            else:\n                email_detail = f'''\n                {contact_info_html if lead_contact_email else \"\"}\n                {context_html}\n                <div class=\"no-email\" style=\"padding: 0.75rem; background: var(--bg-tertiary); border-radius: 6px; color: var(--text-tertiary); font-size: 0.85rem;\">\n                    Email is being prepared for this opportunity. Check back shortly.\n                </div>\n                '''\n            \n            opp_cards += f'''\n            <div class=\"opp-card\" id=\"opp-{opp.id}\" onclick=\"toggleOpp('opp-{opp.id}')\">\n                <div class=\"opp-row\">\n                    <div class=\"opp-main\">\n                        <div class=\"opp-company\">{company_name}</div>\n                        <div class=\"opp-signal\">{signal_summary}</div>\n                    </div>\n                    <div class=\"opp-meta\">\n                        <span class=\"opp-date\">{timestamp}</span>\n                        <span class=\"opp-status {status_class_opp}\">{status_text}</span>\n                    </div>\n                </div>\n                <div class=\"opp-detail\">\n                    {email_detail}\n                </div>\n            </div>\n            '''\n        \n        opportunities_content = opp_cards\n    else:\n        opportunities_content = '''\n        <div class=\"empty-state\">\n            <div class=\"empty-state-title\">No opportunities yet</div>\n            <div class=\"empty-state-sub\">HossAgent is monitoring signals and will identify opportunities for you.</div>\n        </div>\n        '''\n    \n    threads = session.exec(\n        select(Thread).where(\n            Thread.customer_id == customer.id,\n            Thread.status != \"CLOSED\"\n        ).order_by(Thread.updated_at.desc()).limit(20)\n    ).all()\n    \n    total_threads = session.exec(\n        select(func.count(Thread.id)).where(\n            Thread.customer_id == customer.id,\n            Thread.status != \"CLOSED\"\n        )\n    ).one()\n    \n    if threads:\n        conv_cards = \"\"\n        for thread in threads:\n            lead_display = html_module.escape(thread.lead_name or thread.lead_email or \"Unknown Lead\")\n            company_display = f\" ({html_module.escape(thread.lead_company)})\" if thread.lead_company else \"\"\n            preview = html_module.escape(thread.last_summary[:100] if thread.last_summary else \"No messages yet\")\n            timestamp = format_local_time(thread.updated_at, user_tz) if thread.updated_at else \"-\"\n            \n            messages = session.exec(\n                select(Message).where(Message.thread_id == thread.id)\n                .order_by(Message.created_at.desc()).limit(5)\n            ).all()\n            \n            has_drafts = any(m.status == \"DRAFT\" and m.direction == \"OUTBOUND\" for m in messages)\n            \n            if has_drafts:\n                status_class_conv = \"draft\"\n                status_text = \"Draft Ready\"\n            elif thread.status == \"HUMAN_OWNED\":\n                status_class_conv = \"open\"\n                status_text = \"Your Turn\"\n            elif thread.status == \"AUTO\":\n                status_class_conv = \"open\"\n                status_text = \"Auto\"\n            else:\n                status_class_conv = \"open\"\n                status_text = \"Open\"\n            \n            messages_html = \"\"\n            for msg in reversed(messages):\n                msg_time = format_local_time(msg.created_at, user_tz) if msg.created_at else \"-\"\n                msg_body = html_module.escape(msg.body[:500] if msg.body else \"\")\n                is_draft = msg.status == \"DRAFT\" and msg.direction == \"OUTBOUND\"\n                \n                if is_draft:\n                    msg_class = \"msg-draft\"\n                    msg_label = \"AI Draft\"\n                    draft_actions = f'''\n                    <div class=\"draft-actions\">\n                        <button class=\"draft-btn approve\" onclick=\"event.stopPropagation(); handleDraft({msg.id}, 'approve')\">Approve & Send</button>\n                        <button class=\"draft-btn discard\" onclick=\"event.stopPropagation(); handleDraft({msg.id}, 'discard')\">Discard</button>\n                    </div>\n                    '''\n                elif msg.direction == \"INBOUND\":\n                    msg_class = \"msg-inbound\"\n                    msg_label = f\"From: {html_module.escape(msg.from_email)}\"\n                    draft_actions = \"\"\n                else:\n                    msg_class = \"msg-outbound\"\n                    msg_label = f\"Sent: {html_module.escape(msg.to_email)}\"\n                    draft_actions = \"\"\n                \n                messages_html += f'''\n                <div class=\"msg-item {msg_class}\">\n                    <div class=\"msg-header\">\n                        <span>{msg_label}</span>\n                        <span>{msg_time}</span>\n                    </div>\n                    <div class=\"msg-body\">{msg_body}</div>\n                    {draft_actions}\n                </div>\n                '''\n            \n            if not messages_html:\n                messages_html = '<div class=\"empty-state\" style=\"padding: 1rem;\"><div class=\"empty-state-sub\">No messages in this thread yet</div></div>'\n            \n            conv_cards += f'''\n            <div class=\"conv-card\" id=\"conv-{thread.id}\" onclick=\"toggleConv('conv-{thread.id}')\">\n                <div class=\"conv-header\">\n                    <div>\n                        <div class=\"conv-lead\">{lead_display}{company_display}</div>\n                        <div class=\"conv-preview\">{preview}</div>\n                    </div>\n                    <div class=\"conv-meta\">\n                        <span class=\"opp-date\">{timestamp}</span>\n                        <span class=\"conv-status {status_class_conv}\">{status_text}</span>\n                    </div>\n                </div>\n                <div class=\"conv-messages\">\n                    {messages_html}\n                </div>\n            </div>\n            '''\n        \n        conversations_content = conv_cards\n    else:\n        conversations_content = '''\n        <div class=\"empty-state\">\n            <div class=\"empty-state-title\">No conversations yet</div>\n            <div class=\"empty-state-sub\">When leads reply to your outreach, their conversations will appear here.</div>\n        </div>\n        '''\n    \n    reports = session.exec(\n        select(Report).where(Report.customer_id == customer.id).order_by(Report.created_at.desc()).limit(15)\n    ).all()\n    \n    if reports:\n        report_cards = \"\"\n        for idx, report in enumerate(reports):\n            timestamp = format_local_time(report.created_at, user_tz) if report.created_at else \"-\"\n            title = html_module.escape(report.title[:80] if report.title else \"Report\")\n            desc = html_module.escape(report.description[:150] if report.description else \"\")\n            content = html_module.escape(report.content or \"\")\n            \n            report_cards += f'''\n            <div class=\"report-card\" id=\"report-{idx}\" onclick=\"toggleReport('report-{idx}')\">\n                <div class=\"report-header\">\n                    <div>\n                        <div class=\"report-title\">{title}</div>\n                        <div class=\"report-desc\">{desc}</div>\n                    </div>\n                    <span class=\"report-date\">{timestamp}</span>\n                </div>\n                <div class=\"report-content\">{content}</div>\n            </div>\n            '''\n        \n        reports_content = report_cards\n    else:\n        reports_content = '''\n        <div class=\"empty-state\">\n            <div class=\"empty-state-title\">No reports yet</div>\n            <div class=\"empty-state-sub\">Reports will appear here as HossAgent completes work for you.</div>\n        </div>\n        '''\n    \n    with open(\"templates/customer_portal.html\", \"r\") as f:\n        template = f.read()\n    \n    html = template.format(\n        payment_message=payment_banner,\n        approval_banner=approval_banner,\n        plan_name=plan_name,\n        status_class=status_class,\n        status_label=status_label,\n        autopilot_class=autopilot_class,\n        autopilot_label=autopilot_label,\n        billing_info=billing_info,\n        account_cta=account_cta,\n        opportunities_count=total_opportunities,\n        opportunities_content=opportunities_content,\n        conversations_count=total_threads,\n        conversations_content=conversations_content,\n        reports_count=len(reports),\n        reports_content=reports_content\n    )\n    \n    return HTMLResponse(content=html)\n\n\n# ============================================================================\n# CUSTOMER PORTAL - TOKEN BASED ACCESS\n# ============================================================================\n\n\n@app.get(\"/portal/{public_token}\", response_class=HTMLResponse)\ndef customer_portal_token(public_token: str, request: Request, session: Session = Depends(get_session)):\n    \"\"\"\n    Token-based customer portal for admin impersonation or direct link access.\n    \n    Shows:\n    - Account summary (total invoiced, paid, outstanding)\n    - Customer info\n    - Recent tasks\n    - Outstanding invoices with PAY NOW buttons\n    - Paid invoices\n    - Payment status messaging based on Stripe configuration\n    \"\"\"\n    customer = session.exec(\n        select(Customer).where(Customer.public_token == public_token)\n    ).first()\n    \n    if not customer:\n        raise HTTPException(status_code=404, detail=\"Portal not found\")\n    \n    return render_customer_portal(customer, request, session)\n\n\n@app.get(\"/subscribe/{public_token}\")\ndef subscribe_redirect(public_token: str, request: Request, session: Session = Depends(get_session)):\n    \"\"\"\n    Redirect customer to Stripe Checkout for subscription.\n    \n    If Stripe is not configured, shows a friendly message.\n    If already subscribed, redirects to billing portal.\n    \"\"\"\n    customer = session.exec(\n        select(Customer).where(Customer.public_token == public_token)\n    ).first()\n    \n    if not customer:\n        raise HTTPException(status_code=404, detail=\"Customer not found\")\n    \n    plan_status = get_customer_plan_status(customer)\n    \n    if plan_status.is_paid:\n        success, portal_url, mode, error = create_billing_portal_link(\n            customer,\n            return_url=str(request.url_for(\"customer_portal_token\", public_token=public_token))\n        )\n        if success and portal_url:\n            return RedirectResponse(url=portal_url, status_code=303)\n        else:\n            return HTMLResponse(content=f\"\"\"\n<!DOCTYPE html>\n<html>\n<head>\n    <meta charset=\"utf-8\">\n    <title>Billing Portal</title>\n    <style>\n        body {{ background: #0a0a0a; color: #fff; font-family: Georgia, serif; \n               display: flex; align-items: center; justify-content: center; \n               min-height: 100vh; margin: 0; }}\n        .box {{ text-align: center; padding: 3rem; border: 1px solid #333; max-width: 500px; }}\n        h1 {{ font-size: 1.5rem; font-weight: normal; margin-bottom: 1rem; }}\n        p {{ color: #888; margin-bottom: 1.5rem; }}\n        a {{ display: inline-block; background: #fff; color: #0a0a0a; padding: 0.75rem 2rem; \n             text-decoration: none; font-weight: bold; }}\n        a:hover {{ background: #ddd; }}\n    </style>\n</head>\n<body>\n    <div class=\"box\">\n        <h1>Billing Portal Unavailable</h1>\n        <p>The billing management portal is not currently available. Please contact support for billing inquiries.</p>\n        <a href=\"/portal/{public_token}\">Return to Portal</a>\n    </div>\n</body>\n</html>\n\"\"\", status_code=200)\n    \n    base_url = str(request.base_url).rstrip(\"/\")\n    success_url = f\"{base_url}/portal/{public_token}?payment=success\"\n    cancel_url = f\"{base_url}/portal/{public_token}?payment=cancelled\"\n    \n    success, checkout_url, mode, error = get_or_create_subscription_checkout_link(\n        customer,\n        success_url=success_url,\n        cancel_url=cancel_url\n    )\n    \n    if success and checkout_url:\n        return RedirectResponse(url=checkout_url, status_code=303)\n    \n    error_message = error or \"Online billing is not currently configured.\"\n    if mode == \"disabled\":\n        error_message = \"Online payment is not yet configured. Your account manager will be in touch to set up billing.\"\n    \n    return HTMLResponse(content=f\"\"\"\n<!DOCTYPE html>\n<html>\n<head>\n    <meta charset=\"utf-8\">\n    <title>Subscribe - HossAgent</title>\n    <style>\n        body {{ background: #0a0a0a; color: #fff; font-family: Georgia, serif; \n               display: flex; align-items: center; justify-content: center; \n               min-height: 100vh; margin: 0; }}\n        .box {{ text-align: center; padding: 3rem; border: 1px solid #333; max-width: 500px; }}\n        h1 {{ font-size: 1.5rem; font-weight: normal; margin-bottom: 1rem; }}\n        .price {{ font-size: 2rem; font-weight: bold; margin: 1rem 0; }}\n        p {{ color: #888; margin-bottom: 1.5rem; }}\n        a {{ display: inline-block; background: #fff; color: #0a0a0a; padding: 0.75rem 2rem; \n             text-decoration: none; font-weight: bold; }}\n        a:hover {{ background: #ddd; }}\n    </style>\n</head>\n<body>\n    <div class=\"box\">\n        <h1>HossAgent Pro</h1>\n        <div class=\"price\">$99/month</div>\n        <p>{error_message}</p>\n        <a href=\"/portal/{public_token}\">Return to Portal</a>\n    </div>\n</body>\n</html>\n\"\"\", status_code=200)\n\n\n@app.get(\"/billing/{public_token}\")\ndef billing_portal_redirect(public_token: str, request: Request, session: Session = Depends(get_session)):\n    \"\"\"\n    Redirect paid customers to Stripe Customer Portal for billing management.\n    \"\"\"\n    customer = session.exec(\n        select(Customer).where(Customer.public_token == public_token)\n    ).first()\n    \n    if not customer:\n        raise HTTPException(status_code=404, detail=\"Customer not found\")\n    \n    plan_status = get_customer_plan_status(customer)\n    \n    if not plan_status.is_paid:\n        return RedirectResponse(url=f\"/subscribe/{public_token}\", status_code=303)\n    \n    success, portal_url, mode, error = create_billing_portal_link(\n        customer,\n        return_url=str(request.url_for(\"customer_portal_token\", public_token=public_token))\n    )\n    \n    if success and portal_url:\n        return RedirectResponse(url=portal_url, status_code=303)\n    \n    return HTMLResponse(content=f\"\"\"\n<!DOCTYPE html>\n<html>\n<head>\n    <meta charset=\"utf-8\">\n    <title>Billing Portal</title>\n    <style>\n        body {{ background: #0a0a0a; color: #fff; font-family: Georgia, serif; \n               display: flex; align-items: center; justify-content: center; \n               min-height: 100vh; margin: 0; }}\n        .box {{ text-align: center; padding: 3rem; border: 1px solid #333; max-width: 500px; }}\n        h1 {{ font-size: 1.5rem; font-weight: normal; margin-bottom: 1rem; }}\n        p {{ color: #888; margin-bottom: 1.5rem; }}\n        a {{ display: inline-block; background: #fff; color: #0a0a0a; padding: 0.75rem 2rem; \n             text-decoration: none; font-weight: bold; }}\n        a:hover {{ background: #ddd; }}\n    </style>\n</head>\n<body>\n    <div class=\"box\">\n        <h1>Billing Portal</h1>\n        <p>The billing portal is not currently available. Please contact support for billing inquiries.</p>\n        <a href=\"/portal/{public_token}\">Return to Portal</a>\n    </div>\n</body>\n</html>\n\"\"\", status_code=200)\n\n\n# ============================================================================\n# CHECKOUT AND BILLING API ENDPOINTS\n# ============================================================================\n\n\n@app.post(\"/api/create-checkout-session\")\ndef api_create_checkout_session(\n    request: Request,\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Create a Stripe Checkout session for subscription.\n    \n    Requires authenticated customer (session cookie).\n    Returns JSON with checkout URL.\n    \"\"\"\n    session_token = request.cookies.get(SESSION_COOKIE_NAME)\n    customer = get_customer_from_session(session, session_token)\n    \n    if not customer:\n        return JSONResponse(\n            status_code=401,\n            content={\"error\": \"Not authenticated\", \"redirect\": \"/login\"}\n        )\n    \n    plan_status = get_customer_plan_status(customer)\n    \n    if plan_status.is_paid:\n        return JSONResponse(\n            status_code=400,\n            content={\"error\": \"Already subscribed\", \"redirect\": f\"/billing/{customer.public_token}\"}\n        )\n    \n    base_url = str(request.base_url).rstrip(\"/\")\n    success_url = f\"{base_url}/portal?payment=success\"\n    cancel_url = f\"{base_url}/portal?payment=cancelled\"\n    \n    track_funnel_event(EventType.CHECKOUT_STARTED, customer_id=customer.id)\n    \n    success, checkout_url, mode, error = get_or_create_subscription_checkout_link(\n        customer,\n        success_url=success_url,\n        cancel_url=cancel_url\n    )\n    \n    if success and checkout_url:\n        return JSONResponse(content={\"checkout_url\": checkout_url, \"mode\": mode})\n    \n    return JSONResponse(\n        status_code=400,\n        content={\"error\": error or \"Failed to create checkout session\", \"mode\": mode}\n    )\n\n\n@app.post(\"/api/create-billing-portal-session\")\ndef api_create_billing_portal_session(\n    request: Request,\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Create a Stripe Billing Portal session for subscription management.\n    \n    Requires authenticated customer (session cookie) with paid subscription.\n    Returns JSON with portal URL.\n    \"\"\"\n    session_token = request.cookies.get(SESSION_COOKIE_NAME)\n    customer = get_customer_from_session(session, session_token)\n    \n    if not customer:\n        return JSONResponse(\n            status_code=401,\n            content={\"error\": \"Not authenticated\", \"redirect\": \"/login\"}\n        )\n    \n    plan_status = get_customer_plan_status(customer)\n    \n    if not plan_status.is_paid:\n        return JSONResponse(\n            status_code=400,\n            content={\"error\": \"No active subscription\", \"redirect\": f\"/subscribe/{customer.public_token}\"}\n        )\n    \n    base_url = str(request.base_url).rstrip(\"/\")\n    return_url = f\"{base_url}/portal\"\n    \n    success, portal_url, mode, error = create_billing_portal_link(\n        customer,\n        return_url=return_url\n    )\n    \n    if success and portal_url:\n        return JSONResponse(content={\"portal_url\": portal_url, \"mode\": mode})\n    \n    return JSONResponse(\n        status_code=400,\n        content={\"error\": error or \"Failed to create billing portal session\", \"mode\": mode}\n    )\n\n\n# ============================================================================\n# BIZDEV TEMPLATE STATUS API\n# ============================================================================\n\n\n@app.get(\"/api/bizdev/templates\")\ndef get_bizdev_templates():\n    \"\"\"Get current BizDev template configuration and recent generations.\"\"\"\n    from bizdev_templates import get_template_status, get_template_log\n    \n    status = get_template_status()\n    recent = get_template_log(10)\n    \n    return {\n        **status,\n        \"recent_generations\": recent\n    }\n\n\n# ============================================================================\n# KPI DASHBOARD API\n# ============================================================================\n\n\n@app.get(\"/api/kpis\")\ndef get_kpis(request: Request, session: Session = Depends(get_session)):\n    \"\"\"\n    Get KPI counts for today's activity.\n    \n    Returns:\n        - signals_today: Count of signals created today\n        - lead_events_today: Count of lead events created today\n        - outbound_sent_today: Count of outbound with APPROVED/SENT status updated today\n        - reports_delivered_today: Count of reports created today\n        - errors_failed: Count of pending outbound with FAILED or SKIPPED status\n        - enrichment_pending: Count of lead events pending enrichment\n        - enrichment_complete_today: Count of lead events enriched today\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    today_start = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)\n    \n    signals_today = session.exec(\n        select(func.count()).select_from(Signal).where(Signal.created_at >= today_start)\n    ).one()\n    \n    lead_events_today = session.exec(\n        select(func.count()).select_from(LeadEvent).where(LeadEvent.created_at >= today_start)\n    ).one()\n    \n    outbound_sent_today = session.exec(\n        select(func.count()).select_from(PendingOutbound).where(\n            (PendingOutbound.status.in_([\"APPROVED\", \"SENT\"])) &\n            (PendingOutbound.created_at >= today_start)\n        )\n    ).one()\n    \n    reports_delivered_today = session.exec(\n        select(func.count()).select_from(Report).where(Report.created_at >= today_start)\n    ).one()\n    \n    errors_failed = session.exec(\n        select(func.count()).select_from(PendingOutbound).where(\n            PendingOutbound.status.in_([\"FAILED\", \"SKIPPED\"])\n        )\n    ).one()\n    \n    enrichment_pending = session.exec(\n        select(func.count()).select_from(LeadEvent).where(\n            (LeadEvent.enrichment_status == None) | \n            (LeadEvent.enrichment_status.in_([\"UNENRICHED\", \"ENRICHING\"]))\n        )\n    ).one()\n    \n    enrichment_complete_today = session.exec(\n        select(func.count()).select_from(LeadEvent).where(\n            (LeadEvent.enrichment_status.in_([\"ENRICHED\", \"OUTBOUND_READY\"])) &\n            (LeadEvent.enriched_at != None) &\n            (LeadEvent.enriched_at >= today_start)\n        )\n    ).one()\n    \n    return {\n        \"signals_today\": signals_today,\n        \"lead_events_today\": lead_events_today,\n        \"outbound_sent_today\": outbound_sent_today,\n        \"reports_delivered_today\": reports_delivered_today,\n        \"errors_failed\": errors_failed,\n        \"enrichment_pending\": enrichment_pending,\n        \"enrichment_complete_today\": enrichment_complete_today\n    }\n\n\ndef compute_signal_source(lead_event, signal=None):\n    \"\"\"\n    Compute signal source classification for admin display.\n    \n    Classification logic:\n    - news: from news_search signal source\n    - sec: from SEC EDGAR filings (macro_event_id present or sec.gov in data)\n    - job_board: from job board connector (indeed, ziprecruiter, glassdoor)\n    - reddit: from reddit signal source\n    - craigslist: from craigslist connector\n    - synthetic: ForceCast synthetic leads (category patterns like \"HVAC businesses\", \"Tech businesses\")\n    - unknown: fallback for unclassified sources\n    \"\"\"\n    if signal and signal.source_type:\n        source_type = signal.source_type.lower()\n        if source_type in ('news', 'news_search'):\n            return 'news'\n        elif source_type in ('job_board', 'job_posting'):\n            return 'job_board'\n        elif source_type == 'reddit' or source_type == 'reddit_local':\n            return 'reddit'\n        elif source_type in ('craigslist', 'craigslist_local'):\n            return 'craigslist'\n        elif 'sec' in source_type or 'edgar' in source_type:\n            return 'sec'\n    \n    if lead_event.macro_event_id:\n        return 'sec'\n    \n    category = (lead_event.category or '').lower()\n    summary = (lead_event.summary or '').lower()\n    \n    synthetic_patterns = [\n        'hvac businesses', 'tech businesses', 'roofing businesses',\n        'plumbing businesses', 'marketing agency', 'med spa',\n        'realtor', 'immigration attorney', 'businesses in miami',\n        'businesses in broward', 'businesses in south florida'\n    ]\n    for pattern in synthetic_patterns:\n        if pattern in category or pattern in summary:\n            return 'synthetic'\n    \n    if 'job' in category or 'hiring' in category:\n        return 'job_board'\n    if 'reddit' in category:\n        return 'reddit'\n    if 'craigslist' in category:\n        return 'craigslist'\n    \n    return 'unknown'\n\n\ndef compute_company_class(lead_event):\n    \"\"\"\n    Compute company classification for admin display.\n    \n    Classification logic:\n    - enterprise: domain ends in .com AND org size keywords in title/summary\n    - smb: category in [GROWTH_SIGNAL, OPPORTUNITY] AND phone present\n    - unknown: fallback for unclassified leads\n    \"\"\"\n    domain = (lead_event.lead_domain or '').lower()\n    summary = (lead_event.summary or '').lower()\n    lead_company = (lead_event.lead_company or '').lower()\n    category = (lead_event.category or '').upper()\n    \n    enterprise_keywords = [\n        'airlines', 'airline', 'corporation', 'corp', 'inc', 'incorporated',\n        'international', 'holdings', 'group', 'enterprises', 'global',\n        'national', 'regional', 'public company', 'publicly traded',\n        'fortune 500', 'fortune 1000', 'nasdaq', 'nyse', 'stock',\n        'billion', 'thousand employees', 'hundreds of employees',\n        'major', 'largest', 'leading', 'headquarters'\n    ]\n    \n    has_com_domain = domain.endswith('.com') if domain else False\n    has_enterprise_keyword = any(kw in summary or kw in lead_company for kw in enterprise_keywords)\n    \n    if has_com_domain and has_enterprise_keyword:\n        return 'enterprise'\n    \n    smb_categories = ['GROWTH_SIGNAL', 'OPPORTUNITY']\n    has_smb_category = category in smb_categories\n    has_phone = bool(lead_event.lead_phone_e164 or lead_event.lead_phone_raw)\n    \n    if has_smb_category and has_phone:\n        return 'smb'\n    \n    return 'unknown'\n\n\n@app.get(\"/api/lead_events_detailed\")\ndef get_lead_events_detailed(\n    request: Request,\n    limit: int = Query(default=50, le=100),\n    enrichment_status: Optional[str] = Query(default=None),\n    exclude_no_osint: bool = Query(default=False),\n    only_no_osint: bool = Query(default=False),\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Get detailed lead events with related outbound and report info.\n    \n    Supports optional filtering by enrichment_status:\n    - UNENRICHED: Raw signals, no domain/email yet\n    - WITH_DOMAIN_NO_EMAIL: Domain discovered, awaiting email scraping\n    - ENRICHED_NO_OUTBOUND: Ready to send (email found)\n    - OUTBOUND_SENT: Email sent\n    \n    Additional filters:\n    - exclude_no_osint: Exclude leads with NO_OSINT_PRESENCE unenrichable_reason\n    - only_no_osint: Only return leads with NO_OSINT_PRESENCE unenrichable_reason\n    \n    Returns lead events with has_outbound, has_report, and signal_source.\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    query = select(LeadEvent).order_by(LeadEvent.created_at.desc())\n    \n    if enrichment_status:\n        query = query.where(LeadEvent.enrichment_status == enrichment_status)\n    \n    if exclude_no_osint:\n        query = query.where(\n            (LeadEvent.enrichment_status != ENRICHMENT_STATUS_ARCHIVED_UNENRICHABLE) |\n            (LeadEvent.unenrichable_reason != UNENRICHABLE_REASON_NO_OSINT_PRESENCE)\n        )\n    \n    if only_no_osint:\n        query = query.where(\n            LeadEvent.enrichment_status == ENRICHMENT_STATUS_ARCHIVED_UNENRICHABLE,\n            LeadEvent.unenrichable_reason == UNENRICHABLE_REASON_NO_OSINT_PRESENCE\n        )\n    \n    events = session.exec(query.limit(limit)).all()\n    \n    signal_ids = [e.signal_id for e in events if e.signal_id]\n    signals_map = {}\n    if signal_ids:\n        signals = session.exec(select(Signal).where(Signal.id.in_(signal_ids))).all()\n        signals_map = {s.id: s for s in signals}\n    \n    result = []\n    for e in events:\n        has_outbound = session.exec(\n            select(func.count()).select_from(PendingOutbound).where(\n                PendingOutbound.lead_event_id == e.id\n            )\n        ).one() > 0\n        \n        has_report = session.exec(\n            select(func.count()).select_from(Report).where(\n                Report.lead_id == e.lead_id\n            )\n        ).one() > 0 if e.lead_id else False\n        \n        company = None\n        if e.company_id:\n            customer = session.exec(\n                select(Customer).where(Customer.id == e.company_id)\n            ).first()\n            company = customer.company if customer else None\n        \n        signal = signals_map.get(e.signal_id) if e.signal_id else None\n        signal_source = compute_signal_source(e, signal)\n        company_class = compute_company_class(e)\n        \n        result.append({\n            \"id\": e.id,\n            \"summary\": e.summary,\n            \"category\": e.category,\n            \"urgency_score\": e.urgency_score,\n            \"status\": e.status,\n            \"signal_source\": signal_source,\n            \"company_class\": company_class,\n            \"enrichment_status\": e.enrichment_status or \"UNENRICHED\",\n            \"enrichment_attempts\": e.enrichment_attempts or 0,\n            \"max_enrichment_attempts\": getattr(e, 'max_enrichment_attempts', 3),\n            \"unenrichable_reason\": getattr(e, 'unenrichable_reason', None),\n            \"enrichment_mission_log\": getattr(e, 'enrichment_mission_log', None),\n            \"has_outbound\": has_outbound,\n            \"has_report\": has_report,\n            \"company\": company,\n            \"signal_id\": e.signal_id,\n            \"lead_id\": e.lead_id,\n            \"lead_company\": e.lead_company,\n            \"lead_domain\": e.lead_domain,\n            \"lead_email\": e.lead_email,\n            \"lead_phone_raw\": e.lead_phone_raw,\n            \"lead_phone_e164\": e.lead_phone_e164,\n            \"phone_confidence\": e.phone_confidence or 0,\n            \"phone_source\": e.phone_source,\n            \"phone_type\": e.phone_type,\n            \"domain_confidence\": getattr(e, 'domain_confidence', 0),\n            \"email_confidence\": getattr(e, 'email_confidence', 0),\n            \"created_at\": e.created_at.isoformat() if e.created_at else None\n        })\n    \n    return result\n\n\n@app.get(\"/api/enrichment_status_counts\")\ndef get_enrichment_status_counts(\n    request: Request,\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Get counts of LeadEvents by enrichment status for admin dashboard.\n    \n    Returns counts for each enrichment status:\n    - UNENRICHED: Raw signals, no domain/email yet\n    - WITH_DOMAIN_NO_EMAIL: Domain discovered, awaiting email scraping\n    - ENRICHED_NO_OUTBOUND: Ready to send (email found)\n    - OUTBOUND_SENT: Email sent\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    counts = get_lead_events_counts_by_status(session)\n    \n    total = sum(counts.values())\n    \n    return {\n        \"total\": total,\n        \"counts\": counts,\n        \"statuses\": [\n            {\"key\": ENRICHMENT_STATUS_UNENRICHED, \"label\": \"Unenriched (No Domain)\", \"count\": counts.get(ENRICHMENT_STATUS_UNENRICHED, 0)},\n            {\"key\": ENRICHMENT_STATUS_WITH_DOMAIN_NO_EMAIL, \"label\": \"Domain Found (Email Pending)\", \"count\": counts.get(ENRICHMENT_STATUS_WITH_DOMAIN_NO_EMAIL, 0)},\n            {\"key\": ENRICHMENT_STATUS_WITH_PHONE_ONLY, \"label\": \"Phone Found (No Email)\", \"count\": counts.get(ENRICHMENT_STATUS_WITH_PHONE_ONLY, 0)},\n            {\"key\": ENRICHMENT_STATUS_ENRICHED_NO_OUTBOUND, \"label\": \"Email Ready (Awaiting Send)\", \"count\": counts.get(ENRICHMENT_STATUS_ENRICHED_NO_OUTBOUND, 0)},\n            {\"key\": ENRICHMENT_STATUS_OUTBOUND_SENT, \"label\": \"Outbound Sent\", \"count\": counts.get(ENRICHMENT_STATUS_OUTBOUND_SENT, 0)},\n        ]\n    }\n\n\n@app.get(\"/api/output_history\")\ndef get_output_history(\n    request: Request,\n    limit: int = Query(default=50, le=100),\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Get combined output history: outbound messages and reports.\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    outbound = session.exec(\n        select(PendingOutbound).order_by(PendingOutbound.created_at.desc()).limit(limit)\n    ).all()\n    \n    reports = session.exec(\n        select(Report).order_by(Report.created_at.desc()).limit(limit)\n    ).all()\n    \n    outbound_list = []\n    for o in outbound:\n        outbound_list.append({\n            \"id\": o.id,\n            \"lead_event_id\": o.lead_event_id,\n            \"to_email\": o.to_email,\n            \"subject\": o.subject,\n            \"status\": o.status,\n            \"created_at\": o.created_at.isoformat() if o.created_at else None,\n            \"sent_at\": o.sent_at.isoformat() if o.sent_at else None\n        })\n    \n    reports_list = []\n    for r in reports:\n        lead_event = None\n        if r.lead_id:\n            le = session.exec(\n                select(LeadEvent).where(LeadEvent.lead_id == r.lead_id)\n            ).first()\n            lead_event = le.id if le else None\n        \n        reports_list.append({\n            \"id\": r.id,\n            \"title\": r.title,\n            \"lead_event_id\": lead_event,\n            \"report_type\": r.report_type,\n            \"created_at\": r.created_at.isoformat() if r.created_at else None\n        })\n    \n    return {\n        \"outbound\": outbound_list,\n        \"reports\": reports_list\n    }\n\n\n# ============================================================================\n# OPPORTUNITY DETAIL API - CUSTOMER PORTAL\n# ============================================================================\n\n\n@app.get(\"/api/opportunity/{opportunity_id}/detail\")\ndef get_opportunity_detail(\n    opportunity_id: int,\n    request: Request,\n    session: Session = Depends(get_session),\n    customer_id: Optional[int] = Query(None, description=\"Customer ID for portal access\")\n):\n    \"\"\"\n    Get detailed opportunity (LeadEvent) data for customer portal.\n    \n    Returns:\n    - Full LeadEvent data (summary, category, urgency, status, lifecycle info)\n    - Related Signal context (if available)\n    - Related PendingOutbound records\n    - Related Report records\n    - Lead info (if linked)\n    \n    Authenticated via customer session cookie OR customer_id parameter.\n    The customer_id parameter is used when viewing portal via admin token.\n    \"\"\"\n    customer = None\n    \n    session_token = request.cookies.get(SESSION_COOKIE_NAME)\n    if session_token:\n        customer = get_customer_from_session(session, session_token)\n    \n    if not customer and customer_id:\n        customer = session.exec(select(Customer).where(Customer.id == customer_id)).first()\n    \n    if not customer:\n        raise HTTPException(status_code=401, detail=\"Authentication required\")\n    \n    opportunity = session.exec(\n        select(LeadEvent).where(\n            LeadEvent.id == opportunity_id,\n            LeadEvent.company_id == customer.id\n        )\n    ).first()\n    \n    if not opportunity:\n        raise HTTPException(status_code=404, detail=\"Opportunity not found\")\n    \n    signal_data = None\n    if opportunity.signal_id:\n        signal = session.exec(\n            select(Signal).where(Signal.id == opportunity.signal_id)\n        ).first()\n        if signal:\n            signal_data = {\n                \"id\": signal.id,\n                \"source_type\": signal.source_type,\n                \"context_summary\": signal.context_summary,\n                \"geography\": signal.geography,\n                \"created_at\": signal.created_at.isoformat() if signal.created_at else None\n            }\n    \n    outbound_records = session.exec(\n        select(PendingOutbound).where(\n            PendingOutbound.lead_event_id == opportunity.id\n        ).order_by(PendingOutbound.created_at.desc())\n    ).all()\n    \n    outbound_list = []\n    for o in outbound_records:\n        outbound_list.append({\n            \"id\": o.id,\n            \"to_email\": o.to_email,\n            \"to_name\": o.to_name,\n            \"subject\": o.subject,\n            \"body\": o.body,\n            \"context_summary\": o.context_summary,\n            \"status\": o.status,\n            \"created_at\": o.created_at.isoformat() if o.created_at else None,\n            \"sent_at\": o.sent_at.isoformat() if o.sent_at else None,\n            \"approved_at\": o.approved_at.isoformat() if o.approved_at else None\n        })\n    \n    report_records = session.exec(\n        select(Report).where(\n            (Report.lead_event_id == opportunity.id) |\n            (Report.lead_id == opportunity.lead_id)\n        ).order_by(Report.created_at.desc())\n    ).all() if opportunity.lead_id else session.exec(\n        select(Report).where(Report.lead_event_id == opportunity.id).order_by(Report.created_at.desc())\n    ).all()\n    \n    reports_list = []\n    for r in report_records:\n        reports_list.append({\n            \"id\": r.id,\n            \"title\": r.title,\n            \"description\": r.description,\n            \"content\": r.content,\n            \"report_type\": r.report_type,\n            \"created_at\": r.created_at.isoformat() if r.created_at else None\n        })\n    \n    lead_data = None\n    if opportunity.lead_id:\n        lead = session.exec(\n            select(Lead).where(Lead.id == opportunity.lead_id)\n        ).first()\n        if lead:\n            lead_data = {\n                \"id\": lead.id,\n                \"name\": lead.name,\n                \"email\": lead.email,\n                \"company\": lead.company,\n                \"niche\": lead.niche,\n                \"status\": lead.status,\n                \"website\": lead.website,\n                \"source\": lead.source\n            }\n    \n    return {\n        \"id\": opportunity.id,\n        \"summary\": opportunity.summary,\n        \"category\": opportunity.category,\n        \"urgency_score\": opportunity.urgency_score,\n        \"status\": opportunity.status,\n        \"recommended_action\": opportunity.recommended_action,\n        \"outbound_message\": opportunity.outbound_message,\n        \"last_contact_at\": opportunity.last_contact_at.isoformat() if opportunity.last_contact_at else None,\n        \"last_contact_summary\": opportunity.last_contact_summary,\n        \"next_step\": opportunity.next_step,\n        \"next_step_owner\": opportunity.next_step_owner,\n        \"created_at\": opportunity.created_at.isoformat() if opportunity.created_at else None,\n        \"signal\": signal_data,\n        \"outbound_messages\": outbound_list,\n        \"reports\": reports_list,\n        \"lead\": lead_data\n    }\n\n\n# ============================================================================\n# LEAD EVENT DETAIL API - ADMIN CONSOLE\n# ============================================================================\n\n\n@app.get(\"/api/admin/diagnostics\")\ndef get_diagnostics(request: Request, session: Session = Depends(get_session)):\n    \"\"\"\n    Lead funnel diagnostics - measure where bottlenecks are.\n    \n    Returns metrics on signal processing and enrichment pipeline.\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    total_signals = session.exec(select(func.count(Signal.id))).one()\n    total_lead_events = session.exec(select(func.count(LeadEvent.id))).one()\n    \n    enrichment_by_status = {}\n    for status in [\"UNENRICHED\", \"WITH_DOMAIN_NO_EMAIL\", \"ENRICHED_NO_OUTBOUND\", \"OUTBOUND_SENT\", \"ARCHIVED_UNENRICHABLE\"]:\n        count = session.exec(\n            select(func.count(LeadEvent.id)).where(LeadEvent.enrichment_status == status)\n        ).one()\n        enrichment_by_status[status] = count\n    \n    total_enriched = session.exec(\n        select(func.count(LeadEvent.id)).where(LeadEvent.enrichment_status.in_([\"ENRICHED_NO_OUTBOUND\", \"OUTBOUND_SENT\"]))\n    ).one()\n    \n    total_unenrichable = session.exec(\n        select(func.count(LeadEvent.id)).where(LeadEvent.enrichment_status == \"ARCHIVED_UNENRICHABLE\")\n    ).one()\n    \n    avg_enrichment_attempts = session.exec(\n        select(func.avg(LeadEvent.enrichment_attempts))\n    ).one() or 0\n    \n    total_attempted = session.exec(\n        select(func.count(LeadEvent.id)).where(LeadEvent.enrichment_attempts > 0)\n    ).one()\n    \n    signals_by_source = {}\n    signals = session.exec(select(Signal.source_type, func.count(Signal.id)).group_by(Signal.source_type)).all()\n    for source_type, count in signals:\n        signals_by_source[source_type or \"unknown\"] = count\n    \n    enrichment_rate = (total_enriched / total_lead_events * 100) if total_lead_events > 0 else 0\n    attempt_efficiency = (total_enriched / total_attempted * 100) if total_attempted > 0 else 0\n    \n    return {\n        \"timestamp\": datetime.utcnow().isoformat(),\n        \"signals\": {\n            \"total\": total_signals,\n            \"by_source\": signals_by_source\n        },\n        \"lead_events\": {\n            \"total\": total_lead_events,\n            \"enriched\": total_enriched,\n            \"enrichment_rate_percent\": round(enrichment_rate, 1),\n            \"by_status\": enrichment_by_status\n        },\n        \"enrichment\": {\n            \"total_unenrichable\": total_unenrichable,\n            \"avg_attempts\": round(avg_enrichment_attempts, 2),\n            \"total_attempted\": total_attempted,\n            \"attempt_efficiency_percent\": round(attempt_efficiency, 1)\n        },\n        \"funnel\": {\n            \"signals_to_events\": round((total_lead_events / total_signals * 100), 1) if total_signals > 0 else 0,\n            \"events_to_enriched\": round((total_enriched / total_lead_events * 100), 1) if total_lead_events > 0 else 0,\n            \"enriched_to_sent\": round((session.exec(select(func.count(LeadEvent.id)).where(LeadEvent.enrichment_status == \"OUTBOUND_SENT\")).one() or 0) / (total_enriched or 1) * 100, 1)\n        }\n    }\n\n\n@app.post(\"/api/admin/lead_event/{event_id}/force-enrich\")\nasync def force_enrich_lead_event(\n    event_id: int,\n    request: Request,\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Force immediate enrichment of a lead event, bypassing throttles and batch limits.\n    \n    Admin-only endpoint for manually pushing UNENRICHED leads into enrichment.\n    Resets enrichment attempts to allow retry even if budget exhausted.\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    lead_event = session.exec(\n        select(LeadEvent).where(LeadEvent.id == event_id)\n    ).first()\n    \n    if not lead_event:\n        raise HTTPException(status_code=404, detail=\"Lead event not found\")\n    \n    from lead_enrichment import (\n        enrich_lead_event, \n        _apply_enrichment_to_lead_event,\n        ENRICHMENT_STATUS_UNENRICHED,\n        ENRICHMENT_STATUS_WITH_DOMAIN_NO_EMAIL,\n        ENRICHMENT_STATUS_ENRICHED_NO_OUTBOUND\n    )\n    from domain_discovery import discover_domain_for_lead_event\n    \n    old_status = lead_event.enrichment_status\n    old_attempts = lead_event.enrichment_attempts or 0\n    \n    print(f\"[FORCE_ENRICH] Starting force enrichment for lead_event_id={event_id}\")\n    print(f\"[FORCE_ENRICH] Current status: {old_status}, attempts: {old_attempts}\")\n    \n    if old_status == \"ARCHIVED_UNENRICHABLE\":\n        print(f\"[FORCE_ENRICH] Resetting archived lead for retry\")\n        lead_event.enrichment_attempts = 0\n        lead_event.unenrichable_reason = None\n        lead_event.enrichment_status = ENRICHMENT_STATUS_UNENRICHED\n    \n    if not lead_event.lead_domain:\n        print(f\"[FORCE_ENRICH] No domain - running domain discovery first\")\n        domain_result = discover_domain_for_lead_event(lead_event, session)\n        if domain_result and domain_result.domain:\n            lead_event.lead_domain = domain_result.domain\n            lead_event.enrichment_status = ENRICHMENT_STATUS_WITH_DOMAIN_NO_EMAIL\n            session.add(lead_event)\n            session.commit()\n            print(f\"[FORCE_ENRICH] Domain discovered: {domain_result.domain}\")\n    \n    result = await enrich_lead_event(lead_event, session)\n    \n    lead_event.enrichment_attempts = (lead_event.enrichment_attempts or 0) + 1\n    lead_event.last_enrichment_at = datetime.utcnow()\n    \n    new_status = _apply_enrichment_to_lead_event(lead_event, result, session, domain_discovered=bool(lead_event.lead_domain))\n    \n    session.add(lead_event)\n    session.commit()\n    \n    print(f\"[FORCE_ENRICH] Completed: {old_status} -> {new_status}, email={result.email}, phone={result.phone}\")\n    \n    return {\n        \"success\": result.success,\n        \"event_id\": event_id,\n        \"old_status\": old_status,\n        \"new_status\": new_status,\n        \"email_found\": result.email,\n        \"phone_found\": result.phone,\n        \"source\": result.source,\n        \"attempts\": lead_event.enrichment_attempts,\n        \"message\": f\"Force enrichment complete. Status: {old_status} -> {new_status}\"\n    }\n\n\n@app.get(\"/api/admin/lead_event/{event_id}/draft-outbound\")\nasync def get_draft_outbound(\n    event_id: int,\n    request: Request,\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Generate a draft outbound email for a lead event.\n    Returns the generated subject and body for editing before sending.\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    lead_event = session.exec(\n        select(LeadEvent).where(LeadEvent.id == event_id)\n    ).first()\n    \n    if not lead_event:\n        raise HTTPException(status_code=404, detail=\"Lead event not found\")\n    \n    if not lead_event.lead_email:\n        raise HTTPException(status_code=400, detail=\"Lead has no email address\")\n    \n    from agents import generate_miami_contextual_email\n    \n    contact_name = lead_event.lead_name or \"there\"\n    company_name = lead_event.lead_company or \"your company\"\n    niche = \"local service\"\n    \n    subject, body = generate_miami_contextual_email(\n        contact_name=contact_name,\n        company_name=company_name,\n        niche=niche,\n        event_summary=lead_event.summary or \"\",\n        recommended_action=lead_event.recommended_action or \"\",\n        category=lead_event.category or \"\",\n        urgency_score=lead_event.urgency_score or 50,\n        outreach_style=\"transparent_ai\",\n        event_id=event_id,\n        signal_id=lead_event.signal_id\n    )\n    \n    return {\n        \"event_id\": event_id,\n        \"to_email\": lead_event.lead_email,\n        \"to_name\": contact_name,\n        \"company\": company_name,\n        \"subject\": subject,\n        \"body\": body,\n        \"enrichment_status\": lead_event.enrichment_status\n    }\n\n\n@app.post(\"/api/admin/lead_event/{event_id}/send-outbound\")\nasync def send_manual_outbound(\n    event_id: int,\n    request: Request,\n    subject: str = Form(...),\n    body: str = Form(...),\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Manually send outbound email for a lead event.\n    Used when autopilot is OFF to send edited/approved emails.\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    lead_event = session.exec(\n        select(LeadEvent).where(LeadEvent.id == event_id)\n    ).first()\n    \n    if not lead_event:\n        raise HTTPException(status_code=404, detail=\"Lead event not found\")\n    \n    if not lead_event.lead_email:\n        raise HTTPException(status_code=400, detail=\"Lead has no email address\")\n    \n    to_email = lead_event.lead_email\n    lead_name = lead_event.lead_name or \"\"\n    company = lead_event.lead_company or \"\"\n    \n    result = send_email(\n        to_email=to_email,\n        subject=subject,\n        body=body,\n        lead_name=lead_name,\n        company=company\n    )\n    \n    if result.actually_sent or (result.success and result.mode == \"DRY_RUN\"):\n        lead_event.enrichment_status = ENRICHMENT_STATUS_OUTBOUND_SENT\n        lead_event.status = \"CONTACTED\"\n        lead_event.outbound_message = body\n        lead_event.outbound_subject = subject\n        lead_event.last_contact_at = datetime.utcnow()\n        session.add(lead_event)\n        session.commit()\n        \n        mode_label = \"DRY_RUN\" if result.mode == \"DRY_RUN\" else \"SENT\"\n        print(f\"[MANUAL_OUTBOUND] {mode_label} to {to_email} for lead_event_id={event_id}\")\n        \n        return {\n            \"success\": True,\n            \"event_id\": event_id,\n            \"to_email\": to_email,\n            \"message\": f\"Email {mode_label.lower()} successfully\",\n            \"new_status\": ENRICHMENT_STATUS_OUTBOUND_SENT,\n            \"mode\": result.mode\n        }\n    else:\n        error_msg = result.error or result.result or \"Failed to send email\"\n        print(f\"[MANUAL_OUTBOUND] Failed to send to {to_email}: {error_msg}\")\n        return {\n            \"success\": False,\n            \"event_id\": event_id,\n            \"error\": error_msg\n        }\n\n\n@app.get(\"/api/admin/lead_event/{event_id}/detail\")\ndef get_lead_event_detail_admin(\n    event_id: int,\n    request: Request,\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Get detailed LeadEvent data for admin console.\n    \n    Returns:\n    - Full LeadEvent data\n    - Related Signal context\n    - Related PendingOutbound records\n    - Related Report records\n    - Lead info (if linked)\n    \n    Authenticated via admin session cookie.\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    event = session.exec(\n        select(LeadEvent).where(LeadEvent.id == event_id)\n    ).first()\n    \n    if not event:\n        raise HTTPException(status_code=404, detail=\"Lead event not found\")\n    \n    signal_data = None\n    if event.signal_id:\n        signal = session.exec(\n            select(Signal).where(Signal.id == event.signal_id)\n        ).first()\n        if signal:\n            signal_data = {\n                \"id\": signal.id,\n                \"source_type\": signal.source_type,\n                \"context_summary\": signal.context_summary,\n                \"geography\": signal.geography,\n                \"created_at\": signal.created_at.isoformat() if signal.created_at else None\n            }\n    \n    outbound_records = session.exec(\n        select(PendingOutbound).where(\n            PendingOutbound.lead_event_id == event.id\n        ).order_by(PendingOutbound.created_at.desc())\n    ).all()\n    \n    outbound_list = []\n    for o in outbound_records:\n        outbound_list.append({\n            \"id\": o.id,\n            \"to_email\": o.to_email,\n            \"to_name\": o.to_name,\n            \"subject\": o.subject,\n            \"body\": o.body,\n            \"context_summary\": o.context_summary,\n            \"status\": o.status,\n            \"created_at\": o.created_at.isoformat() if o.created_at else None,\n            \"sent_at\": o.sent_at.isoformat() if o.sent_at else None\n        })\n    \n    report_records = []\n    if event.lead_id:\n        report_records = session.exec(\n            select(Report).where(Report.lead_id == event.lead_id).order_by(Report.created_at.desc())\n        ).all()\n    \n    reports_list = []\n    for r in report_records:\n        reports_list.append({\n            \"id\": r.id,\n            \"title\": r.title,\n            \"description\": r.description,\n            \"content\": r.content,\n            \"report_type\": r.report_type,\n            \"created_at\": r.created_at.isoformat() if r.created_at else None\n        })\n    \n    lead_data = None\n    if event.lead_id:\n        lead = session.exec(\n            select(Lead).where(Lead.id == event.lead_id)\n        ).first()\n        if lead:\n            lead_data = {\n                \"id\": lead.id,\n                \"name\": lead.name,\n                \"email\": lead.email,\n                \"company\": lead.company,\n                \"niche\": lead.niche,\n                \"status\": lead.status,\n                \"website\": lead.website,\n                \"source\": lead.source\n            }\n    \n    company_data = None\n    if event.company_id:\n        customer = session.exec(\n            select(Customer).where(Customer.id == event.company_id)\n        ).first()\n        if customer:\n            company_data = {\n                \"id\": customer.id,\n                \"company\": customer.company,\n                \"email\": customer.contact_email\n            }\n    \n    return {\n        \"id\": event.id,\n        \"summary\": event.summary,\n        \"category\": event.category,\n        \"urgency_score\": event.urgency_score,\n        \"status\": event.status,\n        \"recommended_action\": event.recommended_action,\n        \"outbound_message\": event.outbound_message,\n        \"last_contact_at\": event.last_contact_at.isoformat() if event.last_contact_at else None,\n        \"last_contact_summary\": event.last_contact_summary,\n        \"next_step\": event.next_step,\n        \"next_step_owner\": event.next_step_owner,\n        \"created_at\": event.created_at.isoformat() if event.created_at else None,\n        \"signal\": signal_data,\n        \"outbound_messages\": outbound_list,\n        \"reports\": reports_list,\n        \"lead\": lead_data,\n        \"company\": company_data\n    }\n\n\n# ============================================================================\n# SIGNALNET ADMIN API ENDPOINTS\n# ============================================================================\n\n\n@app.get(\"/api/admin/signalnet/status\")\ndef get_signalnet_status(\n    request: Request,\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Get comprehensive SignalNet status including mode, sources, and recent signals.\n    \n    Returns:\n    - mode: Current SIGNAL_MODE (PRODUCTION/SANDBOX/OFF)\n    - lead_geography: Configured geography filter\n    - lead_niche: Configured niche filter\n    - leadevent_threshold: Score threshold for creating LeadEvents\n    - registry: Status of all registered signal sources\n    - total_signals: Total signals in database\n    - last_pipeline_run: Most recent signal timestamp (approximation)\n    - recent_signals: Last 20 signals with details\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    status = get_signal_status()\n    \n    total_signals = session.exec(select(func.count(Signal.id))).one()\n    \n    last_signal = session.exec(\n        select(Signal).order_by(Signal.created_at.desc()).limit(1)\n    ).first()\n    last_pipeline_run = last_signal.created_at.isoformat() if last_signal else None\n    \n    recent_signals = session.exec(\n        select(Signal).order_by(Signal.created_at.desc()).limit(20)\n    ).all()\n    \n    signals_with_events = []\n    for sig in recent_signals:\n        lead_event = session.exec(\n            select(LeadEvent).where(LeadEvent.signal_id == sig.id)\n        ).first()\n        \n        company_name = None\n        if sig.company_id:\n            customer = session.exec(\n                select(Customer).where(Customer.id == sig.company_id)\n            ).first()\n            if customer:\n                company_name = customer.company\n        \n        category = lead_event.category if lead_event else None\n        score = lead_event.urgency_score if lead_event else None\n        \n        signals_with_events.append({\n            \"id\": sig.id,\n            \"source_type\": sig.source_type,\n            \"context_summary\": sig.context_summary,\n            \"geography\": sig.geography,\n            \"company_id\": sig.company_id,\n            \"company_name\": company_name,\n            \"created_at\": sig.created_at.isoformat() if sig.created_at else None,\n            \"has_lead_event\": lead_event is not None,\n            \"lead_event_id\": lead_event.id if lead_event else None,\n            \"category\": category,\n            \"score\": score,\n            \"status\": getattr(sig, 'status', 'ACTIVE'),\n            \"noisy_pattern\": getattr(sig, 'noisy_pattern', False),\n        })\n    \n    return {\n        \"mode\": status[\"mode\"],\n        \"lead_geography\": status[\"lead_geography\"],\n        \"lead_niche\": status[\"lead_niche\"],\n        \"leadevent_threshold\": status[\"leadevent_threshold\"],\n        \"registry\": status[\"registry\"],\n        \"total_signals\": total_signals,\n        \"last_pipeline_run\": last_pipeline_run,\n        \"recent_signals\": signals_with_events,\n    }\n\n\n@app.post(\"/api/admin/signalnet/mode\")\ndef change_signalnet_mode(\n    request: Request,\n    new_mode: str = Query(..., description=\"New mode: PRODUCTION, SANDBOX, or OFF\"),\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Change SIGNAL_MODE.\n    \n    NOTE: This changes the environment variable at runtime but won't persist after restart.\n    For permanent changes, update the SIGNAL_MODE environment variable in Replit Secrets.\n    \n    Valid modes:\n    - PRODUCTION: Run real sources, create LeadEvents for high-scoring signals\n    - SANDBOX: Run sources and score signals, but don't create LeadEvents\n    - OFF: Skip signal ingestion entirely\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    new_mode_upper = new_mode.upper()\n    if new_mode_upper not in (\"PRODUCTION\", \"SANDBOX\", \"OFF\"):\n        raise HTTPException(status_code=400, detail=\"Invalid mode. Use PRODUCTION, SANDBOX, or OFF\")\n    \n    import os\n    old_mode = os.environ.get(\"SIGNAL_MODE\", \"SANDBOX\")\n    os.environ[\"SIGNAL_MODE\"] = new_mode_upper\n    \n    import signal_sources as ss\n    ss.SIGNAL_MODE = new_mode_upper\n    \n    print(f\"[SIGNALNET][ADMIN] Mode changed: {old_mode} -> {new_mode_upper}\")\n    \n    return {\n        \"success\": True,\n        \"old_mode\": old_mode,\n        \"new_mode\": new_mode_upper,\n        \"message\": f\"SIGNAL_MODE changed to {new_mode_upper}. Note: This is a runtime change. Update SIGNAL_MODE in Replit Secrets for persistence.\"\n    }\n\n\n@app.post(\"/api/admin/signalnet/run\")\ndef run_signalnet_pipeline(\n    request: Request,\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Trigger immediate SignalNet pipeline run.\n    \n    Runs all eligible signal sources through the pipeline:\n    1. Fetch raw signals from each source\n    2. Parse into standardized format\n    3. Score each signal\n    4. Persist to database\n    5. Create LeadEvents for high-scoring signals (PRODUCTION mode only)\n    \n    Returns pipeline execution results.\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    try:\n        result = run_signal_pipeline(session)\n        \n        return {\n            \"success\": True,\n            \"message\": f\"Pipeline complete: {result['signals_persisted']} signals, {result['events_created']} events\",\n            \"result\": result\n        }\n    except Exception as e:\n        print(f\"[SIGNALNET][ADMIN] Pipeline error: {e}\")\n        return {\n            \"success\": False,\n            \"message\": f\"Pipeline error: {str(e)}\",\n            \"result\": None\n        }\n\n\n@app.post(\"/api/admin/signalnet/source/{source_name}/toggle\")\ndef toggle_signalnet_source(\n    source_name: str,\n    request: Request,\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Toggle a signal source enabled/disabled status.\n    \n    NOTE: Source enabled status is determined by the source's `enabled` property,\n    which typically checks API keys and SIGNAL_MODE. This endpoint provides info\n    about the source but cannot directly toggle most sources.\n    \n    For sources like weather_openweather that require API keys, \n    set/unset the environment variable to enable/disable.\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    registry = get_registry()\n    source = registry.get_source(source_name)\n    \n    if not source:\n        raise HTTPException(status_code=404, detail=f\"Source '{source_name}' not found\")\n    \n    return {\n        \"source_name\": source_name,\n        \"source_type\": source.source_type,\n        \"enabled\": source.enabled,\n        \"is_eligible\": source.is_eligible(),\n        \"last_run\": source.last_run.isoformat() if source.last_run else None,\n        \"last_error\": source.last_error,\n        \"items_last_run\": source.items_last_run,\n        \"cooldown_seconds\": source.cooldown_seconds,\n        \"message\": \"Source status retrieved. To enable/disable, configure the required environment variables (API keys, SIGNAL_MODE).\"\n    }\n\n\n@app.post(\"/api/admin/signalnet/clear-old\")\ndef clear_old_signals(\n    request: Request,\n    days: int = Query(default=7, description=\"Delete signals older than this many days\"),\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Clear signals older than specified days.\n    \n    This helps manage database size by removing old signal data.\n    LeadEvents are NOT deleted - only the raw Signal records.\n    \n    Default: 7 days\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    cutoff_date = datetime.utcnow() - timedelta(days=days)\n    \n    old_signals = session.exec(\n        select(Signal).where(Signal.created_at < cutoff_date)\n    ).all()\n    \n    count = len(old_signals)\n    \n    for sig in old_signals:\n        session.delete(sig)\n    \n    session.commit()\n    \n    print(f\"[SIGNALNET][ADMIN] Cleared {count} signals older than {days} days\")\n    \n    return {\n        \"success\": True,\n        \"deleted_count\": count,\n        \"cutoff_date\": cutoff_date.isoformat(),\n        \"message\": f\"Deleted {count} signals older than {days} days\"\n    }\n\n\n@app.post(\"/api/admin/signalnet/signal/{signal_id}/promote\")\ndef promote_signal_to_event(\n    signal_id: int,\n    request: Request,\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Manually promote a signal to a LeadEvent.\n    \n    Creates a new LeadEvent from the signal's context and marks the signal as PROMOTED.\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    signal = session.exec(\n        select(Signal).where(Signal.id == signal_id)\n    ).first()\n    \n    if not signal:\n        raise HTTPException(status_code=404, detail=f\"Signal {signal_id} not found\")\n    \n    existing_event = session.exec(\n        select(LeadEvent).where(LeadEvent.signal_id == signal_id)\n    ).first()\n    \n    if existing_event:\n        return {\n            \"success\": False,\n            \"error\": f\"Signal already has LeadEvent #{existing_event.id}\",\n            \"lead_event_id\": existing_event.id\n        }\n    \n    try:\n        payload = json.loads(signal.raw_payload) if signal.raw_payload else {}\n    except:\n        payload = {}\n    \n    category = payload.get(\"category\", \"OPPORTUNITY\")\n    score = payload.get(\"score\", 65)\n    \n    lead_event = LeadEvent(\n        company_id=signal.company_id,\n        lead_id=signal.lead_id,\n        signal_id=signal.id,\n        summary=signal.context_summary or f\"Manual promotion from signal #{signal.id}\",\n        category=category,\n        urgency_score=score,\n        status=\"NEW\",\n        recommended_action=\"Manual review - promoted by admin\"\n    )\n    \n    session.add(lead_event)\n    \n    signal.status = \"PROMOTED\"\n    session.add(signal)\n    \n    session.commit()\n    session.refresh(lead_event)\n    \n    print(f\"[SIGNALNET][ADMIN] Promoted signal {signal_id} to LeadEvent {lead_event.id}\")\n    \n    return {\n        \"success\": True,\n        \"message\": f\"Signal promoted to LeadEvent #{lead_event.id}\",\n        \"lead_event_id\": lead_event.id,\n        \"signal_id\": signal_id\n    }\n\n\n@app.post(\"/api/admin/signalnet/signal/{signal_id}/discard\")\ndef discard_signal(\n    signal_id: int,\n    request: Request,\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Mark a signal as discarded/ignored.\n    \n    The signal will be marked with status=DISCARDED and hidden from the active stream.\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    signal = session.exec(\n        select(Signal).where(Signal.id == signal_id)\n    ).first()\n    \n    if not signal:\n        raise HTTPException(status_code=404, detail=f\"Signal {signal_id} not found\")\n    \n    signal.status = \"DISCARDED\"\n    session.add(signal)\n    session.commit()\n    \n    print(f\"[SIGNALNET][ADMIN] Discarded signal {signal_id}\")\n    \n    return {\n        \"success\": True,\n        \"message\": f\"Signal #{signal_id} discarded\",\n        \"signal_id\": signal_id\n    }\n\n\n@app.post(\"/api/admin/signalnet/signal/{signal_id}/flag-noisy\")\ndef flag_signal_noisy(\n    signal_id: int,\n    request: Request,\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Flag a signal's source pattern as noisy.\n    \n    Marks the signal with noisy_pattern=True. Future signals from similar\n    source patterns may be suppressed or given lower priority.\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    signal = session.exec(\n        select(Signal).where(Signal.id == signal_id)\n    ).first()\n    \n    if not signal:\n        raise HTTPException(status_code=404, detail=f\"Signal {signal_id} not found\")\n    \n    signal.noisy_pattern = True\n    session.add(signal)\n    session.commit()\n    \n    print(f\"[SIGNALNET][ADMIN] Flagged signal {signal_id} source pattern as noisy (source_type: {signal.source_type})\")\n    \n    return {\n        \"success\": True,\n        \"message\": f\"Signal #{signal_id} flagged as noisy pattern. Source type: {signal.source_type}\",\n        \"signal_id\": signal_id,\n        \"source_type\": signal.source_type\n    }\n\n\n# ============================================================================\n# CUSTOMER PORTAL: MANUAL SEND MODE\n# ============================================================================\n\n@app.post(\"/api/manual-send\")\nasync def manual_send_email(\n    request: Request,\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Manual email send from customer portal.\n    \n    Allows customers to write and send their own emails instead of using robot-generated copy.\n    Records in PendingOutbound with SENT status.\n    \"\"\"\n    payload = await request.json()\n    lead_event_id = payload.get(\"lead_event_id\")\n    to_email = payload.get(\"to_email\")\n    subject = payload.get(\"subject\")\n    body = payload.get(\"body\")\n    \n    customer_token = request.cookies.get(\"customer_id\")\n    if not customer_token:\n        raise HTTPException(status_code=403, detail=\"Authentication required\")\n    \n    customer_id = int(customer_token) if customer_token.isdigit() else None\n    if not customer_id:\n        raise HTTPException(status_code=403, detail=\"Invalid authentication\")\n    \n    customer = session.exec(select(Customer).where(Customer.id == customer_id)).first()\n    if not customer:\n        raise HTTPException(status_code=404, detail=\"Customer not found\")\n    \n    if not all([to_email, subject, body]):\n        raise HTTPException(status_code=400, detail=\"Missing required fields: to_email, subject, body\")\n    \n    from email_utils import send_email\n    \n    result = send_email(\n        to_email=to_email,\n        subject=subject,\n        body=body,\n        lead_name=\"\",\n        company=customer.company,\n        cc_email=\"sam@hossagent.net\",\n        reply_to=\"sam@hossagent.net\"\n    )\n    \n    if not result.actually_sent:\n        return {\n            \"success\": False,\n            \"error\": f\"Email send failed: {result.error or result.result}\"\n        }\n    \n    outbound = PendingOutbound(\n        customer_id=customer_id,\n        lead_event_id=lead_event_id,\n        to_email=to_email,\n        subject=subject,\n        body=body,\n        status=\"SENT\",\n        sent_at=datetime.utcnow()\n    )\n    session.add(outbound)\n    session.commit()\n    \n    if lead_event_id:\n        event = session.exec(select(LeadEvent).where(LeadEvent.id == lead_event_id)).first()\n        if event:\n            event.status = \"CONTACTED\"\n            event.enrichment_status = ENRICHMENT_STATUS_OUTBOUND_SENT\n            event.outbound_message = body\n            event.outbound_subject = subject\n            event.last_contact_at = datetime.utcnow()\n            session.add(event)\n            session.commit()\n            print(f\"[MANUAL-SEND] Updated LeadEvent {lead_event_id} to CONTACTED, enrichment_status=OUTBOUND_SENT\")\n    \n    print(f\"[MANUAL-SEND] Sent email from customer {customer_id} to {to_email}\")\n    \n    return {\n        \"success\": True,\n        \"message\": \"Email sent successfully\",\n        \"outbound_id\": outbound.id\n    }\n\n\n@app.get(\"/api/portal/lead_event/{event_id}/draft\")\nasync def get_customer_draft(\n    event_id: int,\n    request: Request,\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Generate a draft outbound email for a customer's lead event.\n    Returns the generated subject and body for editing before sending.\n    \"\"\"\n    customer_token = request.cookies.get(\"customer_id\")\n    if not customer_token:\n        raise HTTPException(status_code=403, detail=\"Authentication required\")\n    \n    customer_id = int(customer_token) if customer_token.isdigit() else None\n    if not customer_id:\n        raise HTTPException(status_code=403, detail=\"Invalid authentication\")\n    \n    customer = session.exec(select(Customer).where(Customer.id == customer_id)).first()\n    if not customer:\n        raise HTTPException(status_code=404, detail=\"Customer not found\")\n    \n    lead_event = session.exec(\n        select(LeadEvent).where(\n            LeadEvent.id == event_id,\n            LeadEvent.company_id == customer_id\n        )\n    ).first()\n    \n    if not lead_event:\n        raise HTTPException(status_code=404, detail=\"Lead event not found or not owned by customer\")\n    \n    if not lead_event.lead_email:\n        raise HTTPException(status_code=400, detail=\"Lead has no email address\")\n    \n    from agents import generate_miami_contextual_email\n    \n    contact_name = lead_event.lead_name or \"there\"\n    company_name = lead_event.lead_company or \"your company\"\n    niche = customer.niche or \"local service\"\n    \n    subject, body = generate_miami_contextual_email(\n        contact_name=contact_name,\n        company_name=company_name,\n        niche=niche,\n        event_summary=lead_event.summary or \"\",\n        recommended_action=lead_event.recommended_action or \"\",\n        category=lead_event.category or \"\",\n        urgency_score=lead_event.urgency_score or 50,\n        outreach_style=\"transparent_ai\",\n        event_id=event_id,\n        signal_id=lead_event.signal_id\n    )\n    \n    return {\n        \"event_id\": event_id,\n        \"to_email\": lead_event.lead_email,\n        \"to_name\": contact_name,\n        \"company\": company_name,\n        \"subject\": subject,\n        \"body\": body,\n        \"enrichment_status\": lead_event.enrichment_status\n    }\n\n\n# ============================================================================\n# CONVERSATION ENGINE ADMIN API ENDPOINTS\n# ============================================================================\n\n\n@app.get(\"/api/admin/conversations/summary\")\ndef get_conversations_summary(\n    request: Request,\n    session: Session = Depends(get_session)\n):\n    \"\"\"\n    Get conversation engine summary for admin console.\n    \n    Returns:\n    - total_threads: Total conversation threads across all customers\n    - threads_by_status: Count of threads by status (OPEN, HUMAN_OWNED, AUTO, CLOSED)\n    - pending_drafts: Number of AI draft messages awaiting approval\n    - recent_threads: Last 20 threads with key details\n    - metrics: Aggregate conversation metrics\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    total_threads = session.exec(select(func.count(Thread.id))).one()\n    \n    threads_by_status = {}\n    for status in [\"OPEN\", \"HUMAN_OWNED\", \"AUTO\", \"CLOSED\"]:\n        count = session.exec(\n            select(func.count(Thread.id)).where(Thread.status == status)\n        ).one()\n        threads_by_status[status] = count\n    \n    pending_drafts = session.exec(\n        select(func.count(Message.id)).where(Message.status == MESSAGE_STATUS_DRAFT)\n    ).one()\n    \n    recent_threads = session.exec(\n        select(Thread).order_by(Thread.updated_at.desc()).limit(20)\n    ).all()\n    \n    threads_list = []\n    for thread in recent_threads:\n        customer = session.exec(\n            select(Customer).where(Customer.id == thread.customer_id)\n        ).first()\n        \n        threads_list.append({\n            \"id\": thread.id,\n            \"lead_email\": thread.lead_email,\n            \"lead_name\": thread.lead_name,\n            \"lead_company\": thread.lead_company,\n            \"customer_id\": thread.customer_id,\n            \"customer_company\": customer.company if customer else None,\n            \"status\": thread.status,\n            \"message_count\": thread.message_count,\n            \"last_direction\": thread.last_direction,\n            \"last_summary\": thread.last_summary[:80] if thread.last_summary else None,\n            \"updated_at\": thread.updated_at.isoformat() if thread.updated_at else None,\n            \"created_at\": thread.created_at.isoformat() if thread.created_at else None\n        })\n    \n    total_messages = session.exec(select(func.count(Message.id))).one()\n    inbound_count = session.exec(\n        select(func.count(Message.id)).where(Message.direction == \"INBOUND\")\n    ).one()\n    outbound_count = session.exec(\n        select(func.count(Message.id)).where(Message.direction == \"OUTBOUND\")\n    ).one()\n    \n    return {\n        \"total_threads\": total_threads,\n        \"threads_by_status\": threads_by_status,\n        \"pending_drafts\": pending_drafts,\n        \"recent_threads\": threads_list,\n        \"total_messages\": total_messages,\n        \"inbound_count\": inbound_count,\n        \"outbound_count\": outbound_count\n    }\n\n\n@app.get(\"/api/admin/conversations/drafts\")\ndef get_admin_drafts(\n    request: Request,\n    session: Session = Depends(get_session)\n):\n    \"\"\"Get all pending draft messages for admin review.\"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    drafts = session.exec(\n        select(Message).where(Message.status == MESSAGE_STATUS_DRAFT)\n        .order_by(Message.created_at.desc()).limit(50)\n    ).all()\n    \n    drafts_list = []\n    for msg in drafts:\n        customer = session.exec(\n            select(Customer).where(Customer.id == msg.customer_id)\n        ).first()\n        \n        guardrails = []\n        if msg.guardrail_flags:\n            try:\n                guardrails = json.loads(msg.guardrail_flags)\n            except:\n                pass\n        \n        drafts_list.append({\n            \"id\": msg.id,\n            \"thread_id\": msg.thread_id,\n            \"to_email\": msg.to_email,\n            \"subject\": msg.subject,\n            \"body_preview\": msg.body_text[:200] if msg.body_text else None,\n            \"customer_id\": msg.customer_id,\n            \"customer_company\": customer.company if customer else None,\n            \"guardrails\": guardrails,\n            \"created_at\": msg.created_at.isoformat() if msg.created_at else None\n        })\n    \n    return {\"drafts\": drafts_list, \"count\": len(drafts_list)}\n\n\n# ============================================================================\n# ADMIN CONSOLE - CONSOLIDATED API ENDPOINTS\n# ============================================================================\n\n\n@app.get(\"/api/admin/pipeline\")\ndef get_admin_pipeline(request: Request, session: Session = Depends(get_session)):\n    \"\"\"Get unified pipeline of all opportunities (Signal  LeadEvent  Email).\"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    pipeline = []\n    events = session.exec(\n        select(LeadEvent).order_by(LeadEvent.created_at.desc()).limit(100)\n    ).all()\n    \n    for event in events:\n        customer = session.exec(select(Customer).where(Customer.id == event.company_id)).first()\n        outbound = session.exec(\n            select(PendingOutbound).where(PendingOutbound.lead_event_id == event.id)\n            .order_by(PendingOutbound.created_at.desc()).limit(1)\n        ).first()\n        \n        stage = \"Signal\"\n        if event.enriched_at:\n            stage = \"Enriched\"\n        if outbound:\n            stage = \"Email Generated\"\n        if outbound and outbound.status == \"SENT\":\n            stage = \"Email Sent\"\n        \n        result = \"Sent\" if (outbound and outbound.status == \"SENT\") else \"Skipped\" if (outbound and outbound.status == \"SKIPPED\") else \"Suppressed\" if event.do_not_contact else \"Pending\"\n        \n        pipeline.append({\n            \"timestamp\": event.created_at.isoformat(),\n            \"customer\": customer.company if customer else \"Unknown\",\n            \"lead_company\": event.lead_company or \"Unknown\",\n            \"signal_type\": \"SignalNet\" if event.signal_id else \"Manual\",\n            \"stage\": stage,\n            \"result\": result\n        })\n    \n    return pipeline\n\n\n@app.get(\"/api/admin/activity-log\")\ndef get_admin_activity_log(request: Request, session: Session = Depends(get_session)):\n    \"\"\"Get chronological activity log of all events.\"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    activities = []\n    \n    signals = session.exec(\n        select(Signal).order_by(Signal.created_at.desc()).limit(50)\n    ).all()\n    for sig in signals:\n        activities.append({\n            \"timestamp\": sig.created_at.isoformat(),\n            \"event\": \"Signal Detected\",\n            \"customer\": None,\n            \"details\": f\"{sig.source_type}: {sig.context_summary[:60] if sig.context_summary else 'N/A'}\"\n        })\n    \n    events = session.exec(\n        select(LeadEvent).order_by(LeadEvent.created_at.desc()).limit(50)\n    ).all()\n    for evt in events:\n        cust = session.exec(select(Customer).where(Customer.id == evt.company_id)).first()\n        activities.append({\n            \"timestamp\": evt.created_at.isoformat(),\n            \"event\": \"LeadEvent Created\",\n            \"customer\": cust.company if cust else None,\n            \"details\": f\"{evt.lead_company}: {evt.summary[:60] if evt.summary else 'N/A'}\"\n        })\n    \n    outbounds = session.exec(\n        select(PendingOutbound).order_by(PendingOutbound.created_at.desc()).limit(50)\n    ).all()\n    for out in outbounds:\n        cust = session.exec(select(Customer).where(Customer.id == out.customer_id)).first()\n        activities.append({\n            \"timestamp\": out.created_at.isoformat(),\n            \"event\": f\"Email {out.status}\",\n            \"customer\": cust.company if cust else None,\n            \"details\": f\"To: {out.to_email}\"\n        })\n    \n    activities.sort(key=lambda x: x[\"timestamp\"], reverse=True)\n    return activities[:100]\n\n\n@app.get(\"/api/admin/customers-list\")\ndef get_admin_customers_list(request: Request, session: Session = Depends(get_session)):\n    \"\"\"Get all customers with plan/usage info.\"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    customers = session.exec(select(Customer)).all()\n    result = []\n    \n    for cust in customers:\n        plan_status = get_customer_plan_status(cust)\n        result.append({\n            \"company\": cust.company,\n            \"contact_name\": cust.contact_name,\n            \"plan\": \"paid\" if plan_status.is_paid else \"trial\",\n            \"status\": \"Active\" if plan_status.is_paid else (\"Expired\" if plan_status.is_expired else \"Active\"),\n            \"autopilot\": cust.autopilot_enabled,\n            \"outreach_mode\": cust.outreach_mode,\n            \"tasks_used\": plan_status.tasks_used,\n            \"tasks_limit\": plan_status.tasks_limit,\n            \"leads_used\": plan_status.leads_used,\n            \"leads_limit\": plan_status.leads_limit,\n            \"public_token\": cust.public_token\n        })\n    \n    return result\n\n\n@app.get(\"/api/admin/stats\")\ndef get_admin_stats(request: Request, session: Session = Depends(get_session)):\n    \"\"\"Get admin dashboard stats.\"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    from email_utils import get_email_mode\n    \n    today_start = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)\n    \n    signals_today = session.exec(\n        select(func.count(Signal.id)).where(Signal.created_at >= today_start)\n    ).one()\n    \n    lead_events = session.exec(select(func.count(LeadEvent.id))).one()\n    \n    emails_sent = session.exec(\n        select(func.count(PendingOutbound.id)).where(PendingOutbound.status == \"SENT\")\n    ).one()\n    \n    messages_sent = session.exec(\n        select(func.count(Message.id)).where(\n            Message.direction == \"OUTBOUND\",\n            Message.status == \"SENT\"\n        )\n    ).one()\n    \n    customers = session.exec(select(func.count(Customer.id))).one()\n    \n    email_mode = get_email_mode()\n    \n    return {\n        \"signals_today\": signals_today,\n        \"lead_events\": lead_events,\n        \"emails_sent\": emails_sent + messages_sent,\n        \"customers\": customers,\n        \"email_mode\": f\"Email: {email_mode.value}\"\n    }\n\n\n@app.get(\"/api/admin/analytics\")\ndef get_admin_analytics(request: Request):\n    \"\"\"\n    Get analytics dashboard data.\n    \n    Returns page views, funnel metrics, and abandonment data.\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    return get_analytics_summary()\n\n\n@app.get(\"/api/admin/analytics/page-views\")\ndef get_admin_page_views(request: Request, days: int = 7):\n    \"\"\"Get page view statistics.\"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    return get_page_view_stats(days)\n\n\n@app.get(\"/api/admin/analytics/funnel\")\ndef get_admin_funnel(request: Request, days: int = 30):\n    \"\"\"Get conversion funnel statistics.\"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    return get_funnel_stats(days)\n\n\n@app.get(\"/api/admin/sendgrid-stats\")\ndef get_admin_sendgrid_stats(request: Request, days: int = 7):\n    \"\"\"\n    Get SendGrid email delivery statistics.\n    \n    Returns opens, clicks, bounces, and delivery rates.\n    \"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    return get_sendgrid_stats(days)\n\n\n@app.get(\"/api/admin/signals\")\ndef get_admin_signals(request: Request, session: Session = Depends(get_session)):\n    \"\"\"Get raw signals from SignalNet.\"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    signals = session.exec(\n        select(Signal).order_by(Signal.created_at.desc()).limit(100)\n    ).all()\n    \n    result = []\n    for sig in signals:\n        lead_event = session.exec(\n            select(LeadEvent).where(LeadEvent.signal_id == sig.id)\n        ).first()\n        \n        score = lead_event.urgency_score if lead_event else 0\n        event_created = lead_event is not None\n        \n        result.append({\n            \"id\": sig.id,\n            \"source_type\": sig.source_type or \"\",\n            \"geography\": sig.geography or \"\",\n            \"score\": score,\n            \"summary\": (sig.context_summary[:150] if sig.context_summary else \"\") or \"\",\n            \"event_created\": event_created,\n            \"created_at\": sig.created_at.isoformat() if sig.created_at else None\n        })\n    \n    return result\n\n\n@app.get(\"/api/admin/lead-events\")\ndef get_admin_lead_events(request: Request, session: Session = Depends(get_session)):\n    \"\"\"Get converted lead events.\"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    events = session.exec(\n        select(LeadEvent).order_by(LeadEvent.created_at.desc()).limit(100)\n    ).all()\n    \n    result = []\n    for evt in events:\n        customer = session.exec(\n            select(Customer).where(Customer.id == evt.company_id)\n        ).first()\n        \n        result.append({\n            \"id\": evt.id,\n            \"customer\": customer.company if customer else None,\n            \"lead_company\": evt.lead_company,\n            \"lead_email\": evt.lead_email,\n            \"lead_domain\": evt.lead_domain,\n            \"category\": evt.category,\n            \"urgency_score\": evt.urgency_score or 0,\n            \"status\": evt.status,\n            \"enrichment_status\": evt.enrichment_status,\n            \"created_at\": evt.created_at.isoformat() if evt.created_at else None\n        })\n    \n    return result\n\n\n@app.get(\"/api/admin/outbound-messages\")\ndef get_admin_outbound_messages(request: Request, session: Session = Depends(get_session)):\n    \"\"\"Get outbound email messages.\"\"\"\n    admin_token = request.cookies.get(ADMIN_COOKIE_NAME)\n    if not verify_admin_session(admin_token):\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    outbound_pending = session.exec(\n        select(PendingOutbound).order_by(PendingOutbound.created_at.desc()).limit(50)\n    ).all()\n    \n    outbound_messages = session.exec(\n        select(Message).where(Message.direction == \"OUTBOUND\")\n        .order_by(Message.created_at.desc()).limit(50)\n    ).all()\n    \n    result = []\n    \n    for out in outbound_pending:\n        customer = session.exec(\n            select(Customer).where(Customer.id == out.customer_id)\n        ).first()\n        result.append({\n            \"id\": out.id,\n            \"type\": \"pending\",\n            \"customer\": customer.company if customer else None,\n            \"to_email\": out.to_email,\n            \"subject\": out.subject or \"\",\n            \"status\": out.status,\n            \"created_at\": out.created_at.isoformat() if out.created_at else None\n        })\n    \n    for msg in outbound_messages:\n        customer = session.exec(\n            select(Customer).where(Customer.id == msg.customer_id)\n        ).first()\n        if not any(r[\"to_email\"] == msg.to_email and r[\"subject\"] == msg.subject for r in result):\n            result.append({\n                \"id\": msg.id,\n                \"type\": \"message\",\n                \"customer\": customer.company if customer else None,\n                \"to_email\": msg.to_email,\n                \"subject\": msg.subject or \"\",\n                \"status\": msg.status,\n                \"created_at\": msg.created_at.isoformat() if msg.created_at else None\n            })\n    \n    result.sort(key=lambda x: x[\"created_at\"] or \"\", reverse=True)\n    return result[:100]\n\n\n@app.get(\"/admin/logout\")\ndef admin_logout():\n    \"\"\"Logout admin and clear cookie.\"\"\"\n    response = RedirectResponse(url=\"/admin/login\", status_code=303)\n    response.delete_cookie(ADMIN_COOKIE_NAME)\n    return response\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=5000)\n\n","path":null,"size_bytes":241637,"size_tokens":null},"models.py":{"content":"from datetime import datetime\nfrom typing import Optional\nfrom sqlmodel import SQLModel, Field\n\n\nclass SystemSettings(SQLModel, table=True):\n    \"\"\"Global system configuration flags.\"\"\"\n    id: Optional[int] = Field(default=None, primary_key=True)\n    autopilot_enabled: bool = Field(default=True)\n    outbound_autopilot_enabled: bool = Field(default=True)\n\n\nLEAD_STATUS_NEW = \"NEW\"\nLEAD_STATUS_CONTACTED = \"CONTACTED\"\nLEAD_STATUS_RESPONDED = \"RESPONDED\"\nLEAD_STATUS_QUALIFIED = \"QUALIFIED\"\nLEAD_STATUS_CLOSED_WON = \"CLOSED_WON\"\nLEAD_STATUS_CLOSED_LOST = \"CLOSED_LOST\"\nLEAD_STATUS_ON_HOLD = \"ON_HOLD\"\n\nNEXT_STEP_OWNER_AGENT = \"AGENT\"\nNEXT_STEP_OWNER_CUSTOMER = \"CUSTOMER\"\n\n\nclass Lead(SQLModel, table=True):\n    \"\"\"\n    Lead model with full lifecycle tracking.\n    \n    Status Flow:\n    NEW -> CONTACTED -> RESPONDED -> QUALIFIED -> CLOSED_WON/CLOSED_LOST\n    Any status can move to ON_HOLD\n    \"\"\"\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str\n    email: str\n    company: str\n    niche: str\n    status: str = \"NEW\"  # NEW, CONTACTED, RESPONDED, QUALIFIED, CLOSED_WON, CLOSED_LOST, ON_HOLD\n    website: Optional[str] = None\n    source: Optional[str] = None  # signalnet, hossnative, manual\n    \n    last_contacted_at: Optional[datetime] = None\n    last_contact_summary: Optional[str] = None\n    next_step: Optional[str] = None\n    next_step_owner: Optional[str] = None  # AGENT or CUSTOMER\n    \n    created_at: datetime = Field(default_factory=datetime.utcnow)\n\n\nOUTREACH_MODE_AUTO = \"AUTO\"\nOUTREACH_MODE_REVIEW = \"REVIEW\"\n\n\nclass Customer(SQLModel, table=True):\n    \"\"\"\n    Customer model with subscription/trial support and authentication.\n    \n    Plan Types:\n    - \"trial\": 7-day restricted trial (limited tasks/leads, DRY_RUN email, no billing)\n    - \"paid\": Full access with $99/month subscription\n    - \"trial_expired\": Trial ended without upgrade\n    \n    Subscription Status:\n    - \"none\": No active subscription\n    - \"active\": Subscription active\n    - \"past_due\": Payment failed\n    - \"canceled\": Subscription canceled\n    \n    Outreach Mode:\n    - \"AUTO\": System sends emails automatically\n    - \"REVIEW\": Customer must approve each outbound before sending\n    \"\"\"\n    id: Optional[int] = Field(default=None, primary_key=True)\n    company: str\n    contact_email: str\n    contact_name: Optional[str] = None\n    \n    password_hash: Optional[str] = None\n    \n    plan: str = \"trial\"  # trial, paid, trial_expired\n    billing_plan: str = \"starter\"  # starter, pro, enterprise (legacy)\n    billing_method: Optional[str] = None  # stripe, admin_override\n    status: str = \"active\"  # active, trial, paused (legacy)\n    \n    trial_start_at: Optional[datetime] = None\n    trial_end_at: Optional[datetime] = None\n    \n    subscription_status: str = \"none\"  # none, active, past_due, canceled\n    stripe_customer_id: Optional[str] = None\n    stripe_subscription_id: Optional[str] = None\n    cancelled_at_period_end: bool = Field(default=False)\n    cancellation_effective_at: Optional[datetime] = None\n    \n    outreach_mode: str = Field(default=\"AUTO\")  # AUTO or REVIEW\n    outreach_style: str = Field(default=\"transparent_ai\")  # transparent_ai or classic\n    autopilot_enabled: bool = Field(default=True)  # Per-customer autopilot toggle\n    \n    time_zone: Optional[str] = Field(default=None)  # IANA timezone e.g. \"America/New_York\"\n    \n    niche: Optional[str] = None\n    geography: Optional[str] = None\n    \n    tasks_this_period: int = Field(default=0)\n    leads_this_period: int = Field(default=0)\n    \n    public_token: Optional[str] = None  # For customer portal access\n    notes: Optional[str] = None\n    created_at: datetime = Field(default_factory=datetime.utcnow)\n\n\nclass TrialIdentity(SQLModel, table=True):\n    \"\"\"\n    Trial abuse prevention tracking.\n    Records fingerprints to prevent multiple trial signups from same user/device.\n    \"\"\"\n    id: Optional[int] = Field(default=None, primary_key=True)\n    email: str = Field(index=True)\n    ip_address: Optional[str] = Field(default=None, index=True)\n    user_agent_hash: Optional[str] = None\n    device_fingerprint: Optional[str] = Field(default=None, index=True)\n    customer_id: Optional[int] = Field(default=None, foreign_key=\"customer.id\")\n    blocked: bool = Field(default=False)\n    block_reason: Optional[str] = None\n    created_at: datetime = Field(default_factory=datetime.utcnow)\n\n\nclass Task(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    customer_id: int = Field(foreign_key=\"customer.id\")\n    description: str\n    status: str = \"pending\"  # pending, running, done, failed\n    reward_cents: int\n    cost_cents: int = 0\n    profit_cents: int = 0\n    result_summary: Optional[str] = None\n    created_at: datetime = Field(default_factory=datetime.utcnow)\n    completed_at: Optional[datetime] = None\n\n\nclass Invoice(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    customer_id: int = Field(foreign_key=\"customer.id\")\n    amount_cents: int\n    status: str = \"draft\"  # draft, sent, paid\n    payment_url: Optional[str] = None  # Stripe payment link URL\n    stripe_payment_id: Optional[str] = None  # Stripe payment link ID\n    created_at: datetime = Field(default_factory=datetime.utcnow)\n    paid_at: Optional[datetime] = None\n    notes: Optional[str] = None\n\n\nclass BusinessProfile(SQLModel, table=True):\n    \"\"\"\n    Business profile for a customer - defines how outreach is personalized.\n    \n    1:1 relationship with Customer.\n    Used for CC/Reply-To, voice/tone, do-not-contact list, etc.\n    \"\"\"\n    id: Optional[int] = Field(default=None, primary_key=True)\n    customer_id: int = Field(foreign_key=\"customer.id\", unique=True)\n    \n    short_description: Optional[str] = None\n    services: Optional[str] = None  # JSON or comma-separated\n    pricing_notes: Optional[str] = None\n    ideal_customer: Optional[str] = None\n    excluded_customers: Optional[str] = None\n    \n    voice_tone: Optional[str] = None  # e.g., \"professional\", \"friendly\", \"casual\"\n    communication_style: Optional[str] = None  # e.g., \"formal\", \"conversational\"\n    constraints: Optional[str] = None  # Any restrictions on messaging\n    \n    primary_contact_name: Optional[str] = None\n    primary_contact_email: Optional[str] = None  # Used for CC + Reply-To\n    \n    do_not_contact_list: Optional[str] = None  # JSON array or comma-separated emails/domains\n    \n    created_at: datetime = Field(default_factory=datetime.utcnow)\n    updated_at: datetime = Field(default_factory=datetime.utcnow)\n\n\nclass Report(SQLModel, table=True):\n    \"\"\"\n    Generated reports for customers.\n    \n    Reports can be research summaries, competitive analyses, market insights, etc.\n    Displayed in the portal under \"Reports / Recent Work\" section.\n    \n    Can be linked to a LeadEvent via lead_event_id for opportunity-specific reports.\n    \"\"\"\n    id: Optional[int] = Field(default=None, primary_key=True)\n    customer_id: int = Field(foreign_key=\"customer.id\")\n    lead_id: Optional[int] = Field(default=None, foreign_key=\"lead.id\")\n    lead_event_id: Optional[int] = Field(default=None, foreign_key=\"leadevent.id\")\n    \n    title: str\n    description: Optional[str] = None\n    content: Optional[str] = None  # Full report content or JSON\n    report_type: str = \"general\"  # research, competitive, market, opportunity\n    \n    created_at: datetime = Field(default_factory=datetime.utcnow)\n\n\nclass PendingOutbound(SQLModel, table=True):\n    \"\"\"\n    Pending outbound emails for REVIEW mode customers.\n    \n    When outreach_mode=\"REVIEW\", emails are queued here for customer approval.\n    Actions: PENDING, APPROVED, SKIPPED, SENT\n    \"\"\"\n    id: Optional[int] = Field(default=None, primary_key=True)\n    customer_id: int = Field(foreign_key=\"customer.id\")\n    lead_id: Optional[int] = Field(default=None, foreign_key=\"lead.id\")\n    lead_event_id: Optional[int] = Field(default=None, foreign_key=\"leadevent.id\")\n    \n    to_email: str\n    to_name: Optional[str] = None\n    subject: str\n    body: str\n    \n    context_summary: Optional[str] = None  # Why this email is being sent\n    \n    status: str = \"PENDING\"  # PENDING, APPROVED, SKIPPED, SENT\n    approved_at: Optional[datetime] = None\n    sent_at: Optional[datetime] = None\n    skipped_reason: Optional[str] = None\n    \n    created_at: datetime = Field(default_factory=datetime.utcnow)\n\n\nclass PasswordResetToken(SQLModel, table=True):\n    \"\"\"\n    Password reset tokens for forgot password flow.\n    \n    Tokens expire after 1 hour.\n    \"\"\"\n    id: Optional[int] = Field(default=None, primary_key=True)\n    customer_id: int = Field(foreign_key=\"customer.id\")\n    token: str = Field(index=True)\n    expires_at: datetime\n    used: bool = Field(default=False)\n    created_at: datetime = Field(default_factory=datetime.utcnow)\n\n\nSIGNAL_STATUS_ACTIVE = \"ACTIVE\"\nSIGNAL_STATUS_DISCARDED = \"DISCARDED\"\nSIGNAL_STATUS_PROMOTED = \"PROMOTED\"\n\n\nclass Signal(SQLModel, table=True):\n    \"\"\"\n    Signals Engine: Captures external context signals about companies.\n    \n    Sources include competitor updates, job postings, reviews, permits, weather events.\n    Each signal can generate one or more LeadEvents for actionable opportunities.\n    \n    Status:\n    - ACTIVE: Signal is active and visible\n    - DISCARDED: Signal was manually discarded by admin\n    - PROMOTED: Signal was manually promoted to a LeadEvent\n    \n    Metadata stores extracted contact info from source (URLs, emails, phones).\n    \"\"\"\n    id: Optional[int] = Field(default=None, primary_key=True)\n    company_id: Optional[int] = Field(default=None, foreign_key=\"customer.id\")\n    lead_id: Optional[int] = Field(default=None, foreign_key=\"lead.id\")\n    source_type: str  # job_posting, review, competitor_update, permit, weather, news\n    raw_payload: str  # JSON string of raw signal data\n    context_summary: Optional[str] = None  # LLM-generated summary\n    geography: Optional[str] = None  # Miami, Broward, etc.\n    status: str = Field(default=\"ACTIVE\")  # ACTIVE, DISCARDED, PROMOTED\n    noisy_pattern: bool = Field(default=False)  # Flagged as noisy source pattern\n    extracted_contact_info: Optional[str] = None  # JSON string: {extracted_urls, extracted_emails, extracted_phones, source_confidence}\n    created_at: datetime = Field(default_factory=datetime.utcnow)\n\n\n# Enrichment status constants - Lifecycle states for LeadEvents\n# Customer-facing states: ENRICHED_NO_OUTBOUND (review mode), OUTBOUND_SENT\n# Admin-only states: UNENRICHED, WITH_DOMAIN_NO_EMAIL, WITH_PHONE_ONLY\nENRICHMENT_STATUS_UNENRICHED = \"UNENRICHED\"  # No domain discovered yet\nENRICHMENT_STATUS_WITH_DOMAIN_NO_EMAIL = \"WITH_DOMAIN_NO_EMAIL\"  # Domain found, email not yet discovered\nENRICHMENT_STATUS_WITH_PHONE_ONLY = \"WITH_PHONE_ONLY\"  # Phone found but no email (PHONESTORM)\nENRICHMENT_STATUS_ENRICHED_NO_OUTBOUND = \"ENRICHED_NO_OUTBOUND\"  # Email found, awaiting outbound\nENRICHMENT_STATUS_OUTBOUND_SENT = \"OUTBOUND_SENT\"  # Outbound email sent successfully\nENRICHMENT_STATUS_ARCHIVED = \"ARCHIVED\"  # Archived (stale or manually archived)\nENRICHMENT_STATUS_ARCHIVED_UNENRICHABLE = \"ARCHIVED_UNENRICHABLE\"  # Exhausted all enrichment attempts\n\n# Unenrichable reason constants - Why a lead cannot be enriched\nUNENRICHABLE_REASON_NO_DOMAIN = \"NO_DOMAIN\"  # Could not discover domain after max attempts\nUNENRICHABLE_REASON_NO_CONTACT_INFO = \"NO_CONTACT_INFO\"  # Domain found but no email/phone discovered\nUNENRICHABLE_REASON_NO_OSINT_PRESENCE = \"NO_OSINT_PRESENCE\"  # No web presence found\nUNENRICHABLE_REASON_BLOCKED_DOMAIN = \"BLOCKED_DOMAIN\"  # Domain is in blocked list\nUNENRICHABLE_REASON_INVALID_COMPANY = \"INVALID_COMPANY\"  # Company name extraction failed completely\n\n# Legacy status mappings (for backward compatibility during transition)\nENRICHMENT_STATUS_ENRICHING = \"ENRICHING\"  # Deprecated - maps to UNENRICHED\nENRICHMENT_STATUS_ENRICHED = \"ENRICHED\"  # Deprecated - maps to ENRICHED_NO_OUTBOUND\nENRICHMENT_STATUS_OUTBOUND_READY = \"OUTBOUND_READY\"  # Deprecated - maps to ENRICHED_NO_OUTBOUND\nENRICHMENT_STATUS_FAILED = \"FAILED\"  # Deprecated - maps to UNENRICHED (retry)\nENRICHMENT_STATUS_SKIPPED = \"SKIPPED\"  # Deprecated - maps to UNENRICHED (no domain)\n\n# Default enrichment budget\nDEFAULT_MAX_ENRICHMENT_ATTEMPTS = 3\n\n\nclass LeadEvent(SQLModel, table=True):\n    \"\"\"\n    Signals Engine: Actionable opportunities derived from Signals.\n    \n    Each event represents a contextual moment for outreach.\n    Categories are Miami-tuned: HURRICANE_SEASON, COMPETITOR_SHIFT, GROWTH_SIGNAL, etc.\n    \n    Lifecycle States (Domain-First Pipeline):\n    - UNENRICHED: No domain discovered yet (admin-only)\n    - WITH_DOMAIN_NO_EMAIL: Domain found but no email discovered (admin-only)\n    - ENRICHED_NO_OUTBOUND: Email found, awaiting outbound (review mode visible)\n    - OUTBOUND_SENT: Outbound email sent (customer visible)\n    - ARCHIVED: Stale or manually archived (hidden)\n    \n    Customer Portal only shows: OUTBOUND_SENT (and ENRICHED_NO_OUTBOUND in REVIEW mode)\n    Admin Console shows all states for debugging.\n    \"\"\"\n    id: Optional[int] = Field(default=None, primary_key=True)\n    company_id: Optional[int] = Field(default=None, foreign_key=\"customer.id\")  # Customer who owns this lead\n    lead_id: Optional[int] = Field(default=None, foreign_key=\"lead.id\")\n    signal_id: Optional[int] = Field(default=None, foreign_key=\"signal.id\")\n    macro_event_id: Optional[int] = Field(default=None, foreign_key=\"macroevent.id\")  # EPIC 5: MacroStorm source\n    company_table_id: Optional[int] = Field(default=None, foreign_key=\"company.id\")  # EPIC 2: Canonical company link\n    lead_name: Optional[str] = None\n    lead_email: Optional[str] = None\n    lead_company: Optional[str] = None\n    lead_domain: Optional[str] = None\n    summary: str  # Human-readable opportunity description\n    category: str  # growth, risk, competitor_move, opportunity, hurricane_season, bilingual_opportunity\n    urgency_score: int = Field(default=50)  # 0-100, higher = more urgent\n    status: str = \"NEW\"  # NEW, CONTACTED, RESPONDED, QUALIFIED, CLOSED_WON, CLOSED_LOST, ON_HOLD\n    recommended_action: Optional[str] = None  # What the system suggests\n    outbound_message: Optional[str] = None  # Generated email if contacted\n    outbound_subject: Optional[str] = None  # Subject line of sent email\n    \n    enrichment_status: Optional[str] = Field(default=\"UNENRICHED\")  # UNENRICHED, ENRICHING, ENRICHED, OUTBOUND_READY, FAILED, SKIPPED, ARCHIVED_UNENRICHABLE\n    enrichment_source: Optional[str] = None  # hunter, clearbit, scrape, signal, manual\n    enrichment_attempts: int = Field(default=0)\n    max_enrichment_attempts: int = Field(default=3)  # ARCHANGEL v2: Budget limit for attempts\n    last_enrichment_at: Optional[datetime] = None\n    enriched_email: Optional[str] = None\n    enriched_phone: Optional[str] = None\n    enriched_contact_name: Optional[str] = None\n    enriched_company_name: Optional[str] = None\n    enriched_social_links: Optional[str] = None  # JSON string of social links (legacy)\n    enriched_at: Optional[datetime] = None\n    \n    # ARCHANGEL v2: Unenrichable tracking\n    unenrichable_reason: Optional[str] = None  # NO_DOMAIN, NO_CONTACT_INFO, NO_OSINT_PRESENCE, BLOCKED_DOMAIN, INVALID_COMPANY\n    \n    # ARCHANGEL v2: Mission Log - JSON array of enrichment attempt records\n    enrichment_mission_log: Optional[str] = None  # JSON: [{timestamp, pass, phase, action, query, result, notes}]\n    \n    # ARCHANGEL v2: Multi-candidate company names\n    candidate_company_names: Optional[str] = None  # JSON: [{name, confidence, source, raw_match}]\n    \n    # ARCHANGEL Confidence Scoring\n    company_name_candidate: Optional[str] = None  # Extracted company name from signal/title (best candidate)\n    domain_confidence: float = Field(default=0.0)  # 0-1.0, domain match confidence score\n    email_confidence: float = Field(default=0.0)  # 0-1.0, email validity and context score\n    social_facebook: Optional[str] = None\n    social_instagram: Optional[str] = None\n    social_linkedin: Optional[str] = None\n    social_twitter: Optional[str] = None\n    \n    # PHONESTORM: Phone enrichment fields\n    lead_phone_raw: Optional[str] = None  # Original extracted phone number\n    lead_phone_e164: Optional[str] = None  # Normalized E.164 format (+1XXXYYYZZZZ)\n    phone_confidence: float = Field(default=0.0)  # 0-1.0, phone validity score\n    phone_source: Optional[str] = None  # contact_page, homepage, footer, schema, tel_link\n    phone_type: Optional[str] = None  # mobile, landline, voip, tollfree, unknown\n    \n    last_contact_at: Optional[datetime] = None\n    last_contact_summary: Optional[str] = None\n    next_step: Optional[str] = None\n    next_step_owner: Optional[str] = None  # AGENT or CUSTOMER\n    \n    do_not_contact: bool = Field(default=False)\n    do_not_contact_reason: Optional[str] = None\n    do_not_contact_at: Optional[datetime] = None\n    contact_count_24h: int = Field(default=0)\n    contact_count_7d: int = Field(default=0)\n    last_subject_hash: Optional[str] = None\n    \n    created_at: datetime = Field(default_factory=datetime.utcnow)\n\n\nTRIAL_TASK_LIMIT = 15\nTRIAL_LEAD_LIMIT = 20\nSUBSCRIPTION_PRICE_CENTS = 9900  # $99/month\nTRIAL_DAYS = 7\n\nMAX_OUTBOUND_PER_LEAD_PER_DAY = 1\nMAX_OUTBOUND_PER_LEAD_PER_WEEK = 3\nMAX_OUTBOUND_PER_CUSTOMER_PER_DAY = 100\n\nOUTREACH_STYLE_TRANSPARENT = \"transparent_ai\"\nOUTREACH_STYLE_CLASSIC = \"classic\"\n\nOPT_OUT_PHRASES = [\n    \"no thanks\", \"no thank you\", \"unsubscribe\", \"stop\", \"remove me\",\n    \"remove my email\", \"don't contact\", \"do not contact\", \"please stop\",\n    \"take me off\", \"opt out\", \"not interested\", \"leave me alone\"\n]\n\n\nclass SignalLog(SQLModel, table=True):\n    \"\"\"\n    Structured logging for signal source activity.\n    \n    Captures all signal pipeline operations for debugging and monitoring.\n    Actions: fetch, parse, score, persist, error, dry_run, auto_disable, reset\n    \"\"\"\n    id: Optional[int] = Field(default=None, primary_key=True)\n    timestamp: datetime = Field(default_factory=datetime.utcnow)\n    source_name: str = Field(index=True)\n    action: str = Field(index=True)\n    details: Optional[str] = None\n    signal_count: int = Field(default=0)\n    error_message: Optional[str] = None\n    dry_run: bool = Field(default=False)\n    created_at: datetime = Field(default_factory=datetime.utcnow)\n\n\n# ============================================================\n# CONVERSATION ENGINE MODELS\n# ============================================================\n\n# Thread status constants\nTHREAD_STATUS_OPEN = \"OPEN\"\nTHREAD_STATUS_HUMAN_OWNED = \"HUMAN_OWNED\"\nTHREAD_STATUS_AUTO = \"AUTO\"\nTHREAD_STATUS_CLOSED = \"CLOSED\"\n\n# Message direction constants\nMESSAGE_DIRECTION_INBOUND = \"INBOUND\"\nMESSAGE_DIRECTION_OUTBOUND = \"OUTBOUND\"\n\n# Message status constants (for outbound)\nMESSAGE_STATUS_QUEUED = \"QUEUED\"\nMESSAGE_STATUS_SENT = \"SENT\"\nMESSAGE_STATUS_DRAFT = \"DRAFT\"\nMESSAGE_STATUS_FAILED = \"FAILED\"\nMESSAGE_STATUS_APPROVED = \"APPROVED\"\n\n# Message generated by\nMESSAGE_GENERATED_AI = \"AI\"\nMESSAGE_GENERATED_HUMAN = \"HUMAN\"\nMESSAGE_GENERATED_SYSTEM = \"SYSTEM\"\n\n# Auto-reply level\nAUTO_REPLY_NONE = \"NONE\"\nAUTO_REPLY_SAFE_ONLY = \"SAFE_ONLY\"\nAUTO_REPLY_AGGRESSIVE = \"AGGRESSIVE\"\n\n\nclass Thread(SQLModel, table=True):\n    \"\"\"\n    Conversation thread linking all messages between HossAgent and a lead.\n    \n    Each LeadEvent can have one primary thread.\n    All inbound/outbound messages for a lead+customer share the same thread.\n    \n    Status:\n    - OPEN: Active conversation, AI can auto-respond (if allowed)\n    - HUMAN_OWNED: Customer took over, AI proposes drafts but doesn't auto-send\n    - AUTO: Thread in automated mode (AI handles responses)\n    - CLOSED: Thread is closed (opted out, completed, etc.)\n    \"\"\"\n    id: Optional[int] = Field(default=None, primary_key=True)\n    lead_id: Optional[int] = Field(default=None, foreign_key=\"lead.id\", index=True)\n    lead_event_id: Optional[int] = Field(default=None, foreign_key=\"leadevent.id\", index=True)\n    customer_id: int = Field(foreign_key=\"customer.id\", index=True)\n    \n    status: str = Field(default=\"OPEN\", index=True)  # OPEN, HUMAN_OWNED, AUTO, CLOSED\n    \n    lead_email: Optional[str] = Field(default=None, index=True)\n    lead_name: Optional[str] = None\n    lead_company: Optional[str] = None\n    \n    last_message_at: Optional[datetime] = None\n    last_direction: Optional[str] = None  # INBOUND or OUTBOUND\n    last_summary: Optional[str] = None  # Short preview for UI\n    \n    message_count: int = Field(default=0)\n    inbound_count: int = Field(default=0)\n    outbound_count: int = Field(default=0)\n    \n    first_response_at: Optional[datetime] = None  # When lead first replied\n    response_time_seconds: Optional[int] = None  # Time to first reply\n    \n    closed_reason: Optional[str] = None  # opt_out, completed, manual, etc.\n    closed_at: Optional[datetime] = None\n    \n    created_at: datetime = Field(default_factory=datetime.utcnow)\n    updated_at: datetime = Field(default_factory=datetime.utcnow)\n\n\nclass Message(SQLModel, table=True):\n    \"\"\"\n    Individual email message in a thread (inbound or outbound).\n    \n    Stores both received replies and sent/draft outbound messages.\n    \n    Direction:\n    - INBOUND: Received from lead\n    - OUTBOUND: Sent to lead (or draft)\n    \n    Status (outbound only):\n    - QUEUED: Ready to send\n    - SENT: Successfully sent\n    - DRAFT: AI-generated, awaiting approval\n    - FAILED: Send failed\n    - APPROVED: Approved by customer, ready to send\n    \"\"\"\n    id: Optional[int] = Field(default=None, primary_key=True)\n    thread_id: Optional[int] = Field(default=None, foreign_key=\"thread.id\", index=True)\n    lead_id: Optional[int] = Field(default=None, foreign_key=\"lead.id\")\n    lead_event_id: Optional[int] = Field(default=None, foreign_key=\"leadevent.id\")\n    customer_id: Optional[int] = Field(default=None, foreign_key=\"customer.id\", index=True)\n    \n    direction: str = Field(index=True)  # INBOUND or OUTBOUND\n    \n    from_email: str\n    to_email: str\n    cc: Optional[str] = None  # JSON array or comma-separated\n    reply_to: Optional[str] = None\n    \n    subject: str\n    body_text: str\n    body_html: Optional[str] = None\n    \n    # Email threading headers\n    message_id: Optional[str] = Field(default=None, index=True)  # Email Message-ID header\n    in_reply_to: Optional[str] = Field(default=None, index=True)  # References header\n    references: Optional[str] = None  # Full references chain\n    \n    # Outbound-specific fields\n    status: Optional[str] = Field(default=None, index=True)  # QUEUED, SENT, DRAFT, FAILED, APPROVED\n    generated_by: Optional[str] = None  # AI, HUMAN, SYSTEM\n    \n    # Guardrails\n    guardrail_flags: Optional[str] = None  # JSON: which guardrails triggered\n    guardrail_approved: bool = Field(default=False)  # Human approved despite guardrails\n    \n    # Metadata\n    raw_metadata: Optional[str] = None  # JSON: headers, MIME info, etc.\n    sendgrid_message_id: Optional[str] = None  # SendGrid X-Message-Id for tracking\n    \n    sent_at: Optional[datetime] = None\n    approved_at: Optional[datetime] = None\n    approved_by: Optional[str] = None  # customer_id or \"admin\"\n    \n    created_at: datetime = Field(default_factory=datetime.utcnow)\n\n\nclass Suppression(SQLModel, table=True):\n    \"\"\"\n    Global and per-customer suppression list.\n    \n    Prevents any future outbound to emails/domains on this list.\n    Can be triggered by:\n    - Opt-out keywords in replies\n    - Manual addition by customer\n    - Bounce/complaint handling\n    \"\"\"\n    id: Optional[int] = Field(default=None, primary_key=True)\n    customer_id: Optional[int] = Field(default=None, foreign_key=\"customer.id\", index=True)\n    \n    email: Optional[str] = Field(default=None, index=True)  # Specific email to suppress\n    domain: Optional[str] = Field(default=None, index=True)  # Entire domain to suppress\n    \n    reason: str  # opt_out, bounce, complaint, manual, etc.\n    source_message_id: Optional[int] = Field(default=None, foreign_key=\"message.id\")\n    source_thread_id: Optional[int] = Field(default=None, foreign_key=\"thread.id\")\n    \n    is_global: bool = Field(default=False)  # True = applies to all customers\n    \n    created_at: datetime = Field(default_factory=datetime.utcnow)\n\n\nclass ConversationMetrics(SQLModel, table=True):\n    \"\"\"\n    Aggregated conversation metrics per customer.\n    \n    Updated periodically to track performance.\n    \"\"\"\n    id: Optional[int] = Field(default=None, primary_key=True)\n    customer_id: int = Field(foreign_key=\"customer.id\", unique=True, index=True)\n    \n    # Lead funnel\n    total_lead_events: int = Field(default=0)\n    total_threads: int = Field(default=0)\n    \n    # Reply rates\n    leads_contacted: int = Field(default=0)\n    leads_replied: int = Field(default=0)\n    reply_rate_pct: float = Field(default=0.0)  # (replied / contacted) * 100\n    \n    # Response time\n    avg_response_time_seconds: Optional[int] = None\n    \n    # Message volume\n    total_outbound: int = Field(default=0)\n    total_inbound: int = Field(default=0)\n    \n    # AI vs Human\n    messages_ai_drafted: int = Field(default=0)\n    messages_ai_sent_auto: int = Field(default=0)\n    messages_human_edited: int = Field(default=0)\n    messages_human_sent: int = Field(default=0)\n    \n    # Thread outcomes\n    threads_human_owned: int = Field(default=0)\n    threads_closed_opt_out: int = Field(default=0)\n    threads_closed_completed: int = Field(default=0)\n    \n    # Depth\n    avg_thread_depth: float = Field(default=0.0)  # Avg messages per thread\n    \n    last_calculated_at: datetime = Field(default_factory=datetime.utcnow)\n    created_at: datetime = Field(default_factory=datetime.utcnow)\n\n\n# ============================================================\n# EPIC 2: COMPANY INTELLIGENCE LAYER\n# ============================================================\n\nclass Company(SQLModel, table=True):\n    \"\"\"\n    ARCHANGEL v2: Canonical company entity.\n    \n    Stores enriched company data that can be reused across multiple LeadEvents.\n    When enrichment succeeds, we upsert to this table and link leads to it.\n    This builds our own mini-ZoomInfo over time.\n    \n    Matching strategy:\n    - Primary: normalized_name + geography\n    - Secondary: domain (unique identifier)\n    \"\"\"\n    id: Optional[int] = Field(default=None, primary_key=True)\n    \n    name: str = Field(index=True)\n    normalized_name: str = Field(index=True)  # Lowercase, stripped, for matching\n    domain: Optional[str] = Field(default=None, index=True, unique=True)  # Primary domain\n    \n    hq_city: Optional[str] = None\n    hq_state: Optional[str] = None\n    hq_country: str = Field(default=\"US\")\n    geography: Optional[str] = None  # Miami, Broward, Palm Beach, etc.\n    \n    phones: Optional[str] = None  # JSON: [{number, type, source_url, confidence}]\n    emails: Optional[str] = None  # JSON: [{email, type, source_url, confidence}]\n    \n    source_confidence: float = Field(default=0.0)  # Overall data quality score\n    source_signal_id: Optional[int] = Field(default=None, foreign_key=\"signal.id\")\n    source_type: Optional[str] = None  # news, craigslist, job_board, reddit, sec_filing\n    \n    tags: Optional[str] = None  # JSON: [\"HVAC\", \"Miami\", \"commercial\"]\n    niche: Optional[str] = None  # Industry/vertical\n    \n    enrichment_complete: bool = Field(default=False)\n    last_enriched_at: Optional[datetime] = None\n    enrichment_attempts: int = Field(default=0)\n    \n    first_seen_at: datetime = Field(default_factory=datetime.utcnow)\n    last_seen_at: datetime = Field(default_factory=datetime.utcnow)\n    created_at: datetime = Field(default_factory=datetime.utcnow)\n\n\n# ============================================================\n# EPIC 4: ENRICHMENT METRICS\n# ============================================================\n\nclass EnrichmentMetrics(SQLModel, table=True):\n    \"\"\"\n    ARCHANGEL v2: Global enrichment performance metrics.\n    \n    Tracks enrichment success rates per source to:\n    - Kill or downrank low-yield sources\n    - Tune scoring thresholds\n    - Decide where to invest engineering time\n    \"\"\"\n    id: Optional[int] = Field(default=None, primary_key=True)\n    \n    source_type: str = Field(index=True)  # news, craigslist, job_board, reddit, sec_filing\n    \n    total_leads: int = Field(default=0)\n    enriched_leads: int = Field(default=0)\n    enrichment_rate: float = Field(default=0.0)  # enriched_leads / total_leads * 100\n    \n    domains_discovered: int = Field(default=0)\n    emails_discovered: int = Field(default=0)\n    phones_discovered: int = Field(default=0)\n    \n    avg_attempts_per_lead: float = Field(default=0.0)\n    \n    unenrichable_no_domain: int = Field(default=0)\n    unenrichable_no_contact: int = Field(default=0)\n    unenrichable_no_osint: int = Field(default=0)\n    unenrichable_blocked: int = Field(default=0)\n    unenrichable_invalid_company: int = Field(default=0)\n    \n    outbound_sent: int = Field(default=0)\n    replies_received: int = Field(default=0)\n    reply_rate: float = Field(default=0.0)  # replies / outbound * 100\n    \n    period_start: datetime = Field(default_factory=datetime.utcnow)\n    period_end: Optional[datetime] = None\n    last_updated_at: datetime = Field(default_factory=datetime.utcnow)\n    created_at: datetime = Field(default_factory=datetime.utcnow)\n\n\n# ============================================================\n# EPIC 5: MACROSTORM / FORCECAST\n# ============================================================\n\nMACRO_FORCE_TYPE_EXPANSION = \"EXPANSION\"\nMACRO_FORCE_TYPE_CONTRACTION = \"CONTRACTION\"\nMACRO_FORCE_TYPE_RESTRUCTURING = \"RESTRUCTURING\"\nMACRO_FORCE_TYPE_MERGER = \"MERGER\"\nMACRO_FORCE_TYPE_BANKRUPTCY = \"BANKRUPTCY\"\nMACRO_FORCE_TYPE_SUPPLY_CHAIN = \"SUPPLY_CHAIN\"\nMACRO_FORCE_TYPE_REGULATORY = \"REGULATORY\"\n\nMACRO_SOURCE_SEC_10K = \"SEC_10K\"\nMACRO_SOURCE_SEC_10Q = \"SEC_10Q\"\nMACRO_SOURCE_SEC_8K = \"SEC_8K\"\nMACRO_SOURCE_SEC_S1 = \"SEC_S1\"\nMACRO_SOURCE_EARNINGS_CALL = \"EARNINGS_CALL\"\nMACRO_SOURCE_BANKRUPTCY = \"BANKRUPTCY\"\nMACRO_SOURCE_STATE_REGISTRY = \"STATE_REGISTRY\"\n\n\nclass MacroEvent(SQLModel, table=True):\n    \"\"\"\n    EPIC 5: MacroStorm / ForceCast strategic intelligence.\n    \n    Captures big-company moves from public filings that create downstream\n    opportunities for small businesses.\n    \n    Example: McDonald's 10-K shows 120 new units in Florida over 3 years\n    -> Creates opportunities for local HVAC, construction, staffing, etc.\n    \"\"\"\n    id: Optional[int] = Field(default=None, primary_key=True)\n    \n    macro_event_id: str = Field(index=True, unique=True)  # e.g., \"macro-2025-SEC-MCD-10K-ops-expansion\"\n    \n    source_type: str = Field(index=True)  # SEC_10K, SEC_8K, BANKRUPTCY, etc.\n    source_ref: Optional[str] = None  # CIK number, filing ID, etc.\n    source_url: Optional[str] = None\n    \n    company_name: str  # The big company (e.g., \"McDonald's Corporation\")\n    ticker: Optional[str] = Field(default=None, index=True)  # Stock ticker if public\n    \n    headline: str  # Human-readable summary: \"Plans 500 new units, 120 in Florida over 3 years\"\n    \n    geographies: Optional[str] = None  # JSON: [\"Florida\", \"Miami-Dade\", \"Broward\"]\n    segments_affected: Optional[str] = None  # JSON: [\"QSR\", \"logistics\", \"construction\"]\n    \n    force_type: str = Field(index=True)  # EXPANSION, CONTRACTION, MERGER, etc.\n    time_horizon: Optional[str] = None  # \"1-3_years\", \"0-12_months\", etc.\n    \n    risk_impact: Optional[str] = None  # JSON: {local_competitors: \"INCREASED_COMPETITION\", ...}\n    \n    raw_snippet: Optional[str] = None  # Actual text from filing/transcript\n    confidence: float = Field(default=0.0)  # Extraction confidence score\n    \n    smb_opportunity_segments: Optional[str] = None  # JSON: [{segment, geo, urgency, window}]\n    \n    leads_generated: int = Field(default=0)\n    leads_enriched: int = Field(default=0)\n    leads_contacted: int = Field(default=0)\n    leads_replied: int = Field(default=0)\n    \n    processed: bool = Field(default=False)\n    processed_at: Optional[datetime] = None\n    \n    created_at: datetime = Field(default_factory=datetime.utcnow)\n","path":null,"size_bytes":32139,"size_tokens":null},"replit.md":{"content":"# HossAgent - Autonomous AI Business Engine\n\n## Overview\nHossAgent is an autonomous AI business system with a noir aesthetic. Its core purpose is to autonomously identify leads, convert them into customers, execute tasks, and manage invoicing and profit tracking via Stripe. The system orchestrates four specialized AI agents (BizDev, Onboarding, Ops, Billing) and offers both customer-facing and administrative interfaces with robust authentication. The business model is a $99/month SaaS subscription, including a 7-day free trial. The project aims to provide a comprehensive, autonomous business solution, initially targeting the South Florida market with ambitions for broader application.\n\n## User Preferences\n- Pure web scraping only, NO paid APIs (Apollo.io explicitly forbidden)\n- Target 5-10% enrichment success rate improvement from ~2% baseline\n\n## System Architecture\nHossAgent utilizes a FastAPI backend and SQLModel for ORM, connecting to PostgreSQL in production and SQLite for development. The system emphasizes asynchronous, idempotent agent cycles.\n\n**UI/UX Design (2025 Professional Noir):**\n- **Fonts**: Inter (customer-facing) and JetBrains Mono (admin).\n- **Color System**: Deep black (`#0a0a0a`), elevated surfaces (`#111111`), subtle borders (`#1f1f1f`), white text (`#ffffff`), and accent green (`#22c55e`).\n- **Design Patterns**: Modern rounded corners, smooth transitions, hover effects, and backdrop blur. No gradients or emojis.\n\n**Interfaces:**\n- **Public**: Marketing Landing Page (`/`), About Page (`/about`), How It Works Page (`/how-it-works`).\n- **Admin Console (`/admin`)**: Consolidated dashboard for Signals, LeadEvents, Outbound Messages, and Customers.\n- **Lead Funnel Diagnostics (`/admin/diagnostics`)**: Real-time metrics dashboard tracking signal flow through enrichment pipeline. Shows conversion funnel (signals  events  enriched  sent), status breakdown, signal source breakdown, and per-lead enrichment attempt tracking. API endpoint: `GET /api/admin/diagnostics`.\n- **Customer Portal (`/portal`)**: Session-authenticated portal with sections for Account Status, Recent Opportunities & Outreach, Conversations (email threads), and Reports. Includes a token-based admin impersonation view (`/portal/<token>`).\n- **Customer Settings (`/portal/settings`)**: Business profile configuration, outreach preferences, and do-not-contact lists.\n\n**Authentication System:**\n- **Customer**: Email + bcrypt password, 14-day session-based HTTP-only cookies, password reset.\n- **Admin**: Password gate via `ADMIN_PASSWORD` environment variable.\n- **Trial Abuse Prevention**: Tracks email hash, IP, user-agent fingerprints, with 90-day cooldown.\n\n**Core Features & Technical Implementations:**\n- **Autonomous Agents**: SignalNet, BizDev, Onboarding, Ops, Billing cycles.\n- **SignalNet System**: 24/7 autonomous signal ingestion with pluggable `SignalSource` framework, scoring, and `LeadEvent` generation for signals scoring >= 65.\n- **Contextual Opportunity Engine**: Manages `AUTO` (immediate send) and `REVIEW` (customer approval) outreach modes, leveraging `BusinessProfile` for customer preferences and enforcing do-not-contact lists.\n- **Outbound Email System**: Uses `hossagent.net` (SendGrid authenticated), prioritizes person-like emails, rotates subject lines, supports `transparent_ai` or `classic` templates, and tailors messages based on signal type (`market_entry`, `competitor_intel`, `growth_opportunity`, `market_shift`). Includes 3 actionable recommendations per email, name parsing, rate limiting, and suppression flow.\n- **HossNative Lead Discovery**: Autonomous lead generation system integrating with SignalNet, performing web scraping for emails, domain resolution, and email validation without external APIs.\n\n**OPERATION ARCHANGEL v2 (Multi-Layered Enrichment Engine):**\n- **State Machine**: Tracks enrichment lifecycle (UNENRICHED  ENRICHING  ENRICHED/OUTBOUND_READY/ARCHIVED_UNENRICHABLE)\n- **Budget System**: max_enrichment_attempts (default 3) per lead before marking ARCHIVED_UNENRICHABLE\n- **Mission Log**: JSON array tracking [method, success, timestamp, details] per enrichment attempt\n- **Company Table**: Canonical entity storage with domain+name upserts, supports lead attachment for reuse\n- **NameStorm**: Multi-candidate company name extraction with branded validation (rejects generics like \"Texas HVAC company\", accepts \"Miami Best Roofing\")\n- **DomainStorm**: Multi-layered domain discovery (Google CSE, Bing, DuckDuckGo fallback)\n- **EmailStorm**: Layered email discovery with confidence scoring (person-like vs generic)\n- **PhoneStorm**: Extracts, normalizes, validates phone numbers from web pages\n- **EnrichmentMetrics**: Per-source yield tracking for optimization\n\n**Admin API Endpoints (ARCHANGEL v2):**\n- `GET /api/lead_events`: Includes enrichment_status, enrichment_attempts, unenrichable_reason, confidence scores\n- `GET /api/lead_events_detailed`: Full lead details with signal_source classification (news, sec, job_board, reddit, craigslist, synthetic, unknown)\n- `GET /api/enrichment/metrics`: Source-level enrichment yield, discovery counts, unenrichable breakdown\n- `GET /api/companies`: Canonical company entities with enrichment status\n- `POST /api/admin/lead_event/{id}/force-enrich`: Force immediate enrichment bypass (resets ARCHIVED_UNENRICHABLE leads)\n- `GET /api/admin/diagnostics`: Lead funnel metrics and conversion data\n\n**Craigslist Connector (EPIC 3.1):**\n- SMB-heavy signal source with niche detection (HVAC, plumbing, roofing, legal, etc.)\n- Job posting and service listing extraction for lead generation\n\n**Job Board Connector (EPIC 3.2):**\n- Indeed, ZipRecruiter, Glassdoor integration for SMB hiring signals\n- Detects HVAC, plumber, roofing hiring patterns in South Florida\n- 1-hour caching layer (CACHE_TTL = 3600) with backoff protection\n- Registered as `JobBoardSignalSource` in SignalNet pipeline\n\n**MacroStorm Strategic Intelligence (EPIC 3.3):**\n- SEC EDGAR Connector: Ingests 10-K, 10-Q, 8-K filings from SEC RSS feeds\n- NLP extraction for expansions, contractions, closures, and M&A events\n- ForceCast Mapping Engine: Maps MacroEvents to SMB target profiles\n- 4-hour cooldown in autopilot loop (`edgar_cooldown = 14400`)\n- Creates LeadEvents with macro_event_id linkage\n\n**Caching Layers:**\n- Job Board Connector: 1-hour cache for HTTP requests\n- Lead Enrichment: 1-hour cache for article body fetches\n- DuckDuckGo: Exponential backoff (5 min  10 min  20 min)\n\n- **Autopilot**: Automates agent cycles every 15 minutes for paid plans.\n- **Outbound Autopilot Toggle**: Separate toggle to control automated email sending. When OFF, BizDev cycles skip auto-send and leads remain in ENRICHED_NO_OUTBOUND status for manual approval. Customer Portal shows \"Awaiting Your Approval\" banner when pending approvals exist.\n- **Manual Outbound Editor**: When autopilot is OFF, operators can manually review and send emails via \"Review & Send\" button. Opens modal with editable subject/body fields, sends via SendGrid, updates enrichment_status to OUTBOUND_SENT. Available in both Admin Console and Customer Portal.\n- **No OSINT Presence Tab**: Dedicated Admin Console tab for leads with NO_OSINT_PRESENCE unenrichable_reason. These leads are filtered from the main Lead Events view via backend filtering (exclude_no_osint=true) for clean data segregation.\n- **Conversation Engine**: Handles inbound email replies, AI-assisted draft generation, guardrails for sensitive content, human-in-the-loop approval, and a suppression system. Uses a state machine (OPEN  HUMAN_OWNED  AUTO  CLOSED).\n- **Subscription Model**: Trial plan (7 days, restricted) and Paid plan ($99/month, full access). Manages signup, Stripe checkout, upgrade, and cancellation flows.\n- **Analytics & Telemetry**: Server-side analytics (`analytics.py`) tracks page views, funnel events, and abandonment (stored in `analytics_events.json` with IP hashing). An Admin Analytics Dashboard provides insights. Optional Google Analytics 4 integration.\n\n## Key Files (ARCHANGEL v2)\n- `models.py`: LeadEvent (enrichment fields), Company, EnrichmentMetrics tables\n- `mission_log.py`: Mission log tracking system\n- `lead_enrichment.py`: Main enrichment pipeline with state machine\n- `email_storm.py`: EmailStorm layered email discovery\n- `craigslist_connector.py`: Craigslist SMB signal source\n- `job_board_connector.py`: Indeed/ZipRecruiter/Glassdoor job board connector\n- `forcecast_engine.py`: MacroEvent to SMB target mapping\n- `sec_edgar_connector.py`: SEC EDGAR filings ingestion\n- `company_name_extraction.py`: NameStorm branded extraction\n- `domain_discovery.py`: DomainStorm multi-layer discovery\n- `phone_extraction.py`: PhoneStorm extraction and validation\n\n## External Dependencies\n- **FastAPI**: Backend web framework.\n- **SQLModel**: ORM.\n- **PostgreSQL**: Production database.\n- **bcrypt**: Password hashing.\n- **SendGrid**: Primary email service provider.\n- **Amazon SES**: Alternative email service provider.\n- **Stripe**: Payment gateway for subscriptions.\n- **OpenWeatherMap**: Used by SignalNet for weather data.\n","path":null,"size_bytes":9120,"size_tokens":null},"release_mode.py":{"content":"\"\"\"\nRelease Mode Configuration for HossAgent.\n\nProvides environment-driven safety rails for production deployment.\n\nEnvironment Variables:\n    RELEASE_MODE = PRODUCTION | SANDBOX (default: SANDBOX)\n    \n    Aliases:\n    - SANDBOX, DEVELOPMENT, FALSE -> SANDBOX mode\n    - PRODUCTION, TRUE -> PRODUCTION mode\n    \nPRODUCTION mode:\n    - Startup banner: [RELEASE_MODE][PRODUCTION]\n    - Uses HossNative for lead generation (autonomous discovery - no external APIs)\n    - Sends real emails via SendGrid (EMAIL_MODE=SENDGRID)\n    - Strict validation of all credentials\n\nSANDBOX mode (default):\n    - Startup banner: [RELEASE_MODE][SANDBOX]\n    - Safe for testing - must explicitly opt into production\n    - Lenient configuration (DRY_RUN acceptable)\n\nEmail Configuration (SendGrid with authenticated domain):\n    EMAIL_MODE = SENDGRID | DRY_RUN\n    SENDGRID_API_KEY       - SendGrid API key\n    OUTBOUND_FROM          - Sending email (e.g., hello@hossagent.net)\n    OUTBOUND_REPLY_TO      - Reply-to address\n    OUTBOUND_DISPLAY_NAME  - Display name (e.g., HossAgent)\n\nTo change modes, update env vars in Replit Secrets:\n    RELEASE_MODE=PRODUCTION  # Enable real lead sources + full pipeline\n    EMAIL_MODE=SENDGRID      # Enable real email sending via SendGrid\n\"\"\"\nimport os\nfrom enum import Enum\nfrom typing import Dict, Any, List, Tuple\nfrom datetime import datetime, timedelta\n\n\nclass ReleaseMode(Enum):\n    \"\"\"Release mode enum for HossAgent deployment modes.\"\"\"\n    PRODUCTION = \"PRODUCTION\"\n    SANDBOX = \"SANDBOX\"\n    # Legacy aliases - map to SANDBOX\n    STAGING = \"SANDBOX\"\n    DEVELOPMENT = \"SANDBOX\"\n\n\ndef get_release_mode() -> ReleaseMode:\n    \"\"\"\n    Get current release mode from environment.\n    \n    Checks RELEASE_MODE env var:\n    - PRODUCTION or TRUE -> ReleaseMode.PRODUCTION\n    - SANDBOX, DEVELOPMENT, or anything else -> ReleaseMode.SANDBOX (safe default)\n    \n    SANDBOX is the default to ensure safe behavior - must explicitly opt into PRODUCTION.\n    \"\"\"\n    mode_str = os.getenv(\"RELEASE_MODE\", \"SANDBOX\").upper().strip()\n    \n    if mode_str in (\"PRODUCTION\", \"TRUE\"):\n        return ReleaseMode.PRODUCTION\n    else:\n        # Default to SANDBOX for safety\n        return ReleaseMode.SANDBOX\n\n\ndef is_release_mode() -> bool:\n    \"\"\"\n    Check if system is running in Production mode.\n    \n    Returns True only for PRODUCTION mode.\n    For backward compatibility with existing code.\n    \"\"\"\n    return get_release_mode() == ReleaseMode.PRODUCTION\n\n\ndef is_production() -> bool:\n    \"\"\"Explicit check for PRODUCTION mode.\"\"\"\n    return get_release_mode() == ReleaseMode.PRODUCTION\n\n\ndef is_sandbox() -> bool:\n    \"\"\"Explicit check for SANDBOX mode.\"\"\"\n    return get_release_mode() == ReleaseMode.SANDBOX\n\n\n# Legacy aliases for backward compatibility\ndef is_staging() -> bool:\n    \"\"\"Legacy alias - maps to SANDBOX.\"\"\"\n    return is_sandbox()\n\n\ndef is_development() -> bool:\n    \"\"\"Legacy alias - maps to SANDBOX.\"\"\"\n    return is_sandbox()\n\n\ndef get_release_mode_status() -> Dict[str, Any]:\n    \"\"\"\n    Get current release mode configuration status for admin display.\n    \n    Returns comprehensive status including mode, email config, and any warnings/errors.\n    \"\"\"\n    mode = get_release_mode()\n    email_mode = os.getenv(\"EMAIL_MODE\", \"DRY_RUN\").upper()\n    enable_stripe = os.getenv(\"ENABLE_STRIPE\", \"FALSE\").upper() == \"TRUE\"\n    max_emails_hour = int(os.getenv(\"MAX_EMAILS_PER_HOUR\", \"50\"))\n    lead_api_configured = bool(os.getenv(\"LEAD_SEARCH_API_KEY\"))\n    \n    # SendGrid credential check (primary email provider)\n    sendgrid_api_key = os.getenv(\"SENDGRID_API_KEY\")\n    outbound_from = os.getenv(\"OUTBOUND_FROM\")\n    outbound_reply_to = os.getenv(\"OUTBOUND_REPLY_TO\")\n    outbound_display_name = os.getenv(\"OUTBOUND_DISPLAY_NAME\", \"HossAgent\")\n    \n    sendgrid_configured = bool(sendgrid_api_key and outbound_from and outbound_reply_to)\n    \n    # Extract sending domain\n    sending_domain = \"\"\n    if outbound_from and \"@\" in outbound_from:\n        sending_domain = outbound_from.split(\"@\")[1]\n    \n    warnings = []\n    errors = []\n    \n    if mode == ReleaseMode.PRODUCTION:\n        if email_mode == \"DRY_RUN\":\n            warnings.append(\"PRODUCTION mode but EMAIL_MODE=DRY_RUN - no real emails will be sent\")\n        \n        if max_emails_hour > 100:\n            warnings.append(f\"MAX_EMAILS_PER_HOUR={max_emails_hour} is high for production\")\n        \n        if not enable_stripe:\n            warnings.append(\"PRODUCTION mode but ENABLE_STRIPE=FALSE - no payment links\")\n        \n        if email_mode == \"SENDGRID\":\n            missing = []\n            if not sendgrid_api_key:\n                missing.append(\"SENDGRID_API_KEY\")\n            if not outbound_from:\n                missing.append(\"OUTBOUND_FROM\")\n            if not outbound_reply_to:\n                missing.append(\"OUTBOUND_REPLY_TO\")\n            if missing:\n                errors.append(f\"SENDGRID mode requires: {', '.join(missing)} - will fallback to DRY_RUN\")\n        elif email_mode == \"SMTP\":\n            errors.append(\"SMTP mode is deprecated - use SENDGRID with hossagent.net domain\")\n    \n    elif mode == ReleaseMode.SANDBOX:\n        if email_mode not in [\"DRY_RUN\", \"SENDGRID\"]:\n            warnings.append(f\"Unrecognized or deprecated EMAIL_MODE: {email_mode}\")\n    \n    # Determine effective email mode (accounting for fallbacks)\n    effective_email_mode = email_mode\n    if email_mode == \"SMTP\":\n        effective_email_mode = \"DRY_RUN (SMTP deprecated)\"\n    elif email_mode == \"SENDGRID\" and not sendgrid_configured:\n        effective_email_mode = \"DRY_RUN (missing config)\"\n    \n    return {\n        \"release_mode\": mode.value,\n        \"is_production\": mode == ReleaseMode.PRODUCTION,\n        \"is_sandbox\": mode == ReleaseMode.SANDBOX,\n        \"email_mode\": email_mode,\n        \"effective_email_mode\": effective_email_mode,\n        \"sendgrid_configured\": sendgrid_configured,\n        \"sending_domain\": sending_domain,\n        \"outbound_from\": outbound_from,\n        \"outbound_display_name\": outbound_display_name,\n        \"stripe_enabled\": enable_stripe,\n        \"max_emails_per_hour\": max_emails_hour,\n        \"lead_api_configured\": lead_api_configured,\n        \"warnings\": warnings,\n        \"errors\": errors\n    }\n\n\ndef print_startup_banners() -> None:\n    \"\"\"Print startup status banners for all subsystems based on release mode.\"\"\"\n    mode = get_release_mode()\n    \n    if mode == ReleaseMode.PRODUCTION:\n        print(\"=\" * 60)\n        print(\"[RELEASE_MODE][PRODUCTION] HossAgent running in PRODUCTION mode\")\n        print(\"=\" * 60)\n    else:\n        print(\"[RELEASE_MODE][SANDBOX] HossAgent running in SANDBOX mode\")\n    \n    email_mode = os.getenv(\"EMAIL_MODE\", \"DRY_RUN\").upper()\n    \n    # Check SendGrid credentials (primary email provider)\n    sendgrid_api_key = os.getenv(\"SENDGRID_API_KEY\")\n    outbound_from = os.getenv(\"OUTBOUND_FROM\")\n    outbound_reply_to = os.getenv(\"OUTBOUND_REPLY_TO\")\n    outbound_display_name = os.getenv(\"OUTBOUND_DISPLAY_NAME\", \"HossAgent\")\n    \n    # Extract sending domain\n    sending_domain = \"\"\n    if outbound_from and \"@\" in outbound_from:\n        sending_domain = outbound_from.split(\"@\")[1]\n    \n    max_emails_hour = int(os.getenv(\"MAX_EMAILS_PER_HOUR\", \"50\"))\n    if mode == ReleaseMode.PRODUCTION and max_emails_hour > 100:\n        print(f\"[PRODUCTION][HIGH_VOLUME_WARNING] MAX_EMAILS_PER_HOUR={max_emails_hour} - consider lower values for warm-up\")\n    \n    if email_mode == \"SENDGRID\":\n        missing = []\n        if not sendgrid_api_key:\n            missing.append(\"SENDGRID_API_KEY\")\n        if not outbound_from:\n            missing.append(\"OUTBOUND_FROM\")\n        if not outbound_reply_to:\n            missing.append(\"OUTBOUND_REPLY_TO\")\n        \n        if missing:\n            print(f\"[EMAIL][STARTUP] Mode: SENDGRID\")\n            print(f\"[EMAIL][ERROR] Missing required env vars: {', '.join(missing)}\")\n            print(\"[EMAIL][STARTUP] Falling back to DRY_RUN mode\")\n        else:\n            print(f\"[EMAIL][STARTUP] Mode: SENDGRID\")\n            print(f\"[EMAIL][STARTUP] Domain: {sending_domain} (authenticated)\")\n            print(f\"[EMAIL][STARTUP] From: {outbound_display_name} <{outbound_from}>\")\n            print(f\"[EMAIL][STARTUP] Reply-To: {outbound_reply_to}\")\n            print(\"[EMAIL][STARTUP] SendGrid configured and ready\")\n    elif email_mode == \"SMTP\":\n        print(f\"[EMAIL][STARTUP] Mode: SMTP\")\n        print(\"[EMAIL][WARNING] SMTP mode is deprecated. Use SENDGRID mode with hossagent.net domain.\")\n        print(\"[EMAIL][STARTUP] Falling back to DRY_RUN mode\")\n    else:\n        print(f\"[EMAIL][STARTUP] Mode: DRY_RUN (no emails will be sent)\")\n    \n    print(\"[LEADS][STARTUP] HossNative (Autonomous Discovery) active\")\n    print(\"[LEADS][STARTUP] Lead discovery via SignalNet + web scraping - no external APIs\")\n\n\ndef get_throttle_defaults() -> Dict[str, int]:\n    \"\"\"\n    Get default throttle values based on release mode.\n    \n    Returns sensible defaults that can be overridden by env vars.\n    SANDBOX and PRODUCTION both use the same defaults for simplicity.\n    \"\"\"\n    mode = get_release_mode()\n    \n    if mode == ReleaseMode.PRODUCTION:\n        return {\n            \"max_emails_per_cycle\": 10,\n            \"max_emails_per_hour\": 50,\n            \"max_new_leads_per_cycle\": 10\n        }\n    else:\n        return {\n            \"max_emails_per_cycle\": 10,\n            \"max_emails_per_hour\": 50,\n            \"max_new_leads_per_cycle\": 10\n        }\n\n\ndef generate_daily_summary(\n    leads_data: Dict[str, Any],\n    email_data: Dict[str, Any],\n    invoice_data: Dict[str, Any],\n    payment_data: Dict[str, Any],\n    hours: int = 24\n) -> Dict[str, Any]:\n    \"\"\"\n    Generate a summary of system activity for the last N hours.\n    \n    Args:\n        leads_data: Lead statistics from lead_service\n        email_data: Email statistics from email_utils\n        invoice_data: Invoice statistics from database\n        payment_data: Payment statistics from stripe_utils\n        hours: Hours to look back (default 24)\n    \n    Returns:\n        Dict with summary statistics\n    \"\"\"\n    cutoff = datetime.utcnow() - timedelta(hours=hours)\n    mode = get_release_mode()\n    \n    return {\n        \"period\": {\n            \"hours\": hours,\n            \"start\": cutoff.isoformat(),\n            \"end\": datetime.utcnow().isoformat()\n        },\n        \"leads\": leads_data,\n        \"emails\": email_data,\n        \"invoices\": invoice_data,\n        \"payments\": payment_data,\n        \"generated_at\": datetime.utcnow().isoformat(),\n        \"release_mode\": mode.value\n    }\n","path":null,"size_bytes":10574,"size_tokens":null},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"openai>=2.8.1\",\n    \"requests>=2.32.5\",\n]\n","path":null,"size_bytes":189,"size_tokens":null},"database.py":{"content":"from sqlmodel import SQLModel, create_engine, Session, select\nimport os\n\nDATABASE_URL = os.environ.get(\"DATABASE_URL\")\nIS_POSTGRES = DATABASE_URL is not None and \"postgresql\" in DATABASE_URL\n\nif DATABASE_URL:\n    engine = create_engine(\n        DATABASE_URL,\n        echo=False,\n        pool_pre_ping=True,\n        pool_recycle=300,\n        pool_size=5,\n        max_overflow=10,\n    )\n    print(f\"[DATABASE] Using PostgreSQL (pool_pre_ping=True, pool_recycle=300s)\")\nelse:\n    DATABASE_URL = \"sqlite:///./hossagent.db\"\n    engine = create_engine(DATABASE_URL, connect_args={\"check_same_thread\": False})\n    print(f\"[DATABASE] Using SQLite (development only)\")\n\n\ndef _run_sqlite_migrations():\n    \"\"\"\n    Run schema migrations for existing SQLite databases.\n    This ensures new columns are added without losing data.\n    Only runs for SQLite - PostgreSQL uses fresh schema from SQLModel.\n    \"\"\"\n    if IS_POSTGRES:\n        return\n    \n    import sqlite3\n    import secrets\n    \n    conn = sqlite3.connect('./hossagent.db')\n    cursor = conn.cursor()\n    \n    cursor.execute(\"PRAGMA table_info(lead)\")\n    lead_columns = {row[1] for row in cursor.fetchall()}\n    \n    if 'website' not in lead_columns:\n        try:\n            cursor.execute('ALTER TABLE lead ADD COLUMN website TEXT')\n            print(\"[MIGRATION] Added 'website' column to lead table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'source' not in lead_columns:\n        try:\n            cursor.execute('ALTER TABLE lead ADD COLUMN source TEXT')\n            print(\"[MIGRATION] Added 'source' column to lead table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    cursor.execute(\"PRAGMA table_info(customer)\")\n    customer_columns = {row[1] for row in cursor.fetchall()}\n    \n    if 'public_token' not in customer_columns:\n        try:\n            cursor.execute('ALTER TABLE customer ADD COLUMN public_token TEXT')\n            print(\"[MIGRATION] Added 'public_token' column to customer table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'trial_start_at' not in customer_columns:\n        try:\n            cursor.execute('ALTER TABLE customer ADD COLUMN trial_start_at TEXT')\n            print(\"[MIGRATION] Added 'trial_start_at' column to customer table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'trial_end_at' not in customer_columns:\n        try:\n            cursor.execute('ALTER TABLE customer ADD COLUMN trial_end_at TEXT')\n            print(\"[MIGRATION] Added 'trial_end_at' column to customer table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'subscription_status' not in customer_columns:\n        try:\n            cursor.execute(\"ALTER TABLE customer ADD COLUMN subscription_status TEXT DEFAULT 'none'\")\n            print(\"[MIGRATION] Added 'subscription_status' column to customer table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'stripe_subscription_id' not in customer_columns:\n        try:\n            cursor.execute('ALTER TABLE customer ADD COLUMN stripe_subscription_id TEXT')\n            print(\"[MIGRATION] Added 'stripe_subscription_id' column to customer table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'tasks_this_period' not in customer_columns:\n        try:\n            cursor.execute('ALTER TABLE customer ADD COLUMN tasks_this_period INTEGER DEFAULT 0')\n            print(\"[MIGRATION] Added 'tasks_this_period' column to customer table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'leads_this_period' not in customer_columns:\n        try:\n            cursor.execute('ALTER TABLE customer ADD COLUMN leads_this_period INTEGER DEFAULT 0')\n            print(\"[MIGRATION] Added 'leads_this_period' column to customer table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'billing_method' not in customer_columns:\n        try:\n            cursor.execute('ALTER TABLE customer ADD COLUMN billing_method TEXT')\n            print(\"[MIGRATION] Added 'billing_method' column to customer table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'cancelled_at_period_end' not in customer_columns:\n        try:\n            cursor.execute('ALTER TABLE customer ADD COLUMN cancelled_at_period_end INTEGER DEFAULT 0')\n            print(\"[MIGRATION] Added 'cancelled_at_period_end' column to customer table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'cancellation_effective_at' not in customer_columns:\n        try:\n            cursor.execute('ALTER TABLE customer ADD COLUMN cancellation_effective_at TEXT')\n            print(\"[MIGRATION] Added 'cancellation_effective_at' column to customer table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'outreach_mode' not in customer_columns:\n        try:\n            cursor.execute(\"ALTER TABLE customer ADD COLUMN outreach_mode TEXT DEFAULT 'AUTO'\")\n            print(\"[MIGRATION] Added 'outreach_mode' column to customer table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'last_contact_summary' not in lead_columns:\n        try:\n            cursor.execute('ALTER TABLE lead ADD COLUMN last_contact_summary TEXT')\n            print(\"[MIGRATION] Added 'last_contact_summary' column to lead table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'next_step' not in lead_columns:\n        try:\n            cursor.execute('ALTER TABLE lead ADD COLUMN next_step TEXT')\n            print(\"[MIGRATION] Added 'next_step' column to lead table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'next_step_owner' not in lead_columns:\n        try:\n            cursor.execute('ALTER TABLE lead ADD COLUMN next_step_owner TEXT')\n            print(\"[MIGRATION] Added 'next_step_owner' column to lead table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    cursor.execute(\"PRAGMA table_info(leadevent)\")\n    leadevent_columns = {row[1] for row in cursor.fetchall()}\n    \n    if 'last_contact_at' not in leadevent_columns:\n        try:\n            cursor.execute('ALTER TABLE leadevent ADD COLUMN last_contact_at TEXT')\n            print(\"[MIGRATION] Added 'last_contact_at' column to leadevent table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'last_contact_summary' not in leadevent_columns:\n        try:\n            cursor.execute('ALTER TABLE leadevent ADD COLUMN last_contact_summary TEXT')\n            print(\"[MIGRATION] Added 'last_contact_summary' column to leadevent table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'next_step' not in leadevent_columns:\n        try:\n            cursor.execute('ALTER TABLE leadevent ADD COLUMN next_step TEXT')\n            print(\"[MIGRATION] Added 'next_step' column to leadevent table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'next_step_owner' not in leadevent_columns:\n        try:\n            cursor.execute('ALTER TABLE leadevent ADD COLUMN next_step_owner TEXT')\n            print(\"[MIGRATION] Added 'next_step_owner' column to leadevent table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'enrichment_attempts' not in leadevent_columns:\n        try:\n            cursor.execute('ALTER TABLE leadevent ADD COLUMN enrichment_attempts INTEGER DEFAULT 0')\n            print(\"[MIGRATION] Added 'enrichment_attempts' column to leadevent table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'last_enrichment_at' not in leadevent_columns:\n        try:\n            cursor.execute('ALTER TABLE leadevent ADD COLUMN last_enrichment_at TEXT')\n            print(\"[MIGRATION] Added 'last_enrichment_at' column to leadevent table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'social_facebook' not in leadevent_columns:\n        try:\n            cursor.execute('ALTER TABLE leadevent ADD COLUMN social_facebook TEXT')\n            print(\"[MIGRATION] Added 'social_facebook' column to leadevent table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'social_instagram' not in leadevent_columns:\n        try:\n            cursor.execute('ALTER TABLE leadevent ADD COLUMN social_instagram TEXT')\n            print(\"[MIGRATION] Added 'social_instagram' column to leadevent table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'social_linkedin' not in leadevent_columns:\n        try:\n            cursor.execute('ALTER TABLE leadevent ADD COLUMN social_linkedin TEXT')\n            print(\"[MIGRATION] Added 'social_linkedin' column to leadevent table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'social_twitter' not in leadevent_columns:\n        try:\n            cursor.execute('ALTER TABLE leadevent ADD COLUMN social_twitter TEXT')\n            print(\"[MIGRATION] Added 'social_twitter' column to leadevent table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    cursor.execute(\"PRAGMA table_info(invoice)\")\n    invoice_columns = {row[1] for row in cursor.fetchall()}\n    \n    if 'payment_url' not in invoice_columns:\n        try:\n            cursor.execute('ALTER TABLE invoice ADD COLUMN payment_url TEXT')\n            print(\"[MIGRATION] Added 'payment_url' column to invoice table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    if 'stripe_payment_id' not in invoice_columns:\n        try:\n            cursor.execute('ALTER TABLE invoice ADD COLUMN stripe_payment_id TEXT')\n            print(\"[MIGRATION] Added 'stripe_payment_id' column to invoice table\")\n        except sqlite3.OperationalError:\n            pass\n    \n    conn.commit()\n    \n    cursor.execute(\"SELECT id FROM customer WHERE public_token IS NULL\")\n    customers_without_token = cursor.fetchall()\n    for (customer_id,) in customers_without_token:\n        import secrets as sec\n        token = sec.token_urlsafe(16)\n        cursor.execute(\"UPDATE customer SET public_token = ? WHERE id = ?\", (token, customer_id))\n        print(f\"[MIGRATION] Generated public_token for customer {customer_id}\")\n    \n    cursor.execute(\"SELECT id, plan FROM customer WHERE plan = 'starter' OR plan IS NULL\")\n    legacy_customers = cursor.fetchall()\n    for (customer_id, plan) in legacy_customers:\n        cursor.execute(\"UPDATE customer SET plan = 'paid', subscription_status = 'active' WHERE id = ?\", (customer_id,))\n        print(f\"[MIGRATION] Upgraded legacy customer {customer_id} to paid plan (grandfathered)\")\n    \n    conn.commit()\n    conn.close()\n\n\ndef create_db_and_tables():\n    \"\"\"Create database tables if they don't exist and initialize SystemSettings.\"\"\"\n    SQLModel.metadata.create_all(engine)\n    \n    _run_sqlite_migrations()\n    \n    from models import SystemSettings\n    with Session(engine) as session:\n        existing = session.exec(select(SystemSettings).where(SystemSettings.id == 1)).first()\n        if not existing:\n            settings = SystemSettings(id=1, autopilot_enabled=True)\n            session.add(settings)\n            session.commit()\n            print(\"[STARTUP] SystemSettings initialized: autopilot_enabled=True\")\n\n\ndef get_session():\n    \"\"\"Get a database session.\"\"\"\n    with Session(engine) as session:\n        yield session\n","path":null,"size_bytes":11334,"size_tokens":null},"agents.py":{"content":"\"\"\"\nAutonomous agents for HossAgent business engine.\nEach agent runs as a cycle function that is idempotent and safe to call repeatedly.\n\nThe *_cycle functions are called both by:\n- Manual admin buttons (/admin/run-* endpoints)\n- The autopilot background loop (runs every 5 minutes if enabled)\n\nAll functions accept a Session and perform idempotent operations.\n\nPlan Gating:\n- Trial customers: Limited tasks/leads, DRY_RUN email, no billing\n- Paid customers: Full access to all features\n\"\"\"\nimport asyncio\nimport hashlib\nimport secrets\nfrom datetime import datetime\nfrom sqlmodel import Session, select\nfrom models import (\n    Lead, Customer, Task, Invoice, LeadEvent, \n    BusinessProfile, PendingOutbound, Report,\n    TRIAL_TASK_LIMIT, TRIAL_LEAD_LIMIT,\n    OUTREACH_MODE_AUTO, OUTREACH_MODE_REVIEW,\n    LEAD_STATUS_NEW, LEAD_STATUS_CONTACTED,\n    NEXT_STEP_OWNER_AGENT, NEXT_STEP_OWNER_CUSTOMER,\n    ENRICHMENT_STATUS_UNENRICHED,\n    ENRICHMENT_STATUS_WITH_DOMAIN_NO_EMAIL,\n    ENRICHMENT_STATUS_ENRICHED_NO_OUTBOUND,\n    ENRICHMENT_STATUS_OUTBOUND_SENT,\n    ENRICHMENT_STATUS_ARCHIVED,\n    ENRICHMENT_STATUS_ENRICHED,\n    ENRICHMENT_STATUS_OUTBOUND_READY,\n)\nfrom email_utils import (\n    send_email,\n    get_email_mode,\n    get_max_emails_per_cycle,\n    get_email_status,\n    EmailMode,\n    EmailResult\n)\nfrom outbound_utils import (\n    get_business_profile,\n    check_do_not_contact,\n    create_pending_outbound,\n    send_lead_event_immediate\n)\nfrom bizdev_templates import (\n    generate_email,\n    log_template_generation,\n    get_template_status\n)\nfrom subscription_utils import (\n    get_customer_plan_status,\n    initialize_trial,\n    should_force_email_dry_run,\n    should_disable_billing_for_customer,\n    increment_task_usage,\n    increment_lead_usage,\n    increment_tasks_used,\n    increment_leads_used\n)\nimport random\n\n\ndef create_report(\n    session: Session,\n    customer_id: int,\n    title: str,\n    description: str = None,\n    content: str = None,\n    report_type: str = \"general\",\n    lead_id: int = None\n) -> Report:\n    \"\"\"\n    Create a Report record for a customer.\n    \n    Reports capture visible work output such as research summaries,\n    competitive analyses, and market insights. Displayed in the portal\n    under \"Reports / Recent Work\".\n    \n    Args:\n        session: Database session\n        customer_id: The customer this report is for\n        title: Report title (e.g., task description)\n        description: Short description or summary\n        content: Full report content or JSON\n        report_type: Type of report (research, competitive, market, opportunity, general)\n        lead_id: Optional related lead ID\n    \n    Returns:\n        The created Report record\n    \"\"\"\n    report = Report(\n        customer_id=customer_id,\n        lead_id=lead_id,\n        title=title,\n        description=description,\n        content=content,\n        report_type=report_type\n    )\n    session.add(report)\n    \n    increment_tasks_used(session, customer_id)\n    \n    print(f\"[REPORT] Created report for customer {customer_id}: {title[:50]}...\")\n    return report\n\n\nasync def run_bizdev_cycle(session: Session) -> str:\n    \"\"\"\n    BizDev Cycle: Send outbound emails to NEW leads using template engine.\n    \n    Steps:\n    1. Find leads with status='new' that haven't been contacted\n    2. For each NEW lead, check customer outreach_mode if lead has company_id\n    3. Check do_not_contact list before sending\n    4. If AUTO mode: send email, update lead status to CONTACTED\n    5. If REVIEW mode: create PendingOutbound, keep lead as NEW\n    6. If email fails, mark as 'email_failed'\n    7. If dry-run, keep as 'new'\n    \n    Throttling: Respects MAX_EMAILS_PER_CYCLE and MAX_EMAILS_PER_HOUR.\n    Safe to call repeatedly - only emails leads with status='new'.\n    \n    Note: Trial customers are forced to DRY_RUN email mode.\n    \n    Outbound Autopilot: When outbound_autopilot_enabled=False, this cycle is skipped.\n    \"\"\"\n    settings = session.exec(\n        select(SystemSettings).where(SystemSettings.id == 1)\n    ).first()\n    \n    outbound_enabled = getattr(settings, 'outbound_autopilot_enabled', True) if settings else True\n    \n    if not outbound_enabled:\n        new_leads_count = len(session.exec(\n            select(Lead).where(Lead.status == \"new\")\n        ).all())\n        if new_leads_count > 0:\n            msg = f\"BizDev: Outbound autopilot OFF. {new_leads_count} leads awaiting manual approval.\"\n            print(f\"[CYCLE] {msg}\")\n            return msg\n        msg = \"BizDev: No new leads to contact.\"\n        print(f\"[CYCLE] {msg}\")\n        return msg\n    \n    max_emails = get_max_emails_per_cycle()\n    email_status = get_email_status()\n    effective_mode = email_status[\"mode\"]\n    template_status = get_template_status()\n    \n    new_leads = session.exec(\n        select(Lead).where(Lead.status == \"new\").limit(max_emails)\n    ).all()\n    \n    if not new_leads:\n        msg = \"BizDev: No new leads to contact.\"\n        print(f\"[CYCLE] {msg}\")\n        return msg\n    \n    emails_sent = 0\n    emails_failed = 0\n    emails_throttled = 0\n    emails_attempted = 0\n    emails_queued = 0\n    emails_blocked = 0\n    contacted_companies = []\n\n    for lead in new_leads:\n        if emails_attempted >= max_emails:\n            print(f\"[BIZDEV] Throttle limit reached ({max_emails} emails per cycle)\")\n            break\n        \n        customer = None\n        business_profile = None\n        outreach_mode = OUTREACH_MODE_AUTO\n        do_not_contact_list = None\n        \n        customer_id = getattr(lead, 'customer_id', None) or getattr(lead, 'company_id', None)\n        if customer_id:\n            customer = session.exec(\n                select(Customer).where(Customer.id == customer_id)\n            ).first()\n            if customer:\n                outreach_mode = customer.outreach_mode or OUTREACH_MODE_AUTO\n                business_profile = get_business_profile(session, customer.id)\n                if business_profile:\n                    do_not_contact_list = business_profile.do_not_contact_list\n        \n        if check_do_not_contact(lead.email, do_not_contact_list):\n            emails_blocked += 1\n            print(f\"[BIZDEV] Lead {lead.name} at {lead.company}: BLOCKED (do_not_contact)\")\n            continue\n        \n        generated = generate_email(\n            first_name=lead.name,\n            company_name=lead.company,\n            niche=lead.niche,\n            email=lead.email\n        )\n        \n        log_template_generation(generated, lead.id, lead.email)\n        emails_attempted += 1\n\n        if outreach_mode == OUTREACH_MODE_REVIEW and customer:\n            create_pending_outbound(\n                session=session,\n                customer_id=customer.id,\n                lead_id=lead.id,\n                to_email=lead.email,\n                to_name=lead.name,\n                subject=generated.subject,\n                body=generated.body,\n                context_summary=f\"Intro email for lead from {lead.company}\"\n            )\n            lead.next_step = \"Awaiting your review\"\n            lead.next_step_owner = NEXT_STEP_OWNER_CUSTOMER\n            emails_queued += 1\n            session.add(lead)\n            print(f\"[BIZDEV] Lead {lead.name} at {lead.company}: QUEUED for review (template={generated.template_pack})\")\n            continue\n\n        email_result: EmailResult = send_email(\n            to_email=lead.email,\n            subject=generated.subject,\n            body=generated.body,\n            lead_name=lead.name,\n            company=lead.company\n        )\n        \n        if email_result.actually_sent:\n            lead.status = LEAD_STATUS_CONTACTED\n            lead.last_contacted_at = datetime.utcnow()\n            lead.last_contact_summary = \"Intro email sent\"\n            lead.next_step_owner = NEXT_STEP_OWNER_AGENT\n            emails_sent += 1\n            contacted_companies.append(lead.company)\n            print(f\"[BIZDEV] Lead {lead.name} at {lead.company}: CONTACTED (template={generated.template_pack})\")\n        elif email_result.result == \"throttled\":\n            emails_throttled += 1\n            print(f\"[BIZDEV] Lead {lead.name} at {lead.company}: THROTTLED\")\n        elif email_result.result in (\"dry_run\", \"fallback\"):\n            print(f\"[BIZDEV] Lead {lead.name} at {lead.company}: status=new (mode={email_result.mode})\")\n        else:\n            lead.status = \"email_failed\"\n            emails_failed += 1\n            print(f\"[BIZDEV] Lead {lead.name} at {lead.company}: EMAIL_FAILED error=\\\"{email_result.error}\\\"\")\n\n        session.add(lead)\n\n    session.commit()\n    \n    companies_str = \", \".join(contacted_companies) if contacted_companies else \"None\"\n    throttle_info = f\", Throttled: {emails_throttled}\" if emails_throttled > 0 else \"\"\n    queued_info = f\", Queued: {emails_queued}\" if emails_queued > 0 else \"\"\n    blocked_info = f\", Blocked: {emails_blocked}\" if emails_blocked > 0 else \"\"\n    msg = f\"BizDev: Contacted {emails_sent}/{emails_attempted} leads ({companies_str}). Failed: {emails_failed}{throttle_info}{queued_info}{blocked_info}. Mode: {effective_mode}, Template: {template_status['active_pack']}\"\n    print(f\"[CYCLE] {msg}\")\n    return msg\n\n\nasync def run_onboarding_cycle(session: Session) -> str:\n    \"\"\"\n    Onboarding Cycle: Convert a contacted/responded lead into a customer.\n    Create 1-2 template tasks for the customer.\n    Mark lead as qualified.\n    \n    Priority: 'responded' leads first, then 'contacted' leads.\n    Idempotent: Skips leads already converted.\n    \n    New customers start in TRIAL mode with 7-day restricted access.\n    \"\"\"\n    lead = session.exec(\n        select(Lead).where(Lead.status == \"responded\").limit(1)\n    ).first()\n    \n    if not lead:\n        lead = session.exec(\n            select(Lead).where(Lead.status == \"contacted\").limit(1)\n        ).first()\n\n    if not lead:\n        msg = \"Onboarding: No unqualified leads available.\"\n        print(f\"[CYCLE] {msg}\")\n        return msg\n\n    existing_customer = session.exec(\n        select(Customer).where(Customer.contact_email == lead.email)\n    ).first()\n    if existing_customer:\n        msg = f\"Onboarding: Lead {lead.company} already converted to customer {existing_customer.id}.\"\n        print(f\"[CYCLE] {msg}\")\n        return msg\n\n    customer = Customer(\n        company=lead.company,\n        contact_email=lead.email,\n        billing_plan=\"starter\",\n        status=\"active\",\n        public_token=secrets.token_urlsafe(16),\n        notes=f\"Converted from lead: {lead.company}\",\n    )\n    \n    customer = initialize_trial(customer)\n    customer.leads_this_period = 1\n    \n    session.add(customer)\n    session.flush()\n    \n    plan_status = get_customer_plan_status(customer)\n\n    task_descriptions = [\n        f\"Initial market research for {lead.company}\",\n        f\"Competitive landscape review for {lead.niche}\",\n    ]\n    tasks_created = 0\n    for desc in task_descriptions[:random.randint(1, 2)]:\n        if plan_status.tasks_used + tasks_created >= plan_status.tasks_limit:\n            print(f\"[ONBOARDING] Trial task limit reached for new customer {customer.id}\")\n            break\n        \n        task = Task(\n            customer_id=customer.id,\n            description=desc,\n            status=\"pending\",\n            reward_cents=random.randint(50, 200),\n        )\n        session.add(task)\n        tasks_created += 1\n\n    lead.status = \"qualified\"\n    session.add(lead)\n    session.commit()\n\n    plan_info = f\" (Plan: {customer.plan})\"\n    msg = f\"Onboarding: Converted {lead.company}  Customer {customer.id}. Created {tasks_created} tasks.{plan_info}\"\n    print(f\"[CYCLE] {msg}\")\n    return msg\n\n\nasync def run_ops_cycle(session: Session) -> str:\n    \"\"\"\n    Ops Cycle: Pick next pending task, mark running, simulate work, mark done.\n    Calculates cost and profit.\n    \n    Plan Gating:\n    - Trial customers: Limited to TRIAL_TASK_LIMIT tasks total\n    - Paid customers: Unlimited tasks\n    \n    Hook for real OpenAI integration:\n    - Replace simulated result with real API call\n    - Read OPENAI_API_KEY from environment\n    - Call gpt-4-mini or gpt-4o-mini\n    - Parse response and estimate token cost\n    \"\"\"\n    statement = select(Task).where(Task.status == \"pending\").limit(1)\n    task = session.exec(statement).first()\n\n    if not task:\n        msg = \"Ops: No pending tasks.\"\n        print(f\"[CYCLE] {msg}\")\n        return msg\n\n    customer = session.exec(\n        select(Customer).where(Customer.id == task.customer_id)\n    ).first()\n\n    if not customer:\n        msg = f\"Ops: Task {task.id} has no associated customer.\"\n        print(f\"[CYCLE] {msg}\")\n        return msg\n\n    plan_status = get_customer_plan_status(customer)\n    \n    if plan_status.is_expired:\n        msg = f\"Ops: Customer {customer.company} trial expired. Upgrade required.\"\n        print(f\"[CYCLE][GATED] {msg}\")\n        return msg\n    \n    if not plan_status.can_run_tasks:\n        msg = f\"Ops: Customer {customer.company} reached trial task limit ({plan_status.tasks_used}/{plan_status.tasks_limit}). Upgrade required.\"\n        print(f\"[CYCLE][GATED] {msg}\")\n        return msg\n\n    task.status = \"running\"\n    session.add(task)\n    session.commit()\n\n    simulated_result = f\"Research Summary: Analyzed '{task.description}' for {customer.company}. Key findings: market opportunity identified, competitive positioning clear, actionable recommendations provided.\"\n    cost_cents = random.randint(2, 8)\n    profit_cents = max(0, task.reward_cents - cost_cents)\n\n    task.status = \"done\"\n    task.cost_cents = cost_cents\n    task.profit_cents = profit_cents\n    task.result_summary = simulated_result\n    task.completed_at = datetime.utcnow()\n    session.add(task)\n    \n    create_report(\n        session=session,\n        customer_id=customer.id,\n        title=task.description,\n        description=f\"Research completed for {customer.company}\",\n        content=simulated_result,\n        report_type=\"research\"\n    )\n    \n    session.commit()\n    \n    session.refresh(customer)\n\n    plan_info = f\" [Plan: {customer.plan}, Tasks: {customer.tasks_this_period}/{plan_status.tasks_limit if plan_status.is_trial else 'unlimited'}]\"\n    msg = f\"Ops: Completed task {task.id} ({customer.company}). Cost: {cost_cents}c, Profit: {profit_cents}c{plan_info}\"\n    print(f\"[CYCLE] {msg}\")\n    return msg\n\n\nasync def run_billing_cycle(session: Session) -> str:\n    \"\"\"\n    Billing Cycle: Aggregate completed tasks per customer.\n    Generate draft invoice records for uninvoiced work.\n    Create Stripe payment links if ENABLE_STRIPE=TRUE.\n    \n    Plan Gating:\n    - Trial customers: Billing agent DISABLED (no invoices, no payment links)\n    - Paid customers: Full billing functionality\n    \n    Safe to call repeatedly: skips customers/tasks already invoiced.\n    Amount safety clamp: $1-$500 (configurable in stripe_utils).\n    \"\"\"\n    from stripe_utils import create_payment_link, is_stripe_enabled\n    \n    statement = select(Customer).limit(100)\n    customers = session.exec(statement).all()\n\n    invoices_created = 0\n    payment_links_created = 0\n    trial_skipped = 0\n    msg_parts = []\n\n    for customer in customers:\n        plan_status = get_customer_plan_status(customer)\n        \n        if not plan_status.can_use_billing:\n            trial_skipped += 1\n            continue\n\n        task_statement = select(Task).where(\n            (Task.customer_id == customer.id) & (Task.status == \"done\")\n        )\n        completed_tasks = session.exec(task_statement).all()\n\n        if not completed_tasks:\n            continue\n\n        total_reward = sum(t.reward_cents for t in completed_tasks)\n\n        if total_reward > 0:\n            invoice_statement = select(Invoice).where(\n                (Invoice.customer_id == customer.id) & (Invoice.status == \"draft\")\n            )\n            existing_invoice = session.exec(invoice_statement).first()\n\n            if not existing_invoice:\n                invoice = Invoice(\n                    customer_id=customer.id,\n                    amount_cents=total_reward,\n                    status=\"draft\",\n                    notes=f\"Generated from {len(completed_tasks)} completed tasks\",\n                )\n                session.add(invoice)\n                session.flush()\n                invoices_created += 1\n                \n                if is_stripe_enabled():\n                    result = create_payment_link(\n                        amount_cents=total_reward,\n                        customer_id=customer.id,\n                        customer_email=customer.contact_email,\n                        description=f\"Invoice #{invoice.id} - {customer.company}\",\n                        invoice_id=invoice.id\n                    )\n                    \n                    if result.success:\n                        invoice.payment_url = result.payment_url\n                        invoice.stripe_payment_id = result.stripe_id\n                        session.add(invoice)\n                        payment_links_created += 1\n                        print(f\"[BILLING] Stripe payment link created for invoice {invoice.id}\")\n                    else:\n                        print(f\"[BILLING] Stripe payment link failed: {result.error}\")\n                \n                msg_parts.append(f\"{customer.company}: ${total_reward/100:.2f}\")\n\n    session.commit()\n    \n    stripe_status = \" (Stripe: enabled)\" if is_stripe_enabled() else \" (Stripe: disabled)\"\n    trial_info = f\" Trial customers skipped: {trial_skipped}.\" if trial_skipped > 0 else \"\"\n    msg = f\"Billing: Generated {invoices_created} invoices, {payment_links_created} payment links.{stripe_status}{trial_info} \" + (\"; \".join(msg_parts) if msg_parts else \"None.\")\n    print(f\"[CYCLE] {msg}\")\n    return msg\n\n\nasync def run_event_driven_bizdev_cycle(session: Session) -> str:\n    \"\"\"\n    Event-Driven BizDev Cycle: Send contextual outreach based on LeadEvents from Signals Engine.\n    \n    This is a moment-aware outreach system that:\n    1. Selects LeadEvents with enrichment_status = ENRICHED_NO_OUTBOUND (ready to send)\n    2. Skips UNENRICHED and WITH_DOMAIN_NO_EMAIL (wait for enrichment pipeline)\n    3. Checks customer's outreach_mode and do_not_contact list\n    4. Generates Miami-style contextual emails based on event.summary and event.recommended_action\n    5. If AUTO mode: sends email immediately\n    6. If REVIEW mode: creates PendingOutbound for customer approval\n    7. Gets CC/Reply-To from BusinessProfile.primary_contact_email\n    8. Updates enrichment_status to OUTBOUND_SENT and status to 'CONTACTED'\n    \n    Enrichment Status Flow:\n    - UNENRICHED  WITH_DOMAIN_NO_EMAIL  ENRICHED_NO_OUTBOUND  OUTBOUND_SENT\n    \n    Safe to call repeatedly - only processes ENRICHED_NO_OUTBOUND events.\n    \n    Outbound Autopilot: When outbound_autopilot_enabled=False, this cycle is skipped\n    and leads remain in ENRICHED_NO_OUTBOUND for manual approval.\n    \"\"\"\n    settings = session.exec(\n        select(SystemSettings).where(SystemSettings.id == 1)\n    ).first()\n    \n    outbound_enabled = getattr(settings, 'outbound_autopilot_enabled', True) if settings else True\n    \n    if not outbound_enabled:\n        awaiting = len(session.exec(\n            select(LeadEvent)\n            .where(LeadEvent.status == LEAD_STATUS_NEW)\n            .where(LeadEvent.enrichment_status.in_([\n                ENRICHMENT_STATUS_ENRICHED_NO_OUTBOUND,\n                ENRICHMENT_STATUS_ENRICHED,\n                ENRICHMENT_STATUS_OUTBOUND_READY\n            ]))\n        ).all())\n        msg = f\"Event-Driven BizDev: Outbound autopilot OFF. {awaiting} leads awaiting manual approval.\"\n        print(f\"[CYCLE] {msg}\")\n        return msg\n    \n    max_events = get_max_emails_per_cycle()\n    email_status = get_email_status()\n    effective_mode = email_status[\"mode\"]\n    \n    new_events = session.exec(\n        select(LeadEvent)\n        .where(LeadEvent.status == LEAD_STATUS_NEW)\n        .where(LeadEvent.enrichment_status.in_([\n            ENRICHMENT_STATUS_ENRICHED_NO_OUTBOUND,\n            ENRICHMENT_STATUS_ENRICHED,\n            ENRICHMENT_STATUS_OUTBOUND_READY\n        ]))\n        .order_by(LeadEvent.urgency_score.desc())\n        .limit(max_events)\n    ).all()\n    \n    unenriched_count = len(session.exec(\n        select(LeadEvent)\n        .where(LeadEvent.status == LEAD_STATUS_NEW)\n        .where(LeadEvent.enrichment_status == ENRICHMENT_STATUS_UNENRICHED)\n    ).all())\n    \n    with_domain_count = len(session.exec(\n        select(LeadEvent)\n        .where(LeadEvent.status == LEAD_STATUS_NEW)\n        .where(LeadEvent.enrichment_status == ENRICHMENT_STATUS_WITH_DOMAIN_NO_EMAIL)\n    ).all())\n    \n    print(f\"[EVENT-BIZDEV] Found {len(new_events)} ready for outbound, {unenriched_count} unenriched, {with_domain_count} with domain only\")\n    \n    if not new_events:\n        if unenriched_count > 0:\n            msg = f\"Event-Driven BizDev: No enriched events to process. {unenriched_count} events awaiting enrichment.\"\n        else:\n            msg = \"Event-Driven BizDev: No new lead events to process.\"\n        print(f\"[CYCLE] {msg}\")\n        return msg\n    \n    events_processed = 0\n    events_contacted = 0\n    events_failed = 0\n    events_queued = 0\n    events_blocked = 0\n    events_rate_limited = 0\n    contacted_summaries = []\n    \n    for event in new_events:\n        company_name = event.lead_company or event.enriched_company_name or \"Your company\"\n        \n        result = send_lead_event_immediate(session, event, commit=False)\n        events_processed += 1\n        \n        if result.success:\n            if result.email_sent:\n                events_contacted += 1\n                contacted_summaries.append(f\"{company_name} ({event.category})\")\n                print(f\"[EVENT-BIZDEV] Event {event.id} for {company_name}: SENT via immediate-send\")\n            elif result.queued_for_review:\n                events_queued += 1\n                print(f\"[EVENT-BIZDEV] Event {event.id} for {company_name}: QUEUED for review\")\n        else:\n            if result.action == \"blocked\":\n                events_blocked += 1\n            elif result.action == \"rate_limited\":\n                events_rate_limited += 1\n            else:\n                events_failed += 1\n            print(f\"[EVENT-BIZDEV] Event {event.id} for {company_name}: {result.action.upper()} - {result.reason}\")\n\n    session.commit()\n    \n    summaries_str = \", \".join(contacted_summaries[:5]) if contacted_summaries else \"None\"\n    if len(contacted_summaries) > 5:\n        summaries_str += f\" (+{len(contacted_summaries) - 5} more)\"\n    \n    queued_info = f\", Queued: {events_queued}\" if events_queued > 0 else \"\"\n    blocked_info = f\", Blocked: {events_blocked}\" if events_blocked > 0 else \"\"\n    rate_limit_info = f\", Rate-limited: {events_rate_limited}\" if events_rate_limited > 0 else \"\"\n    unenriched_info = f\", Awaiting enrichment: {unenriched_count}\" if unenriched_count > 0 else \"\"\n    msg = f\"Event-Driven BizDev: Processed {events_processed} events, contacted {events_contacted}. Failed: {events_failed}{queued_info}{blocked_info}{rate_limit_info}{unenriched_info}. Mode: {effective_mode}. Companies: {summaries_str}\"\n    print(f\"[CYCLE] {msg}\")\n    return msg\n\n\nSUBJECT_LINE_LIBRARY = [\n    \"Quick heads-up for {company}\",\n    \"New signal in your market\",\n    \"Saw something relevant to {company}\",\n    \"Small opportunity I noticed near {city}\",\n    \"Thought this might be timely for you\",\n    \"Local lead-gen idea for {company}\",\n    \"Context on a shift near {city}\",\n    \"Your competitors are moving\",\n    \"New biz dev opportunity in your space\",\n    \"Short note about {signal_handle}\",\n    \"Something came up for {company}\",\n    \"Noticed a shift in {niche}\",\n]\n\n\ndef parse_first_name(full_name: str) -> str:\n    \"\"\"\n    Parse first name from full name string.\n    \n    Rules:\n    - If first_name exists and is non-empty: use it\n    - If only have full name string: use first token as first name\n    - Else: return \"there\"\n    - Never use \"First Last\" as greeting\n    \"\"\"\n    if not full_name or full_name.strip().lower() in (\"there\", \"unknown\", \"none\", \"\"):\n        return \"there\"\n    \n    parts = full_name.strip().split()\n    if len(parts) >= 1:\n        first = parts[0].strip()\n        if first and len(first) > 1:\n            return first.title()\n    \n    return \"there\"\n\n\ndef get_subject_line(\n    company_name: str,\n    city: str,\n    niche: str,\n    signal_handle: str,\n    event_id: int,\n    signal_id: int = None\n) -> str:\n    \"\"\"\n    Get subject line from library with consistent rotation.\n    \n    Uses hash of (event_id, signal_id) to pick subject consistently\n    so same lead+signal always gets same subject line.\n    \"\"\"\n    import hashlib\n    \n    hash_input = f\"{event_id}-{signal_id or 0}\"\n    hash_value = int(hashlib.md5(hash_input.encode()).hexdigest()[:8], 16)\n    index = hash_value % len(SUBJECT_LINE_LIBRARY)\n    \n    template = SUBJECT_LINE_LIBRARY[index]\n    \n    return template.format(\n        company=company_name or \"your company\",\n        city=city or \"Miami\",\n        niche=niche or \"your space\",\n        signal_handle=signal_handle or \"a recent signal\"\n    )\n\n\ndef _detect_signal_type(event_summary: str, category: str) -> str:\n    \"\"\"\n    Detect the strategic type of signal to determine outreach approach.\n    \n    Returns: 'market_entry', 'competitor_intel', 'growth_opportunity', 'market_shift'\n    \"\"\"\n    summary_lower = (event_summary or \"\").lower()\n    \n    if any(phrase in summary_lower for phrase in [\n        \"competitor\", \"rival\", \"new player\", \"competing\", \"market share\"\n    ]):\n        return \"competitor_intel\"\n    \n    if any(phrase in summary_lower for phrase in [\n        \"opening\", \"expand\", \"enters\", \"entering\", \"launch\", \"new location\",\n        \"setting up\", \"establish\", \"relocat\", \"move to\", \"operations in\"\n    ]):\n        return \"market_entry\"\n    \n    if any(phrase in summary_lower for phrase in [\n        \"hiring\", \"job posting\", \"recruit\", \"growing team\", \"expansion\"\n    ]):\n        return \"growth_opportunity\"\n    \n    if category in [\"COMPETITOR_SHIFT\", \"MIAMI_PRICE_MOVE\"]:\n        return \"competitor_intel\"\n    \n    return \"market_shift\"\n\n\ndef _generate_actionable_insights(signal_type: str, niche: str, city: str) -> tuple[str, str, str]:\n    \"\"\"\n    Generate actionable insights based on signal type.\n    \n    Returns: (market_context, specific_recommendations, product_tie_in)\n    \"\"\"\n    insights = {\n        \"market_entry\": (\n            f\"The {city} {niche} market is competitive but has clear patterns for success\",\n            f\"\"\"Here's what I'm seeing work for {niche} businesses breaking into {city}:\n1. Local partnerships beat cold advertising 3-to-1 for customer acquisition\n2. Bilingual operations (English/Spanish) typically see 40% higher retention\n3. The first 90 days determine 80% of long-term success - speed matters\"\"\",\n            f\"I track these patterns across hundreds of {city} businesses and can show you exactly what's working in your space right now\"\n        ),\n        \"competitor_intel\": (\n            f\"New competition means the {city} {niche} landscape is shifting\",\n            f\"\"\"When new players enter, here's what successful {niche} operators do:\n1. Double down on what differentiates you - now is not the time to be generic\n2. Lock in your best customers before competitors start poaching\n3. Watch their pricing strategy - early signals predict their long game\"\"\",\n            f\"I monitor competitor moves across {city} in real-time and can flag when you need to react\"\n        ),\n        \"growth_opportunity\": (\n            f\"Growth signals in {city}'s {niche} sector indicate timing advantages\",\n            f\"\"\"The businesses capitalizing fastest on these moments typically:\n1. Move within 2-3 weeks of the signal - timing decay is real\n2. Have a clear \"next step\" ready for interested leads\n3. Use the momentum for social proof and referral asks\"\"\",\n            f\"I can surface these opportunities the moment they appear so you're first to act\"\n        ),\n        \"market_shift\": (\n            f\"Market conditions in {city} are creating short windows of opportunity\",\n            f\"\"\"Here's what's working for {niche} businesses right now:\n1. Businesses that adapt their messaging to current conditions see 2x engagement\n2. Proactive outreach during market shifts outperforms waiting\n3. The businesses that act in the next 30 days will set the pace for the next quarter\"\"\",\n            f\"I track these signals continuously so you never miss a window\"\n        )\n    }\n    \n    return insights.get(signal_type, insights[\"market_shift\"])\n\n\ndef generate_miami_contextual_email(\n    contact_name: str,\n    company_name: str,\n    niche: str,\n    event_summary: str,\n    recommended_action: str,\n    category: str,\n    urgency_score: int,\n    outreach_style: str = \"transparent_ai\",\n    event_id: int = 0,\n    signal_id: int = None,\n    city: str = \"Miami\",\n    source_url: str = None\n) -> tuple[str, str]:\n    \"\"\"\n    Generate strategic contextual email with actionable insights.\n    \n    Key principles:\n    1. Introduce sender FIRST, AI disclosure comes later\n    2. Provide NOVEL value - not just regurgitating news they already know\n    3. Give specific, actionable recommendations tied to their situation\n    4. Clear product tie-in that explains the \"so what\"\n    \n    Two template styles:\n    - \"transparent_ai\": Full disclosure with strategic insights\n    - \"classic\": Professional outbound with clear value prop\n    \n    Returns: (subject, body) tuple\n    \"\"\"\n    import os\n    \n    website_url = os.environ.get(\"HOSSAGENT_WEBSITE_URL\", \"https://hossagent.net\")\n    \n    first_name = parse_first_name(contact_name)\n    \n    signal_type = _detect_signal_type(event_summary, category)\n    market_context, recommendations, product_tie = _generate_actionable_insights(signal_type, niche, city)\n    \n    signal_handle = None\n    summary_lower = event_summary.lower() if event_summary else \"\"\n    if \"job posting\" in summary_lower or \"hiring\" in summary_lower:\n        signal_handle = \"growth activity\"\n    elif \"review\" in summary_lower:\n        signal_handle = \"market feedback\"\n    elif \"competitor\" in summary_lower:\n        signal_handle = \"competitive movement\"\n    elif \"opening\" in summary_lower or \"expand\" in summary_lower or \"entering\" in summary_lower:\n        signal_handle = \"market entry\"\n    else:\n        signal_handle = \"a market signal\"\n    \n    subject = get_subject_line(\n        company_name=company_name,\n        city=city,\n        niche=niche,\n        signal_handle=signal_handle,\n        event_id=event_id,\n        signal_id=signal_id\n    )\n    \n    source_line = \"\"\n    if source_url:\n        source_line = f\"\\n(Story: {source_url})\\n\"\n    \n    if outreach_style == \"transparent_ai\":\n        body = f\"\"\"Hi {first_name},\n\nMy name is Sam Holliday - I run HossAgent, an AI-powered business autopilot for local service companies in {city}.\n\nI built it so owners don't have to watch the news or babysit signals all day. It flags moments worth acting on and drafts clean outreach automatically.\n\nToday {company_name} popped onto my radar because of this specific event:\n\n{event_summary}{source_line}\nHere's why this matters: {market_context}.\n\n{recommendations}\n\n{product_tie}.\n\nIf any of this resonates, I'd be happy to share a quick competitive snapshot of your space - no pitch, just useful intel. Reply \"send it\" and I'll put something together, or grab 15 minutes on my calendar if that's easier.\n\nYou can see what we're building at {website_url} - we offer a 7-day free trial if you'd rather have the system watching the market for you.\n\nReply \"no thanks\" if this isn't relevant and I won't reach out again.\n\n- Sam Holliday\nFounder, HossAgent\n{website_url}\"\"\"\n\n    else:\n        body = f\"\"\"Hi {first_name},\n\nMy name is Sam Holliday - I run a market intelligence service for local businesses in {city}.\n\nToday {company_name} came across my radar:\n\n{event_summary}{source_line}\nHere's why this matters: {market_context}.\n\n{recommendations}\n\n{product_tie}.\n\nWould a quick 15-minute call make sense? I can share what's working for similar businesses in your space right now.\n\nReply \"interested\" and I'll send over some times, or \"no thanks\" if this isn't a fit.\n\n- Sam Holliday\n{website_url}\"\"\"\n\n    return subject, body\n\n\ndef check_rate_limits(\n    session,\n    lead_email: str,\n    event_id: int,\n    customer_id: int = None\n) -> tuple[bool, str]:\n    \"\"\"\n    Check if outbound to this lead is allowed under rate limits.\n    \n    Checks both lead_email and enriched_email fields for historical sends.\n    \n    Returns: (allowed, reason)\n    - allowed: True if email can be sent\n    - reason: Explanation if blocked\n    \"\"\"\n    from models import (\n        LeadEvent, MAX_OUTBOUND_PER_LEAD_PER_DAY, \n        MAX_OUTBOUND_PER_LEAD_PER_WEEK, MAX_OUTBOUND_PER_CUSTOMER_PER_DAY\n    )\n    from datetime import timedelta\n    from sqlalchemy import or_\n    \n    now = datetime.utcnow()\n    day_ago = now - timedelta(days=1)\n    week_ago = now - timedelta(days=7)\n    \n    contacted_24h = session.exec(\n        select(LeadEvent)\n        .where(or_(LeadEvent.enriched_email == lead_email, LeadEvent.lead_email == lead_email))\n        .where(LeadEvent.status == LEAD_STATUS_CONTACTED)\n        .where(LeadEvent.last_contact_at >= day_ago)\n    ).all()\n    \n    if len(contacted_24h) >= MAX_OUTBOUND_PER_LEAD_PER_DAY:\n        return False, f\"Rate limit: {lead_email} already contacted in last 24h\"\n    \n    contacted_7d = session.exec(\n        select(LeadEvent)\n        .where(or_(LeadEvent.enriched_email == lead_email, LeadEvent.lead_email == lead_email))\n        .where(LeadEvent.status == LEAD_STATUS_CONTACTED)\n        .where(LeadEvent.last_contact_at >= week_ago)\n    ).all()\n    \n    if len(contacted_7d) >= MAX_OUTBOUND_PER_LEAD_PER_WEEK:\n        return False, f\"Rate limit: {lead_email} contacted {len(contacted_7d)} times this week\"\n    \n    if customer_id:\n        customer_today = session.exec(\n            select(LeadEvent)\n            .where(LeadEvent.company_id == customer_id)\n            .where(LeadEvent.status == LEAD_STATUS_CONTACTED)\n            .where(LeadEvent.last_contact_at >= day_ago)\n        ).all()\n        \n        if len(customer_today) >= MAX_OUTBOUND_PER_CUSTOMER_PER_DAY:\n            return False, f\"Rate limit: Customer daily cap ({MAX_OUTBOUND_PER_CUSTOMER_PER_DAY}) reached\"\n    \n    return True, \"OK\"\n\n\ndef check_opt_out(reply_text: str) -> bool:\n    \"\"\"\n    Check if reply text contains opt-out phrases.\n    \n    Returns True if this is an opt-out request.\n    \"\"\"\n    from models import OPT_OUT_PHRASES\n    \n    if not reply_text:\n        return False\n    \n    reply_lower = reply_text.lower().strip()\n    \n    for phrase in OPT_OUT_PHRASES:\n        if phrase in reply_lower:\n            return True\n    \n    return False\n\n\ndef mark_do_not_contact(session, event: 'LeadEvent', reason: str = \"opt_out_reply\"):\n    \"\"\"\n    Mark a LeadEvent as do-not-contact.\n    \"\"\"\n    from datetime import datetime\n    \n    event.do_not_contact = True\n    event.do_not_contact_reason = reason\n    event.do_not_contact_at = datetime.utcnow()\n    session.add(event)\n    session.commit()\n    print(f\"[SUPPRESSION] Marked event {event.id} ({event.enriched_email}) as do_not_contact: {reason}\")\n","path":null,"size_bytes":35276,"size_tokens":null},"email_utils.py":{"content":"\"\"\"\nOutbound email infrastructure for HossAgent.\nSupports three modes: DRY_RUN, SENDGRID, SES\n\nDomain: hossagent.net (authenticated)\n\nEnvironment Variables:\n  EMAIL_MODE = DRY_RUN | SENDGRID | SES (defaults to DRY_RUN)\n  \n  For SENDGRID (required when EMAIL_MODE=SENDGRID):\n    SENDGRID_API_KEY       - SendGrid API key\n    OUTBOUND_FROM          - Sending email (e.g., hello@hossagent.net)\n    OUTBOUND_REPLY_TO      - Reply-to email address\n    OUTBOUND_DISPLAY_NAME  - Display name (e.g., HossAgent)\n    \n  For SES (required when EMAIL_MODE=SES):\n    AWS_SES_ACCESS_KEY     - AWS access key ID\n    AWS_SES_SECRET_KEY     - AWS secret access key\n    AWS_SES_REGION         - AWS region (e.g., us-east-1)\n    OUTBOUND_FROM          - Verified sending email (e.g., hello@hossagent.net)\n    OUTBOUND_DISPLAY_NAME  - Display name (e.g., HossAgent)\n    \n  Throttling:\n    MAX_EMAILS_PER_CYCLE (default: 10)\n    MAX_EMAILS_PER_HOUR (default: 50)\n    \n  Deliverability:\n    EMAIL_SEND_DELAY_MIN (default: 1) - minimum delay between sends in seconds\n    EMAIL_SEND_DELAY_MAX (default: 5) - maximum delay between sends in seconds\n\"\"\"\nimport os\nimport json\nimport time\nimport random\nimport html\nfrom enum import Enum\nfrom datetime import datetime, timedelta\nfrom typing import Optional, List, Dict, Any, Tuple\nfrom dataclasses import dataclass, asdict, field\nfrom pathlib import Path\n\n\nclass EmailMode(str, Enum):\n    DRY_RUN = \"DRY_RUN\"\n    SENDGRID = \"SENDGRID\"\n    SES = \"SES\"\n\n\n@dataclass\nclass EmailResult:\n    \"\"\"Unified result object for all email send attempts.\"\"\"\n    success: bool\n    mode: str  # \"DRY_RUN\", \"SENDGRID\"\n    result: str  # \"success\", \"failed\", \"dry_run\", \"throttled\"\n    error: Optional[str] = None\n    actually_sent: bool = False  # True only if email was really sent (not DRY_RUN)\n    sendgrid_response: Optional[Dict[str, Any]] = None  # Full SendGrid response for debugging\n\n\n@dataclass\nclass EmailAttempt:\n    timestamp: str\n    lead_name: str\n    company: str\n    to_email: str\n    subject: str\n    mode: str\n    result: str  # \"success\", \"failed\", \"dry_run\", \"throttled\"\n    error: Optional[str] = None\n    sending_domain: Optional[str] = None\n    sendgrid_headers: Optional[Dict[str, Any]] = None\n\n\nEMAIL_LOG_FILE = Path(\"email_attempts.json\")\nHOURLY_COUNTER_FILE = Path(\"email_hourly_counter.json\")\nMAX_LOG_ENTRIES = 5000  # Capped log entries to prevent unbounded growth\n\n\ndef get_send_delay_range() -> Tuple[int, int]:\n    \"\"\"Get the delay range between email sends for deliverability.\"\"\"\n    try:\n        min_delay = int(os.getenv(\"EMAIL_SEND_DELAY_MIN\", \"1\"))\n        max_delay = int(os.getenv(\"EMAIL_SEND_DELAY_MAX\", \"5\"))\n        return max(0, min_delay), max(min_delay, max_delay)\n    except ValueError:\n        return 1, 5\n\n\ndef apply_send_delay() -> None:\n    \"\"\"Apply a random delay between email sends for deliverability.\"\"\"\n    min_delay, max_delay = get_send_delay_range()\n    if min_delay > 0 or max_delay > 0:\n        delay = random.uniform(min_delay, max_delay)\n        time.sleep(delay)\n\n\ndef extract_domain(email: str) -> str:\n    \"\"\"Extract domain from email address.\"\"\"\n    if \"@\" in email:\n        return email.split(\"@\")[1]\n    return \"\"\n\n\ndef _load_email_log() -> List[Dict[str, Any]]:\n    \"\"\"Load email attempt log from file.\"\"\"\n    try:\n        if EMAIL_LOG_FILE.exists():\n            with open(EMAIL_LOG_FILE, \"r\") as f:\n                return json.load(f)\n    except Exception:\n        pass\n    return []\n\n\ndef _save_email_log(entries: List[Dict[str, Any]]) -> None:\n    \"\"\"Save email attempt log to file.\"\"\"\n    try:\n        entries = entries[-MAX_LOG_ENTRIES:]\n        with open(EMAIL_LOG_FILE, \"w\") as f:\n            json.dump(entries, f, indent=2)\n    except Exception as e:\n        print(f\"[EMAIL] Warning: Could not save email log: {e}\")\n\n\ndef log_email_attempt(attempt: EmailAttempt) -> None:\n    \"\"\"Log an email attempt for admin console visibility.\"\"\"\n    entries = _load_email_log()\n    entries.append(asdict(attempt))\n    _save_email_log(entries)\n\n\ndef get_email_log(limit: int = 10) -> List[Dict[str, Any]]:\n    \"\"\"Get the last N email attempts for display in admin console.\"\"\"\n    entries = _load_email_log()\n    return entries[-limit:]\n\n\ndef _load_hourly_counter() -> Dict[str, Any]:\n    \"\"\"Load hourly email counter from file.\"\"\"\n    try:\n        if HOURLY_COUNTER_FILE.exists():\n            with open(HOURLY_COUNTER_FILE, \"r\") as f:\n                data = json.load(f)\n                hour_key = datetime.utcnow().strftime(\"%Y-%m-%d-%H\")\n                if data.get(\"hour\") == hour_key:\n                    return data\n    except Exception:\n        pass\n    return {\"hour\": datetime.utcnow().strftime(\"%Y-%m-%d-%H\"), \"count\": 0}\n\n\ndef _save_hourly_counter(data: Dict[str, Any]) -> None:\n    \"\"\"Save hourly email counter to file.\"\"\"\n    try:\n        with open(HOURLY_COUNTER_FILE, \"w\") as f:\n            json.dump(data, f)\n    except Exception as e:\n        print(f\"[EMAIL] Warning: Could not save hourly counter: {e}\")\n\n\ndef get_max_emails_per_hour() -> int:\n    \"\"\"Get the maximum number of emails to send per hour.\"\"\"\n    try:\n        return int(os.getenv(\"MAX_EMAILS_PER_HOUR\", \"50\"))\n    except ValueError:\n        return 50\n\n\ndef check_hourly_limit() -> Tuple[bool, int, int]:\n    \"\"\"\n    Check if hourly email limit has been reached.\n    \n    Returns:\n        (can_send, current_count, max_count)\n    \"\"\"\n    counter = _load_hourly_counter()\n    max_per_hour = get_max_emails_per_hour()\n    current = counter.get(\"count\", 0)\n    return current < max_per_hour, current, max_per_hour\n\n\ndef increment_hourly_counter() -> None:\n    \"\"\"Increment the hourly email counter after a successful send.\"\"\"\n    counter = _load_hourly_counter()\n    counter[\"count\"] = counter.get(\"count\", 0) + 1\n    _save_hourly_counter(counter)\n\n\ndef get_hourly_counter_status() -> Dict[str, Any]:\n    \"\"\"Get current hourly counter status for admin display.\"\"\"\n    counter = _load_hourly_counter()\n    max_per_hour = get_max_emails_per_hour()\n    return {\n        \"hour\": counter.get(\"hour\"),\n        \"count\": counter.get(\"count\", 0),\n        \"max\": max_per_hour,\n        \"remaining\": max_per_hour - counter.get(\"count\", 0)\n    }\n\n\ndef get_email_mode() -> EmailMode:\n    \"\"\"\n    Get the configured email mode from environment.\n    Falls back to DRY_RUN if EMAIL_MODE is not set or invalid.\n    \"\"\"\n    mode_str = os.getenv(\"EMAIL_MODE\", \"DRY_RUN\").upper()\n    \n    # Map legacy SMTP mode to DRY_RUN (SMTP no longer supported)\n    if mode_str == \"SMTP\":\n        print(\"[EMAIL][WARNING] SMTP mode is deprecated. Use SENDGRID mode with hossagent.net domain.\")\n        return EmailMode.DRY_RUN\n    \n    try:\n        return EmailMode(mode_str)\n    except ValueError:\n        print(f\"[EMAIL] Warning: Invalid EMAIL_MODE '{mode_str}', falling back to DRY_RUN\")\n        return EmailMode.DRY_RUN\n\n\ndef get_sendgrid_config() -> Dict[str, str]:\n    \"\"\"\n    Get SendGrid configuration from environment variables.\n    \n    Returns dict with:\n        - api_key: SENDGRID_API_KEY\n        - from_email: OUTBOUND_FROM\n        - reply_to: OUTBOUND_REPLY_TO\n        - display_name: OUTBOUND_DISPLAY_NAME\n    \"\"\"\n    return {\n        \"api_key\": os.getenv(\"SENDGRID_API_KEY\", \"\"),\n        \"from_email\": os.getenv(\"OUTBOUND_FROM\", \"\"),\n        \"reply_to\": os.getenv(\"OUTBOUND_REPLY_TO\", \"\"),\n        \"display_name\": os.getenv(\"OUTBOUND_DISPLAY_NAME\", \"HossAgent\"),\n    }\n\n\ndef validate_sendgrid_config() -> Tuple[bool, List[str]]:\n    \"\"\"\n    Validate that all required SendGrid environment variables are set.\n    \n    Returns:\n        (is_valid, missing_vars)\n    \"\"\"\n    config = get_sendgrid_config()\n    required_vars = {\n        \"SENDGRID_API_KEY\": config[\"api_key\"],\n        \"OUTBOUND_FROM\": config[\"from_email\"],\n        \"OUTBOUND_REPLY_TO\": config[\"reply_to\"],\n        \"OUTBOUND_DISPLAY_NAME\": config[\"display_name\"],\n    }\n    \n    missing = [var for var, value in required_vars.items() if not value]\n    return len(missing) == 0, missing\n\n\ndef validate_ses_config() -> Tuple[bool, List[str]]:\n    \"\"\"\n    Validate that all required SES environment variables are set.\n    \n    Returns:\n        (is_valid, missing_vars)\n    \"\"\"\n    config = get_ses_config()\n    required_vars = {\n        \"AWS_SES_ACCESS_KEY\": config[\"access_key\"],\n        \"AWS_SES_SECRET_KEY\": config[\"secret_key\"],\n        \"OUTBOUND_FROM\": config[\"from_email\"],\n    }\n    \n    missing = [var for var, value in required_vars.items() if not value]\n    return len(missing) == 0, missing\n\n\ndef validate_email_config() -> tuple[EmailMode, bool, str]:\n    \"\"\"\n    Validate email configuration for the selected mode.\n    \n    Returns:\n        (effective_mode, is_valid, message)\n        If validation fails, returns DRY_RUN with error message (no silent fallback).\n    \"\"\"\n    mode = get_email_mode()\n    \n    if mode == EmailMode.DRY_RUN:\n        return mode, True, \"DRY_RUN mode - no credentials required\"\n    \n    if mode == EmailMode.SENDGRID:\n        is_valid, missing = validate_sendgrid_config()\n        \n        if not is_valid:\n            error_msg = f\"SENDGRID mode requires these environment variables: {', '.join(missing)}\"\n            print(f\"[EMAIL][ERROR] {error_msg}\")\n            return EmailMode.DRY_RUN, False, error_msg\n        \n        config = get_sendgrid_config()\n        domain = extract_domain(config[\"from_email\"])\n        return mode, True, f\"SendGrid configured with domain: {domain}\"\n    \n    if mode == EmailMode.SES:\n        is_valid, missing = validate_ses_config()\n        \n        if not is_valid:\n            error_msg = f\"SES mode requires these environment variables: {', '.join(missing)}\"\n            print(f\"[EMAIL][ERROR] {error_msg}\")\n            return EmailMode.DRY_RUN, False, error_msg\n        \n        config = get_ses_config()\n        domain = extract_domain(config[\"from_email\"])\n        return mode, True, f\"Amazon SES configured with domain: {domain} (region: {config['region']})\"\n    \n    return EmailMode.DRY_RUN, True, \"Default DRY_RUN mode\"\n\n\ndef get_max_emails_per_cycle() -> int:\n    \"\"\"Get the maximum number of emails to send per cycle.\"\"\"\n    try:\n        return int(os.getenv(\"MAX_EMAILS_PER_CYCLE\", \"10\"))\n    except ValueError:\n        return 10\n\n\ndef plain_to_html(plain_text: str) -> str:\n    \"\"\"\n    Convert plain text email body to simple HTML.\n    Preserves paragraphs and line breaks.\n    \"\"\"\n    escaped = html.escape(plain_text)\n    \n    paragraphs = escaped.split('\\n\\n')\n    html_paragraphs = []\n    \n    for para in paragraphs:\n        lines = para.split('\\n')\n        html_para = '<br>\\n'.join(lines)\n        html_paragraphs.append(f'<p style=\"margin: 0 0 16px 0; line-height: 1.5;\">{html_para}</p>')\n    \n    html_body = '\\n'.join(html_paragraphs)\n    \n    return f\"\"\"<!DOCTYPE html>\n<html>\n<head>\n    <meta charset=\"utf-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n</head>\n<body style=\"font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; font-size: 14px; color: #333333; max-width: 600px; margin: 0 auto; padding: 20px;\">\n{html_body}\n</body>\n</html>\"\"\"\n\n\ndef send_email_dry_run(\n    to_email: str,\n    subject: str,\n    body: str,\n    lead_name: str = \"\",\n    company: str = \"\",\n    cc_email: Optional[str] = None,\n    reply_to: Optional[str] = None\n) -> EmailResult:\n    \"\"\"\n    Simulate sending email without actually sending.\n    Logs the full email content for review.\n    \"\"\"\n    config = get_sendgrid_config()\n    from_email = config[\"from_email\"] or \"hello@hossagent.net\"\n    display_name = config[\"display_name\"] or \"HossAgent\"\n    actual_reply_to = reply_to or config[\"reply_to\"] or from_email\n    sending_domain = extract_domain(from_email)\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"[EMAIL][DRY_RUN] Simulated Send\")\n    print(f\"{'='*60}\")\n    print(f\"  From: {display_name} <{from_email}>\")\n    print(f\"  To: {to_email}\")\n    if cc_email:\n        print(f\"  CC: {cc_email}\")\n    print(f\"  Reply-To: {actual_reply_to}\")\n    print(f\"  Subject: {subject}\")\n    print(f\"  Domain: {sending_domain}\")\n    print(f\"  Lead: {lead_name} ({company})\")\n    print(f\"  Body Preview: {body[:200]}...\")\n    print(f\"{'='*60}\\n\")\n    \n    log_email_attempt(EmailAttempt(\n        timestamp=datetime.utcnow().isoformat(),\n        lead_name=lead_name,\n        company=company,\n        to_email=to_email,\n        subject=subject,\n        mode=\"DRY_RUN\",\n        result=\"dry_run\",\n        sending_domain=sending_domain\n    ))\n    \n    return EmailResult(\n        success=True,  # DRY_RUN is considered \"successful\" for testing\n        mode=\"DRY_RUN\",\n        result=\"dry_run\",\n        error=None,\n        actually_sent=False\n    )\n\n\ndef send_email_sendgrid(\n    to_email: str,\n    subject: str,\n    body: str,\n    lead_name: str = \"\",\n    company: str = \"\",\n    cc_email: Optional[str] = None,\n    reply_to_override: Optional[str] = None\n) -> EmailResult:\n    \"\"\"\n    Send email via SendGrid API using authenticated hossagent.net domain.\n    \n    Sends multipart message with both plain text and HTML versions.\n    Logs detailed response including domain authentication status.\n    \n    Email headers:\n        From: OUTBOUND_DISPLAY_NAME <OUTBOUND_FROM>\n        To: lead_email (the prospect)\n        CC: customer.email (for visibility)\n        Reply-To: OUTBOUND_REPLY_TO (or override)\n    \"\"\"\n    try:\n        import requests\n        \n        config = get_sendgrid_config()\n        api_key = config[\"api_key\"]\n        from_email = config[\"from_email\"]\n        display_name = config[\"display_name\"]\n        default_reply_to = config[\"reply_to\"]\n        \n        # Validate required config\n        if not all([api_key, from_email]):\n            raise ValueError(\"SendGrid configuration incomplete. Required: SENDGRID_API_KEY, OUTBOUND_FROM\")\n        \n        sending_domain = extract_domain(from_email)\n        actual_reply_to = reply_to_override or default_reply_to or from_email\n        \n        # Build HTML version\n        html_body = plain_to_html(body)\n        \n        # Build personalization (To + optional CC)\n        personalization = {\"to\": [{\"email\": to_email}]}\n        if cc_email:\n            personalization[\"cc\"] = [{\"email\": cc_email}]\n        \n        # Build mail payload with multipart content\n        mail_data = {\n            \"personalizations\": [personalization],\n            \"from\": {\n                \"email\": from_email,\n                \"name\": display_name\n            },\n            \"reply_to\": {\n                \"email\": actual_reply_to\n            },\n            \"subject\": subject,\n            \"content\": [\n                {\"type\": \"text/plain\", \"value\": body},\n                {\"type\": \"text/html\", \"value\": html_body}\n            ]\n        }\n        \n        # Log outbound details before sending\n        print(f\"\\n[EMAIL][SENDGRID] Preparing send...\")\n        print(f\"  From: {display_name} <{from_email}>\")\n        print(f\"  To: {to_email}\")\n        if cc_email:\n            print(f\"  CC: {cc_email}\")\n        print(f\"  Reply-To: {actual_reply_to}\")\n        print(f\"  Subject: {subject[:60]}...\")\n        print(f\"  Domain: {sending_domain}\")\n        \n        # Send via SendGrid API\n        response = requests.post(\n            \"https://api.sendgrid.com/v3/mail/send\",\n            headers={\n                \"Authorization\": f\"Bearer {api_key}\",\n                \"Content-Type\": \"application/json\"\n            },\n            json=mail_data,\n            timeout=30\n        )\n        \n        # Parse response\n        response_headers = dict(response.headers)\n        response_body = response.text if response.text else \"{}\"\n        \n        # Extract useful SendGrid headers for debugging\n        sendgrid_debug = {\n            \"status_code\": response.status_code,\n            \"x_message_id\": response_headers.get(\"X-Message-Id\", \"\"),\n            \"content_length\": response_headers.get(\"Content-Length\", \"\"),\n            \"date\": response_headers.get(\"Date\", \"\"),\n            \"response_body\": response_body[:500] if response_body else \"\"\n        }\n        \n        if response.status_code in [200, 201, 202]:\n            print(f\"[EMAIL][SUCCESS][SENDGRID] Sent to {to_email}\")\n            print(f\"  Message-ID: {sendgrid_debug['x_message_id']}\")\n            print(f\"  Domain: {sending_domain} (authenticated)\")\n            \n            log_email_attempt(EmailAttempt(\n                timestamp=datetime.utcnow().isoformat(),\n                lead_name=lead_name,\n                company=company,\n                to_email=to_email,\n                subject=subject,\n                mode=\"SENDGRID\",\n                result=\"success\",\n                sending_domain=sending_domain,\n                sendgrid_headers=sendgrid_debug\n            ))\n            \n            return EmailResult(\n                success=True,\n                mode=\"SENDGRID\",\n                result=\"success\",\n                error=None,\n                actually_sent=True,\n                sendgrid_response=sendgrid_debug\n            )\n        else:\n            error_msg = f\"SendGrid API error {response.status_code}: {response_body[:300]}\"\n            print(f\"[EMAIL][FAIL][SENDGRID] {error_msg}\")\n            print(f\"  Full response: {response_body}\")\n            \n            log_email_attempt(EmailAttempt(\n                timestamp=datetime.utcnow().isoformat(),\n                lead_name=lead_name,\n                company=company,\n                to_email=to_email,\n                subject=subject,\n                mode=\"SENDGRID\",\n                result=\"failed\",\n                error=error_msg,\n                sending_domain=sending_domain,\n                sendgrid_headers=sendgrid_debug\n            ))\n            \n            return EmailResult(\n                success=False,\n                mode=\"SENDGRID\",\n                result=\"failed\",\n                error=error_msg,\n                actually_sent=False,\n                sendgrid_response=sendgrid_debug\n            )\n            \n    except ImportError:\n        error_msg = \"'requests' library not available\"\n        print(f\"[EMAIL][FAIL][SENDGRID] {error_msg}\")\n        return EmailResult(\n            success=False,\n            mode=\"SENDGRID\",\n            result=\"failed\",\n            error=error_msg,\n            actually_sent=False\n        )\n    except Exception as e:\n        error_msg = str(e)\n        print(f\"[EMAIL][FAIL][SENDGRID] Exception: {error_msg}\")\n        \n        log_email_attempt(EmailAttempt(\n            timestamp=datetime.utcnow().isoformat(),\n            lead_name=lead_name,\n            company=company,\n            to_email=to_email,\n            subject=subject,\n            mode=\"SENDGRID\",\n            result=\"failed\",\n            error=error_msg,\n            sending_domain=extract_domain(get_sendgrid_config().get(\"from_email\", \"\"))\n        ))\n        \n        return EmailResult(\n            success=False,\n            mode=\"SENDGRID\",\n            result=\"failed\",\n            error=error_msg,\n            actually_sent=False\n        )\n\n\ndef get_ses_config() -> Dict[str, str]:\n    \"\"\"Get Amazon SES configuration from environment variables.\"\"\"\n    return {\n        \"access_key\": os.getenv(\"AWS_SES_ACCESS_KEY\", \"\"),\n        \"secret_key\": os.getenv(\"AWS_SES_SECRET_KEY\", \"\"),\n        \"region\": os.getenv(\"AWS_SES_REGION\", \"us-east-1\"),\n        \"from_email\": os.getenv(\"OUTBOUND_FROM\", \"hello@hossagent.net\"),\n        \"display_name\": os.getenv(\"OUTBOUND_DISPLAY_NAME\", \"HossAgent\"),\n    }\n\n\ndef send_email_ses(\n    to_email: str,\n    subject: str,\n    body: str,\n    lead_name: str = \"\",\n    company: str = \"\",\n    cc_email: Optional[str] = None,\n    reply_to_override: Optional[str] = None\n) -> EmailResult:\n    \"\"\"\n    Send email via Amazon SES API.\n    \n    Uses boto3 SES client with MIME multipart for HTML + plain text.\n    \n    Email headers:\n        From: OUTBOUND_DISPLAY_NAME <OUTBOUND_FROM>\n        To: lead_email (the prospect)\n        CC: customer.email (for visibility)\n        Reply-To: customer.email (or override)\n    \"\"\"\n    try:\n        import boto3\n        from botocore.exceptions import ClientError\n        from email.mime.multipart import MIMEMultipart\n        from email.mime.text import MIMEText\n        \n        config = get_ses_config()\n        access_key = config[\"access_key\"]\n        secret_key = config[\"secret_key\"]\n        region = config[\"region\"]\n        from_email = config[\"from_email\"]\n        display_name = config[\"display_name\"]\n        \n        if not all([access_key, secret_key, from_email]):\n            raise ValueError(\"SES configuration incomplete. Required: AWS_SES_ACCESS_KEY, AWS_SES_SECRET_KEY, OUTBOUND_FROM\")\n        \n        sending_domain = extract_domain(from_email)\n        actual_reply_to = reply_to_override or from_email\n        \n        ses_client = boto3.client(\n            'ses',\n            aws_access_key_id=access_key,\n            aws_secret_access_key=secret_key,\n            region_name=region\n        )\n        \n        msg = MIMEMultipart('alternative')\n        msg['Subject'] = subject\n        msg['From'] = f\"{display_name} <{from_email}>\"\n        msg['To'] = to_email\n        if cc_email:\n            msg['Cc'] = cc_email\n        msg['Reply-To'] = actual_reply_to\n        \n        html_body = plain_to_html(body)\n        \n        part_text = MIMEText(body, 'plain')\n        part_html = MIMEText(html_body, 'html')\n        msg.attach(part_text)\n        msg.attach(part_html)\n        \n        destinations = [to_email]\n        if cc_email:\n            destinations.append(cc_email)\n        \n        print(f\"\\n[EMAIL][SES] Preparing send...\")\n        print(f\"  From: {display_name} <{from_email}>\")\n        print(f\"  To: {to_email}\")\n        if cc_email:\n            print(f\"  CC: {cc_email}\")\n        print(f\"  Reply-To: {actual_reply_to}\")\n        print(f\"  Subject: {subject[:60]}...\")\n        print(f\"  Domain: {sending_domain}\")\n        \n        response = ses_client.send_raw_email(\n            Source=f\"{display_name} <{from_email}>\",\n            Destinations=destinations,\n            RawMessage={'Data': msg.as_string()}\n        )\n        \n        message_id = response.get('MessageId', '')\n        \n        print(f\"[EMAIL][SUCCESS][SES] Sent to {to_email}\")\n        print(f\"  Message-ID: {message_id}\")\n        print(f\"  Domain: {sending_domain}\")\n        \n        log_email_attempt(EmailAttempt(\n            timestamp=datetime.utcnow().isoformat(),\n            lead_name=lead_name,\n            company=company,\n            to_email=to_email,\n            subject=subject,\n            mode=\"SES\",\n            result=\"success\",\n            sending_domain=sending_domain\n        ))\n        \n        return EmailResult(\n            success=True,\n            mode=\"SES\",\n            result=\"success\",\n            error=None,\n            actually_sent=True,\n            sendgrid_response={\"message_id\": message_id}\n        )\n        \n    except ImportError:\n        error_msg = \"'boto3' library not available\"\n        print(f\"[EMAIL][FAIL][SES] {error_msg}\")\n        return EmailResult(\n            success=False,\n            mode=\"SES\",\n            result=\"failed\",\n            error=error_msg,\n            actually_sent=False\n        )\n    except Exception as e:\n        error_msg = str(e)\n        print(f\"[EMAIL][FAIL][SES] Exception: {error_msg}\")\n        \n        log_email_attempt(EmailAttempt(\n            timestamp=datetime.utcnow().isoformat(),\n            lead_name=lead_name,\n            company=company,\n            to_email=to_email,\n            subject=subject,\n            mode=\"SES\",\n            result=\"failed\",\n            error=error_msg,\n            sending_domain=extract_domain(get_ses_config().get(\"from_email\", \"\"))\n        ))\n        \n        return EmailResult(\n            success=False,\n            mode=\"SES\",\n            result=\"failed\",\n            error=error_msg,\n            actually_sent=False\n        )\n\n\ndef send_email(\n    to_email: str,\n    subject: str,\n    body: str,\n    lead_name: str = \"\",\n    company: str = \"\",\n    cc_email: Optional[str] = None,\n    reply_to: Optional[str] = None\n) -> EmailResult:\n    \"\"\"\n    Unified email sending entrypoint.\n    \n    Determines the effective mode (with validation),\n    then dispatches to the appropriate sender.\n    \n    In SENDGRID mode:\n        - Uses authenticated hossagent.net domain\n        - Sends multipart (plain + HTML)\n        - NO fallback to other providers\n        \n    In DRY_RUN mode:\n        - Simulates send and logs for review\n        - Does NOT call SendGrid API\n    \n    Enforces both per-cycle and per-hour throttling limits.\n    Applies deliverability delays between real sends.\n    \n    Args:\n        to_email: Recipient email address (the lead/prospect)\n        subject: Email subject line\n        body: Plain text email body\n        lead_name: Name of the lead (for logging)\n        company: Company name (for logging)\n        cc_email: Optional CC email address (the customer)\n        reply_to: Optional Reply-To override\n    \n    Returns:\n        EmailResult with success status, mode, and error details.\n        This function NEVER crashes - all exceptions are caught.\n    \"\"\"\n    try:\n        effective_mode, is_valid, msg = validate_email_config()\n        \n        # In DRY_RUN mode, simulate without sending\n        if effective_mode == EmailMode.DRY_RUN:\n            if not is_valid:\n                print(f\"[EMAIL][CONFIG_ERROR] {msg}\")\n            return send_email_dry_run(to_email, subject, body, lead_name, company, cc_email=cc_email, reply_to=reply_to)\n        \n        # Check hourly rate limit\n        can_send, current, max_hour = check_hourly_limit()\n        if not can_send:\n            error_msg = f\"Hourly limit reached: {current}/{max_hour}\"\n            print(f\"[EMAIL][THROTTLED] hour_count={current}/{max_hour} email={to_email}\")\n            log_email_attempt(EmailAttempt(\n                timestamp=datetime.utcnow().isoformat(),\n                lead_name=lead_name,\n                company=company,\n                to_email=to_email,\n                subject=subject,\n                mode=effective_mode.value,\n                result=\"throttled\",\n                error=error_msg\n            ))\n            return EmailResult(\n                success=False,\n                mode=effective_mode.value,\n                result=\"throttled\",\n                error=error_msg,\n                actually_sent=False\n            )\n        \n        # Apply deliverability delay\n        apply_send_delay()\n        \n        # Route to appropriate email provider\n        if effective_mode == EmailMode.SES:\n            result = send_email_ses(\n                to_email, subject, body, lead_name, company,\n                cc_email=cc_email, reply_to_override=reply_to\n            )\n        else:\n            result = send_email_sendgrid(\n                to_email, subject, body, lead_name, company,\n                cc_email=cc_email, reply_to_override=reply_to\n            )\n        \n        # Increment hourly counter on successful send\n        if result.actually_sent:\n            increment_hourly_counter()\n        \n        return result\n            \n    except Exception as e:\n        error_msg = str(e)\n        print(f\"[EMAIL][FAIL] Unexpected error: {error_msg}\")\n        return EmailResult(\n            success=False,\n            mode=\"UNKNOWN\",\n            result=\"failed\",\n            error=error_msg,\n            actually_sent=False\n        )\n\n\ndef send_email_legacy(\n    to_email: str,\n    subject: str,\n    body: str,\n    lead_name: str = \"\",\n    company: str = \"\"\n) -> bool:\n    \"\"\"\n    Legacy wrapper that returns bool for backward compatibility.\n    Use send_email() for the full EmailResult object.\n    \"\"\"\n    result = send_email(to_email, subject, body, lead_name, company)\n    return result.actually_sent\n\n\ndef get_email_status() -> Dict[str, Any]:\n    \"\"\"\n    Get current email configuration status for admin display.\n    \n    Returns dict with:\n        - mode: Current effective mode\n        - configured_mode: What EMAIL_MODE env var says\n        - is_valid: Whether config is valid\n        - message: Status message\n        - domain: Sending domain (if configured)\n        - max_per_cycle: Throttle limit per cycle\n        - max_per_hour: Throttle limit per hour\n        - hourly: Current hourly counter status\n    \"\"\"\n    configured_mode = os.getenv(\"EMAIL_MODE\", \"DRY_RUN\").upper()\n    effective_mode, is_valid, message = validate_email_config()\n    hourly_status = get_hourly_counter_status()\n    \n    config = get_sendgrid_config()\n    sending_domain = extract_domain(config[\"from_email\"]) if config[\"from_email\"] else \"\"\n    \n    return {\n        \"mode\": effective_mode.value,\n        \"configured_mode\": configured_mode,\n        \"is_valid\": is_valid,\n        \"message\": message,\n        \"domain\": sending_domain,\n        \"from_email\": config[\"from_email\"],\n        \"display_name\": config[\"display_name\"],\n        \"reply_to\": config[\"reply_to\"],\n        \"max_per_cycle\": get_max_emails_per_cycle(),\n        \"max_per_hour\": get_max_emails_per_hour(),\n        \"hourly\": hourly_status\n    }\n\n\ndef get_sendgrid_stats(days: int = 7) -> Dict[str, Any]:\n    \"\"\"\n    Fetch email delivery statistics from SendGrid Stats API.\n    \n    Returns aggregated stats for the specified number of days:\n        - requests: Total emails requested to send\n        - delivered: Emails confirmed delivered\n        - opens: Total opens\n        - unique_opens: Unique recipients who opened\n        - clicks: Total link clicks  \n        - unique_clicks: Unique recipients who clicked\n        - bounces: Hard bounces\n        - blocks: Blocked by ISP\n        - spam_reports: Marked as spam\n        - daily: List of daily breakdowns\n    \n    Args:\n        days: Number of days to fetch (default 7, max 30)\n    \n    Returns:\n        Dict with stats or error info\n    \"\"\"\n    import requests\n    \n    api_key = os.getenv(\"SENDGRID_API_KEY\", \"\")\n    if not api_key:\n        return {\n            \"success\": False,\n            \"error\": \"SENDGRID_API_KEY not configured\",\n            \"stats\": None\n        }\n    \n    days = min(days, 30)\n    end_date = datetime.utcnow().strftime(\"%Y-%m-%d\")\n    start_date = (datetime.utcnow() - timedelta(days=days)).strftime(\"%Y-%m-%d\")\n    \n    try:\n        url = \"https://api.sendgrid.com/v3/stats\"\n        params = {\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"aggregated_by\": \"day\"\n        }\n        headers = {\n            \"Authorization\": f\"Bearer {api_key}\",\n            \"Content-Type\": \"application/json\"\n        }\n        \n        response = requests.get(url, params=params, headers=headers, timeout=15)\n        \n        if response.status_code != 200:\n            return {\n                \"success\": False,\n                \"error\": f\"SendGrid API error: {response.status_code} - {response.text[:200]}\",\n                \"stats\": None\n            }\n        \n        data = response.json()\n        \n        totals = {\n            \"requests\": 0,\n            \"delivered\": 0,\n            \"opens\": 0,\n            \"unique_opens\": 0,\n            \"clicks\": 0,\n            \"unique_clicks\": 0,\n            \"bounces\": 0,\n            \"blocks\": 0,\n            \"spam_reports\": 0,\n            \"deferred\": 0,\n            \"invalid_emails\": 0\n        }\n        \n        daily = []\n        for day_data in data:\n            date = day_data.get(\"date\", \"\")\n            stats_list = day_data.get(\"stats\", [])\n            \n            if stats_list:\n                metrics = stats_list[0].get(\"metrics\", {})\n                \n                day_stats = {\n                    \"date\": date,\n                    \"requests\": metrics.get(\"requests\", 0),\n                    \"delivered\": metrics.get(\"delivered\", 0),\n                    \"opens\": metrics.get(\"opens\", 0),\n                    \"unique_opens\": metrics.get(\"unique_opens\", 0),\n                    \"clicks\": metrics.get(\"clicks\", 0),\n                    \"unique_clicks\": metrics.get(\"unique_clicks\", 0),\n                    \"bounces\": metrics.get(\"bounces\", 0),\n                    \"blocks\": metrics.get(\"blocks\", 0),\n                    \"spam_reports\": metrics.get(\"spam_reports\", 0)\n                }\n                daily.append(day_stats)\n                \n                for key in totals:\n                    totals[key] += metrics.get(key, 0)\n        \n        open_rate = 0.0\n        click_rate = 0.0\n        bounce_rate = 0.0\n        \n        if totals[\"delivered\"] > 0:\n            open_rate = round((totals[\"unique_opens\"] / totals[\"delivered\"]) * 100, 1)\n            click_rate = round((totals[\"unique_clicks\"] / totals[\"delivered\"]) * 100, 1)\n        \n        if totals[\"requests\"] > 0:\n            bounce_rate = round((totals[\"bounces\"] / totals[\"requests\"]) * 100, 1)\n        \n        return {\n            \"success\": True,\n            \"error\": None,\n            \"period\": {\n                \"start\": start_date,\n                \"end\": end_date,\n                \"days\": days\n            },\n            \"totals\": totals,\n            \"rates\": {\n                \"open_rate\": open_rate,\n                \"click_rate\": click_rate,\n                \"bounce_rate\": bounce_rate\n            },\n            \"daily\": sorted(daily, key=lambda x: x[\"date\"], reverse=True)\n        }\n        \n    except requests.exceptions.Timeout:\n        return {\n            \"success\": False,\n            \"error\": \"SendGrid API timeout\",\n            \"stats\": None\n        }\n    except Exception as e:\n        return {\n            \"success\": False,\n            \"error\": f\"Error fetching SendGrid stats: {str(e)}\",\n            \"stats\": None\n        }\n","path":null,"size_bytes":33592,"size_tokens":null},"signals_agent.py":{"content":"\"\"\"\nSignals Agent - The Ethical Briefcase System\n\nThis agent monitors external context signals about companies and generates\nactionable LeadEvents for moment-aware outreach. It transforms HossAgent\nfrom generic lead gen into a context-aware intelligence engine.\n\nMiami-tuned heuristics included for South Florida market.\n\n============================================================================\nSIGNALNET INTEGRATION\n============================================================================\nThe Signals Agent now integrates with the SignalNet framework for real signal\nsources (weather, news, Reddit). The SIGNAL_MODE environment variable controls\nthe pipeline behavior:\n\n  PRODUCTION: Run real sources, create LeadEvents for high-scoring signals\n  SANDBOX: Run sources and score signals, but don't create LeadEvents\n  OFF: Skip signal ingestion entirely\n\nDefault: SANDBOX (safe mode for development)\n\nWhen SIGNAL_MODE is OFF, the agent skips the entire SignalNet pipeline.\n\n============================================================================\nMIAMI BIAS CONFIGURATION\n============================================================================\nThe Signals Engine is configured via two key environment variables:\n\n  LEAD_GEOGRAPHY: Target geographic market (e.g., \"Miami, Broward, South Florida\")\n  LEAD_NICHE: Target industry verticals (e.g., \"HVAC, Roofing, Med Spa\")\n\nThese values affect:\n  1. Signal Scoring - Signals matching LEAD_GEOGRAPHY get +15 urgency boost\n  2. LeadEvent Creation - Events from target geography are prioritized\n  3. Category Assignment - Miami-specific categories (HURRICANE_SEASON, \n     MIAMI_PRICE_MOVE, BILINGUAL_OPPORTUNITY) get higher base weights when\n     geography matches South Florida\n\nDefault fallbacks if env vars not set:\n  LEAD_GEOGRAPHY -> \"Miami, Broward, South Florida\"\n  LEAD_NICHE -> \"HVAC, Roofing, Med Spa, Immigration Attorney\"\n============================================================================\n\"\"\"\n\nimport json\nimport random\nimport os\nfrom datetime import datetime\nfrom typing import Optional, Dict, Sequence\nfrom sqlmodel import Session, select\n\nfrom models import (\n    Signal, LeadEvent, Customer, Lead,\n    ENRICHMENT_STATUS_OUTBOUND_SENT,\n    ENRICHMENT_STATUS_ENRICHED_NO_OUTBOUND,\n    ENRICHMENT_STATUS_WITH_DOMAIN_NO_EMAIL,\n    ENRICHMENT_STATUS_UNENRICHED,\n)\nfrom subscription_utils import increment_leads_used\nfrom signal_sources import (\n    run_signal_pipeline,\n    get_signal_status,\n    get_signal_mode,\n    SIGNAL_MODE,\n    LEAD_GEOGRAPHY as SIGNALNET_GEOGRAPHY,\n    LEAD_NICHE as SIGNALNET_NICHE,\n)\n\n\n# ============================================================================\n# Miami-first targeting via LEAD_GEOGRAPHY, LEAD_NICHE\n# These env vars control the geographic and industry bias of the signals engine\n# ============================================================================\nLEAD_GEOGRAPHY = os.environ.get(\"LEAD_GEOGRAPHY\", \"Miami, Broward, South Florida\")\nLEAD_NICHE = os.environ.get(\"LEAD_NICHE\", \"HVAC, Roofing, Med Spa, Immigration Attorney\")\n\n# Parse LEAD_GEOGRAPHY into searchable list for matching\nLEAD_GEOGRAPHY_LIST = [g.strip().lower() for g in LEAD_GEOGRAPHY.split(\",\")]\n\n# Parse LEAD_NICHE into searchable list for industry matching\nLEAD_NICHE_LIST = [n.strip().lower() for n in LEAD_NICHE.split(\",\")]\n\n# Log configuration at module load (startup) - include SignalNet mode\nprint(f\"[SIGNALS][STARTUP] Mode: {SIGNAL_MODE}\")\nprint(f\"[SIGNALS][STARTUP] Geography: {LEAD_GEOGRAPHY}, Niche: {LEAD_NICHE}\")\n\ndef _log_signalnet_sources_status():\n    \"\"\"Log status of SignalNet sources at startup.\"\"\"\n    status = get_signal_status()\n    registry = status.get(\"registry\", {})\n    sources = registry.get(\"sources\", [])\n    \n    enabled_sources = [s[\"name\"] for s in sources if s.get(\"enabled\")]\n    disabled_sources = [s[\"name\"] for s in sources if not s.get(\"enabled\")]\n    \n    if enabled_sources:\n        print(f\"[SIGNALS][STARTUP] Enabled sources: {', '.join(enabled_sources)}\")\n    if disabled_sources:\n        print(f\"[SIGNALS][STARTUP] Disabled sources: {', '.join(disabled_sources)}\")\n\n_log_signalnet_sources_status()\n\n\n# Miami-specific industry verticals - high-value niches for South Florida market\nMIAMI_INDUSTRIES = [\n    \"med spa\", \"hvac\", \"roofing\", \"immigration attorney\", \n    \"realtor\", \"insurance broker\", \"marketing agency\",\n    \"dental practice\", \"auto repair\", \"landscaping\"\n]\n\n# Miami/South Florida geographic areas for signal generation\nMIAMI_AREAS = [\n    \"Miami\", \"Coral Gables\", \"Brickell\", \"Wynwood\", \"Little Havana\",\n    \"Doral\", \"Hialeah\", \"Miami Beach\", \"Fort Lauderdale\", \"Broward County\",\n    \"Hollywood\", \"Pembroke Pines\", \"Aventura\", \"Kendall\", \"Homestead\"\n]\n\n\ndef matches_lead_geography(geography: Optional[str]) -> bool:\n    \"\"\"\n    Check if a geography string matches the configured LEAD_GEOGRAPHY.\n    \n    Miami-first targeting: Returns True if the geography contains any of\n    the target areas specified in LEAD_GEOGRAPHY env var.\n    \n    Args:\n        geography: Geographic area string (e.g., \"Miami\", \"Broward County\")\n    \n    Returns:\n        True if geography matches LEAD_GEOGRAPHY, False otherwise\n    \"\"\"\n    if not geography:\n        return False\n    geo_lower = geography.lower()\n    return any(target in geo_lower for target in LEAD_GEOGRAPHY_LIST)\n\n\ndef matches_lead_niche(niche: Optional[str]) -> bool:\n    \"\"\"\n    Check if a niche string matches the configured LEAD_NICHE.\n    \n    Miami-first targeting: Returns True if the niche contains any of\n    the target industries specified in LEAD_NICHE env var.\n    \n    Args:\n        niche: Industry/niche string (e.g., \"HVAC\", \"roofing contractor\")\n    \n    Returns:\n        True if niche matches LEAD_NICHE, False otherwise\n    \"\"\"\n    if not niche:\n        return False\n    niche_lower = niche.lower()\n    return any(target in niche_lower for target in LEAD_NICHE_LIST)\n\n\ndef infer_category(signal_type: str, context: str) -> str:\n    \"\"\"\n    Infer LeadEvent category from signal content.\n    \n    Miami-tuned categories:\n    - HURRICANE_SEASON: Storm/hurricane-related signals (high priority in South FL)\n    - MIAMI_PRICE_MOVE: Pricing changes in Miami market\n    - BILINGUAL_OPPORTUNITY: Spanish/bilingual signals (critical in Miami market)\n    - COMPETITOR_SHIFT: Competitor positioning changes\n    - GROWTH_SIGNAL: Hiring/expansion signals\n    - REPUTATION_CHANGE: Review-based signals\n    - OPPORTUNITY: General opportunity signals\n    \"\"\"\n    context_lower = context.lower()\n    \n    # Miami-specific high-priority categories\n    if \"hurricane\" in context_lower or \"storm\" in context_lower:\n        return \"HURRICANE_SEASON\"\n    elif \"bilingual\" in context_lower or \"spanish\" in context_lower:\n        return \"BILINGUAL_OPPORTUNITY\"\n    elif \"price\" in context_lower and (\"miami\" in context_lower or \"local\" in context_lower):\n        return \"MIAMI_PRICE_MOVE\"\n    # General categories\n    elif \"competitor\" in context_lower or \"pricing\" in context_lower:\n        return \"COMPETITOR_SHIFT\"\n    elif \"hiring\" in context_lower or \"job\" in context_lower or \"growth\" in context_lower:\n        return \"GROWTH_SIGNAL\"\n    elif \"review\" in context_lower:\n        return \"REPUTATION_CHANGE\"\n    elif \"price\" in context_lower or \"pricing\" in context_lower:\n        return \"MIAMI_PRICE_MOVE\"\n    else:\n        return \"OPPORTUNITY\"\n\n\ndef calculate_urgency(signal_type: str, category: str, geography: Optional[str] = None, niche: Optional[str] = None) -> int:\n    \"\"\"\n    Calculate urgency score 0-100 based on signal characteristics.\n    \n    Miami-first targeting via LEAD_GEOGRAPHY, LEAD_NICHE:\n    - Base scores are set per category (Miami-tuned categories get higher base)\n    - +15 urgency boost if geography matches LEAD_GEOGRAPHY\n    - +10 urgency boost if niche matches LEAD_NICHE\n    \n    Category base weights (Miami-tuned):\n    - HURRICANE_SEASON: 75 (highest - critical for South FL)\n    - REPUTATION_CHANGE: 70\n    - COMPETITOR_SHIFT: 65\n    - GROWTH_SIGNAL: 60\n    - MIAMI_PRICE_MOVE: 60\n    - BILINGUAL_OPPORTUNITY: 55\n    - OPPORTUNITY: 50 (default)\n    \n    Args:\n        signal_type: Type of signal source\n        category: Inferred category from signal content\n        geography: Optional geography for boost calculation\n        niche: Optional niche for boost calculation\n    \n    Returns:\n        Urgency score 0-100 (clamped to 30-95 range)\n    \"\"\"\n    # Base scores - Miami-tuned categories get higher weights\n    base_score = 50\n    \n    if category == \"HURRICANE_SEASON\":\n        base_score = 75  # Highest priority - critical for South Florida\n    elif category == \"REPUTATION_CHANGE\":\n        base_score = 70\n    elif category == \"COMPETITOR_SHIFT\":\n        base_score = 65\n    elif category == \"GROWTH_SIGNAL\":\n        base_score = 60\n    elif category == \"MIAMI_PRICE_MOVE\":\n        base_score = 60\n    elif category == \"BILINGUAL_OPPORTUNITY\":\n        base_score = 55\n    \n    # Miami-first targeting: Boost signals from LEAD_GEOGRAPHY\n    geography_boost = 0\n    if geography and matches_lead_geography(geography):\n        geography_boost = 15\n    \n    # Boost signals matching LEAD_NICHE industries\n    niche_boost = 0\n    if niche and matches_lead_niche(niche):\n        niche_boost = 10\n    \n    # Add random variation for natural distribution\n    variation = random.randint(-10, 10)\n    \n    final_score = base_score + geography_boost + niche_boost + variation\n    \n    return max(30, min(95, final_score))\n\n\ndef generate_recommended_action(category: str, signal_summary: str) -> str:\n    \"\"\"\n    Generate recommended action based on category.\n    \n    Miami-first targeting: Actions are tuned for South Florida market context.\n    Categories like HURRICANE_SEASON, BILINGUAL_OPPORTUNITY, and MIAMI_PRICE_MOVE\n    have Miami-specific recommended actions.\n    \"\"\"\n    actions = {\n        \"HURRICANE_SEASON\": \"Offer hurricane-season discount bundle or preparedness package\",\n        \"COMPETITOR_SHIFT\": \"Send competitive analysis snapshot highlighting your differentiators\",\n        \"GROWTH_SIGNAL\": \"Propose partnership or capacity-building services\",\n        \"BILINGUAL_OPPORTUNITY\": \"Highlight bilingual staff on homepage - big ROI in Miami market\",\n        \"REPUTATION_CHANGE\": \"Offer reputation management or customer experience audit\",\n        \"MIAMI_PRICE_MOVE\": \"Prepare market pricing comparison and value proposition\",\n        \"OPPORTUNITY\": \"Send contextual outreach with relevant service offer\"\n    }\n    return actions.get(category, \"Prepare contextual outreach based on signal\")\n\n\ndef run_signals_agent(session: Session, max_signals: int = 10) -> Dict:\n    \"\"\"\n    Run the Signals Agent with SignalNet integration.\n    \n    Pipeline behavior is controlled by SIGNAL_MODE environment variable:\n    \n      OFF: Skip SignalNet entirely, log and return immediately\n      SANDBOX: Run SignalNet sources, score signals, but don't create LeadEvents\n      PRODUCTION: Full pipeline including LeadEvent creation for high-scoring signals\n    \n    Miami-first targeting via LEAD_GEOGRAPHY, LEAD_NICHE:\n    - Signals are scored with Miami-area geography boost (+15)\n    - Urgency scores are boosted for signals matching LEAD_NICHE (+10)\n    - Miami-tuned categories (HURRICANE_SEASON, etc.) get higher base urgency\n    \n    Returns dict with counts of signals and events generated, plus source details.\n    \"\"\"\n    print(f\"[SIGNALS] ============================================================\")\n    print(f\"[SIGNALS] Starting Signals Agent cycle - Mode: {SIGNAL_MODE}\")\n    print(f\"[SIGNALS] Geography: {LEAD_GEOGRAPHY}\")\n    print(f\"[SIGNALS] Niche: {LEAD_NICHE}\")\n    print(f\"[SIGNALS] ============================================================\")\n    \n    if SIGNAL_MODE == \"OFF\":\n        print(\"[SIGNALS] SIGNAL_MODE is OFF - skipping SignalNet pipeline entirely\")\n        return {\n            \"signals_created\": 0,\n            \"events_created\": 0,\n            \"mode\": \"OFF\",\n            \"skipped\": True,\n            \"message\": \"SignalNet is disabled (SIGNAL_MODE=OFF)\"\n        }\n    \n    print(f\"[SIGNALS] Running SignalNet pipeline in {SIGNAL_MODE} mode...\")\n    \n    pipeline_result = run_signal_pipeline(session)\n    \n    signals_from_pipeline = pipeline_result.get(\"signals_persisted\", 0)\n    events_from_pipeline = pipeline_result.get(\"events_created\", 0)\n    sources_run = pipeline_result.get(\"sources_run\", [])\n    errors = pipeline_result.get(\"errors\", [])\n    \n    print(f\"[SIGNALS] SignalNet pipeline results:\")\n    print(f\"[SIGNALS]   - Sources checked: {pipeline_result.get('sources_checked', 0)}\")\n    print(f\"[SIGNALS]   - Sources eligible: {pipeline_result.get('sources_eligible', 0)}\")\n    print(f\"[SIGNALS]   - Signals fetched: {pipeline_result.get('signals_fetched', 0)}\")\n    print(f\"[SIGNALS]   - Signals persisted: {signals_from_pipeline}\")\n    print(f\"[SIGNALS]   - Events created: {events_from_pipeline}\")\n    \n    for source_result in sources_run:\n        source_name = source_result.get(\"source\", \"unknown\")\n        fetched = source_result.get(\"fetched\", 0)\n        persisted = source_result.get(\"persisted\", 0)\n        events = source_result.get(\"events_created\", 0)\n        error = source_result.get(\"error\")\n        \n        if error:\n            print(f\"[SIGNALS][{source_name.upper()}] ERROR: {error}\")\n        else:\n            print(f\"[SIGNALS][{source_name.upper()}] Fetched: {fetched}, Persisted: {persisted}, Events: {events}\")\n    \n    if errors:\n        print(f\"[SIGNALS] Pipeline errors:\")\n        for err in errors:\n            print(f\"[SIGNALS]   - {err.get('source')}: {err.get('error')}\")\n    \n    print(f\"[SIGNALS] Cycle complete. Mode: {SIGNAL_MODE}, Signals: {signals_from_pipeline}, Events: {events_from_pipeline}\")\n    \n    return {\n        \"signals_created\": signals_from_pipeline,\n        \"events_created\": events_from_pipeline,\n        \"mode\": SIGNAL_MODE,\n        \"source\": \"signalnet\",\n        \"signalnet_result\": pipeline_result,\n        \"message\": f\"SignalNet pipeline completed in {SIGNAL_MODE} mode\"\n    }\n\n\ndef get_todays_opportunities(\n    session: Session, \n    company_id: Optional[int] = None, \n    limit: int = 10,\n    enrichment_status: Optional[str] = None,\n    include_review_mode: bool = False\n) -> Sequence[LeadEvent]:\n    \"\"\"\n    Get today's opportunities (LeadEvents) for display in customer portal.\n    \n    By default, only shows OUTBOUND_SENT events (emails that have been sent).\n    If include_review_mode=True, also includes ENRICHED_NO_OUTBOUND (pending review).\n    \n    Sorted by urgency_score desc, then by created_at desc.\n    \"\"\"\n    query = select(LeadEvent).order_by(\n        LeadEvent.urgency_score.desc(),\n        LeadEvent.created_at.desc()\n    ).limit(limit)\n    \n    if company_id:\n        query = query.where(LeadEvent.company_id == company_id)\n    \n    if enrichment_status:\n        query = query.where(LeadEvent.enrichment_status == enrichment_status)\n    elif include_review_mode:\n        query = query.where(LeadEvent.enrichment_status.in_([\n            ENRICHMENT_STATUS_OUTBOUND_SENT,\n            ENRICHMENT_STATUS_ENRICHED_NO_OUTBOUND\n        ]))\n    else:\n        query = query.where(LeadEvent.enrichment_status == ENRICHMENT_STATUS_OUTBOUND_SENT)\n    \n    return session.exec(query).all()\n\n\ndef get_lead_events_by_enrichment_status(\n    session: Session, \n    enrichment_status: str, \n    limit: int = 50\n) -> Sequence[LeadEvent]:\n    \"\"\"\n    Get LeadEvents filtered by enrichment status for admin console.\n    \n    Enrichment Status Flow:\n    - UNENRICHED: Raw signals, no domain/email yet\n    - WITH_DOMAIN_NO_EMAIL: Domain discovered, awaiting email scraping\n    - ENRICHED_NO_OUTBOUND: Ready to send (email found)\n    - OUTBOUND_SENT: Email sent\n    \"\"\"\n    return session.exec(\n        select(LeadEvent)\n        .where(LeadEvent.enrichment_status == enrichment_status)\n        .order_by(LeadEvent.created_at.desc())\n        .limit(limit)\n    ).all()\n\n\ndef get_lead_events_counts_by_status(session: Session) -> Dict[str, int]:\n    \"\"\"Get counts of LeadEvents by enrichment status for admin dashboard.\"\"\"\n    statuses = [\n        ENRICHMENT_STATUS_UNENRICHED,\n        ENRICHMENT_STATUS_WITH_DOMAIN_NO_EMAIL,\n        ENRICHMENT_STATUS_ENRICHED_NO_OUTBOUND,\n        ENRICHMENT_STATUS_OUTBOUND_SENT,\n    ]\n    \n    counts = {}\n    for status in statuses:\n        count = len(session.exec(\n            select(LeadEvent).where(LeadEvent.enrichment_status == status)\n        ).all())\n        counts[status] = count\n    \n    return counts\n\n\ndef get_signals_summary(session: Session, limit: int = 20) -> Sequence[Signal]:\n    \"\"\"Get recent signals for admin display.\"\"\"\n    return session.exec(\n        select(Signal).order_by(Signal.created_at.desc()).limit(limit)\n    ).all()\n\n\ndef get_lead_events_summary(session: Session, limit: int = 20) -> Sequence[LeadEvent]:\n    \"\"\"Get recent lead events for admin display.\"\"\"\n    return session.exec(\n        select(LeadEvent).order_by(LeadEvent.created_at.desc()).limit(limit)\n    ).all()\n","path":null,"size_bytes":16982,"size_tokens":null},"lead_enrichment.py":{"content":"\"\"\"\nLead Enrichment Pipeline - Free Tier Services Only\n\nEnriches LeadEvents with contact information using:\n1. Hunter.io - Domain to email (free tier: 25 requests/month)\n2. Clearbit Logo/Company API - Company info (free tier available)\n3. Website scraping - Contact/About/Team pages\n4. Social link extraction - Facebook, Instagram, LinkedIn URLs\n\nPipeline runs asynchronously and respects rate limits.\n\n============================================================================\nENVIRONMENT VARIABLES\n============================================================================\nHUNTER_API_KEY: Optional - Hunter.io API key for email discovery\nCLEARBIT_API_KEY: Optional - Clearbit API key for company enrichment\nENRICHMENT_DRY_RUN: If true, skip actual API calls and log intentions\n\nIf no API keys are set, the pipeline falls back to web scraping only.\n============================================================================\n\"\"\"\n\nimport asyncio\nimport json\nimport os\nimport re\nimport time\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional\nfrom urllib.parse import urljoin, urlparse\n\nimport requests\nfrom sqlmodel import Session, select\n\nfrom models import (\n    LeadEvent,\n    Lead,\n    Signal,\n    Company,\n    EnrichmentMetrics,\n    ENRICHMENT_STATUS_UNENRICHED,\n    ENRICHMENT_STATUS_WITH_DOMAIN_NO_EMAIL,\n    ENRICHMENT_STATUS_WITH_PHONE_ONLY,\n    ENRICHMENT_STATUS_ENRICHED_NO_OUTBOUND,\n    ENRICHMENT_STATUS_OUTBOUND_SENT,\n    ENRICHMENT_STATUS_ARCHIVED,\n    ENRICHMENT_STATUS_ARCHIVED_UNENRICHABLE,\n    ENRICHMENT_STATUS_ENRICHED,\n    ENRICHMENT_STATUS_FAILED,\n    ENRICHMENT_STATUS_SKIPPED,\n    UNENRICHABLE_REASON_NO_DOMAIN,\n    UNENRICHABLE_REASON_NO_CONTACT_INFO,\n    UNENRICHABLE_REASON_NO_OSINT_PRESENCE,\n    UNENRICHABLE_REASON_BLOCKED_DOMAIN,\n    UNENRICHABLE_REASON_INVALID_COMPANY,\n    DEFAULT_MAX_ENRICHMENT_ATTEMPTS,\n)\nfrom domain_discovery import discover_domain_for_lead_event, DomainDiscoveryResult, extract_company_name_from_summary, _is_valid_branded_company\nfrom outbound_utils import send_lead_event_immediate\nfrom phone_extraction import discover_phones, get_domain_from_phone, PhoneDiscoveryResult\nfrom company_name_extraction import extract_company_candidates, get_best_company_name, NameStormResult\nfrom mission_log import MissionLog, log_enrichment_attempt, should_attempt_action\nfrom email_storm import discover_emails, EmailStormResult, EmailCandidate\n\n\nHUNTER_API_KEY = os.environ.get(\"HUNTER_API_KEY\", \"\")\nCLEARBIT_API_KEY = os.environ.get(\"CLEARBIT_API_KEY\", \"\")\nENRICHMENT_DRY_RUN = os.environ.get(\"ENRICHMENT_DRY_RUN\", \"false\").lower() in (\"true\", \"1\", \"yes\")\n\nHUNTER_API_BASE = \"https://api.hunter.io/v2\"\nCLEARBIT_API_BASE = \"https://company.clearbit.com/v2\"\n\nREQUEST_TIMEOUT = 10\nRATE_LIMIT_DELAY = 1.0\nMAX_RETRIES = 2\n\nCONTACT_PAGE_PATHS = [\n    \"/contact\", \"/contact-us\", \"/contact_us\", \"/contactus\", \"/contact-page\",\n    \"/about\", \"/about-us\", \"/about_us\", \"/aboutus\", \"/about-page\",\n    \"/team\", \"/our-team\", \"/our_team\", \"/ourteam\", \"/team-page\", \"/meet-team\",\n    \"/connect\", \"/get-in-touch\", \"/reach-us\", \"/reach-out\", \"/lets-talk\",\n    \"/support\", \"/help\", \"/inquiries\", \"/inquiry\", \"/help-center\",\n    \"/locations\", \"/location\", \"/offices\", \"/office\", \"/service-areas\",\n    \"/staff\", \"/leadership\", \"/management\", \"/people\", \"/careers\",\n    \"/company\", \"/who-we-are\", \"/meet-the-team\", \"/about-company\",\n    \"/business\", \"/services\", \"/contact-information\", \"/footer\",\n    \"/agents\", \"/partners\", \"/testimonials\", \"/newsletter\",\n]\n\nMAILTO_REGEX = re.compile(r'href=[\"\\']mailto:([^\"\\'?]+)', re.IGNORECASE)\n\nCONTACT_LINK_PATTERNS = [\n    r'href=[\"\\']([^\"\\']*contact[^\"\\']*)[\"\\']',\n    r'href=[\"\\']([^\"\\']*about[^\"\\']*)[\"\\']',\n    r'href=[\"\\']([^\"\\']*team[^\"\\']*)[\"\\']',\n    r'href=[\"\\']([^\"\\']*get-in-touch[^\"\\']*)[\"\\']',\n    r'href=[\"\\']([^\"\\']*reach[^\"\\']*)[\"\\']',\n]\n\nEMAIL_REGEX = re.compile(\n    r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\",\n    re.IGNORECASE\n)\n\nEMAIL_PATTERNS = [\n    re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'),\n    re.compile(r'(?:email|mail|contact|reach|hello):\\s*([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})', re.IGNORECASE),\n    re.compile(r'(?:info|contact|hello|support|sales)@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', re.IGNORECASE),\n]\n\nPHONE_REGEX = re.compile(\n    r\"(?:\\+?1[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\",\n    re.IGNORECASE\n)\n\nSOCIAL_PATTERNS = {\n    \"facebook\": re.compile(r\"https?://(?:www\\.)?facebook\\.com/[a-zA-Z0-9._-]+/?\", re.IGNORECASE),\n    \"instagram\": re.compile(r\"https?://(?:www\\.)?instagram\\.com/[a-zA-Z0-9._-]+/?\", re.IGNORECASE),\n    \"linkedin\": re.compile(r\"https?://(?:www\\.)?linkedin\\.com/(?:company|in)/[a-zA-Z0-9._-]+/?\", re.IGNORECASE),\n    \"twitter\": re.compile(r\"https?://(?:www\\.)?(?:twitter\\.com|x\\.com)/[a-zA-Z0-9._-]+/?\", re.IGNORECASE),\n}\n\nBROWSER_HEADERS = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n    \"Accept-Language\": \"en-US,en;q=0.5\",\n    \"Accept-Encoding\": \"gzip, deflate\",\n    \"Connection\": \"keep-alive\",\n    \"Upgrade-Insecure-Requests\": \"1\",\n}\n\n\n_article_body_fetch_cache: Dict[str, tuple] = {}\n_CACHE_TTL_SECONDS = 3600\n\ndef _extract_company_from_article_body(source_url: Optional[str], lead_event_id: Optional[int] = None) -> Optional[str]:\n    \"\"\"\n    ARCHANGEL FALLBACK: Extract company name from article body when summary extraction fails.\n    \n    Fetches the article, extracts the first paragraph, and searches for branded company names.\n    Includes caching to avoid repeated fetches for the same URL.\n    \"\"\"\n    if not source_url:\n        return None\n    \n    if 'news.google.com' in source_url:\n        return None\n    \n    if source_url in _article_body_fetch_cache:\n        cached_result, cached_time = _article_body_fetch_cache[source_url]\n        if time.time() - cached_time < _CACHE_TTL_SECONDS:\n            return cached_result\n        del _article_body_fetch_cache[source_url]\n    \n    try:\n        start_time = time.time()\n        response = requests.get(source_url, headers=BROWSER_HEADERS, timeout=6)\n        fetch_time = time.time() - start_time\n        \n        if response.status_code != 200:\n            print(f\"[ARCHANGEL][ARTICLE_BODY][SKIP] status={response.status_code} url={source_url[:60]}...\")\n            _article_body_fetch_cache[source_url] = (None, time.time())\n            return None\n        \n        html = response.text[:50000]\n        \n        from bs4 import BeautifulSoup\n        soup = BeautifulSoup(html, 'html.parser')\n        \n        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):\n            tag.decompose()\n        \n        paragraphs = soup.find_all('p')[:5]\n        text_content = ' '.join(p.get_text(strip=True) for p in paragraphs)\n        \n        if not text_content:\n            print(f\"[ARCHANGEL][ARTICLE_BODY][SKIP] no_text url={source_url[:60]}...\")\n            _article_body_fetch_cache[source_url] = (None, time.time())\n            return None\n        \n        business_pattern = re.compile(\n            r'([A-Z][a-zA-Z]+(?:\\s+[A-Z]?[a-zA-Z&\\'-]+)*\\s+(?:Air|Roofing|Plumbing|HVAC|Electric|Electrical|Landscaping|Construction|Realty|Properties|Solutions|Services|Partners|Group|Corp|Inc|LLC|Company|Associates|Consulting|Agency|Technologies|Systems|Holdings))',\n            re.IGNORECASE\n        )\n        \n        matches = business_pattern.findall(text_content)\n        for match in matches:\n            if _is_valid_branded_company(match):\n                print(f\"[ARCHANGEL][ARTICLE_BODY][FOUND] company={match} fetch_time={fetch_time:.2f}s\")\n                _article_body_fetch_cache[source_url] = (match, time.time())\n                return match\n        \n        print(f\"[ARCHANGEL][ARTICLE_BODY][NONE] no_branded_match fetch_time={fetch_time:.2f}s\")\n        _article_body_fetch_cache[source_url] = (None, time.time())\n        return None\n        \n    except requests.Timeout:\n        print(f\"[ARCHANGEL][ARTICLE_BODY][TIMEOUT] url={source_url[:60]}...\")\n        _article_body_fetch_cache[source_url] = (None, time.time())\n        return None\n    except requests.RequestException as e:\n        print(f\"[ARCHANGEL][ARTICLE_BODY][ERROR] {str(e)[:50]} url={source_url[:60]}...\")\n        _article_body_fetch_cache[source_url] = (None, time.time())\n        return None\n    except Exception as e:\n        print(f\"[ARCHANGEL][ARTICLE_BODY][ERROR] unexpected: {str(e)[:50]}\")\n        _article_body_fetch_cache[source_url] = (None, time.time())\n        return None\n\n\ndef _get_dry_run_prefix() -> str:\n    \"\"\"Get log prefix for dry run mode.\"\"\"\n    return \"[DRY_RUN]\" if ENRICHMENT_DRY_RUN else \"\"\n\n\ndef log_enrichment(\n    action: str,\n    domain: Optional[str] = None,\n    lead_event_id: Optional[int] = None,\n    source: Optional[str] = None,\n    details: Optional[Dict] = None,\n    error: Optional[str] = None,\n    success: bool = True\n) -> None:\n    \"\"\"\n    Structured logging for enrichment pipeline activity.\n    \n    Args:\n        action: Action being performed (attempt, success, failure, skip, rate_limit)\n        domain: Target domain being enriched\n        lead_event_id: LeadEvent ID if applicable\n        source: Enrichment source (hunter, clearbit, scrape)\n        details: Additional context data\n        error: Error message if any\n        success: Whether the operation succeeded\n    \"\"\"\n    prefix = _get_dry_run_prefix()\n    timestamp = datetime.utcnow().isoformat()\n    \n    log_level = \"ERROR\" if error else (\"INFO\" if success else \"WARN\")\n    domain_part = f\" | Domain: {domain}\" if domain else \"\"\n    event_part = f\" | LeadEvent: {lead_event_id}\" if lead_event_id else \"\"\n    source_part = f\" | Source: {source}\" if source else \"\"\n    details_str = f\" | {json.dumps(details)[:150]}\" if details else \"\"\n    error_part = f\" | Error: {error}\" if error else \"\"\n    \n    print(f\"{prefix}[ENRICHMENT][{action.upper()}]{domain_part}{event_part}{source_part}{details_str}{error_part}\")\n\n\ndef check_enrichment_budget(lead_event: LeadEvent) -> bool:\n    \"\"\"\n    ARCHANGEL v2: Check if lead has exceeded its enrichment budget.\n    \n    Returns:\n        True if lead can still be enriched, False if budget exhausted\n    \"\"\"\n    max_attempts = lead_event.max_enrichment_attempts or DEFAULT_MAX_ENRICHMENT_ATTEMPTS\n    current_attempts = lead_event.enrichment_attempts or 0\n    \n    return current_attempts < max_attempts\n\n\ndef mark_as_unenrichable(\n    lead_event: LeadEvent,\n    reason: str,\n    session: Session,\n    mission_log: Optional[MissionLog] = None\n) -> None:\n    \"\"\"\n    ARCHANGEL v2: Mark a lead as permanently unenrichable.\n    \n    Transitions lead to ARCHIVED_UNENRICHABLE status with reason code.\n    \n    Args:\n        lead_event: LeadEvent to mark\n        reason: One of UNENRICHABLE_REASON_* constants\n        session: Database session\n        mission_log: Optional mission log to persist\n    \"\"\"\n    lead_event.enrichment_status = ENRICHMENT_STATUS_ARCHIVED_UNENRICHABLE\n    lead_event.unenrichable_reason = reason\n    lead_event.last_enrichment_at = datetime.utcnow()\n    \n    if mission_log:\n        mission_log.add_entry(\n            phase=\"ARCHANGEL\",\n            action=\"mark_unenrichable\",\n            result=\"completed\",\n            notes=f\"Reason: {reason}\"\n        )\n        lead_event.enrichment_mission_log = mission_log.to_json()\n    \n    log_enrichment(\"ARCHANGEL_UNENRICHABLE\", lead_event_id=lead_event.id,\n                   details={\"reason\": reason, \n                            \"attempts\": lead_event.enrichment_attempts,\n                            \"max_attempts\": lead_event.max_enrichment_attempts})\n    \n    session.add(lead_event)\n    session.commit()\n\n\ndef get_mission_log(lead_event: LeadEvent) -> MissionLog:\n    \"\"\"\n    ARCHANGEL v2: Get or create mission log for a lead.\n    \n    Args:\n        lead_event: LeadEvent to get mission log for\n        \n    Returns:\n        MissionLog instance (may be empty if new)\n    \"\"\"\n    return MissionLog.from_json(lead_event.enrichment_mission_log)\n\n\ndef save_mission_log(lead_event: LeadEvent, mission_log: MissionLog) -> None:\n    \"\"\"\n    ARCHANGEL v2: Save mission log back to lead event.\n    \n    Args:\n        lead_event: LeadEvent to update\n        mission_log: MissionLog instance to persist\n    \"\"\"\n    lead_event.enrichment_mission_log = mission_log.to_json()\n\n\ndef upsert_company(\n    session: Session,\n    name: str,\n    domain: Optional[str] = None,\n    geography: Optional[str] = None,\n    source_type: Optional[str] = None,\n    source_signal_id: Optional[int] = None,\n    phones: Optional[List[Dict]] = None,\n    emails: Optional[List[Dict]] = None\n) -> Company:\n    \"\"\"\n    ARCHANGEL v2: Upsert company to Company table.\n    \n    Matching strategy:\n    1. If domain provided, match by domain (unique)\n    2. Otherwise, match by normalized_name + geography\n    \n    Args:\n        session: Database session\n        name: Company name\n        domain: Primary domain (optional)\n        geography: Geographic region (optional)\n        source_type: Signal source type\n        source_signal_id: Source signal ID\n        phones: List of phone dicts\n        emails: List of email dicts\n        \n    Returns:\n        Company instance (existing or new)\n    \"\"\"\n    normalized = name.lower().strip()\n    normalized = re.sub(r'\\s+(inc|llc|corp|co|ltd|llp|pllc|pc|pa)\\.?$', '', normalized, flags=re.IGNORECASE)\n    normalized = re.sub(r'[^\\w\\s]', '', normalized)\n    normalized = ' '.join(normalized.split())\n    \n    existing = None\n    \n    if domain:\n        existing = session.exec(\n            select(Company).where(Company.domain == domain)\n        ).first()\n    \n    if not existing:\n        existing = session.exec(\n            select(Company)\n            .where(Company.normalized_name == normalized)\n            .where(Company.geography == geography)\n        ).first()\n    \n    if existing:\n        existing.last_seen_at = datetime.utcnow()\n        if domain and not existing.domain:\n            existing.domain = domain\n        if phones:\n            existing_phones = json.loads(existing.phones) if existing.phones else []\n            existing_phones.extend(phones)\n            existing.phones = json.dumps(existing_phones[-10:])\n        if emails:\n            existing_emails = json.loads(existing.emails) if existing.emails else []\n            existing_emails.extend(emails)\n            existing.emails = json.dumps(existing_emails[-10:])\n        session.add(existing)\n        session.commit()\n        return existing\n    \n    company = Company(\n        name=name,\n        normalized_name=normalized,\n        domain=domain,\n        geography=geography,\n        source_type=source_type,\n        source_signal_id=source_signal_id,\n        phones=json.dumps(phones) if phones else None,\n        emails=json.dumps(emails) if emails else None\n    )\n    session.add(company)\n    session.commit()\n    session.refresh(company)\n    \n    log_enrichment(\"COMPANY_CREATED\", details={\n        \"company_id\": company.id,\n        \"name\": name,\n        \"domain\": domain,\n        \"geography\": geography\n    })\n    \n    return company\n\n\ndef link_lead_to_company(lead_event: LeadEvent, company: Company, session: Session) -> None:\n    \"\"\"\n    ARCHANGEL v2: Link a LeadEvent to its canonical Company.\n    \n    Args:\n        lead_event: LeadEvent to link\n        company: Company to link to\n        session: Database session\n    \"\"\"\n    if lead_event.company_table_id != company.id:\n        lead_event.company_table_id = company.id\n        session.add(lead_event)\n        session.commit()\n        \n        log_enrichment(\"LEAD_LINKED_TO_COMPANY\", lead_event_id=lead_event.id,\n                       details={\"company_id\": company.id, \"company_name\": company.name})\n\n\ndef update_enrichment_metrics(\n    session: Session,\n    source_type: str,\n    enriched: bool = False,\n    domain_discovered: bool = False,\n    email_discovered: bool = False,\n    phone_discovered: bool = False,\n    unenrichable_reason: Optional[str] = None,\n    outbound_sent: bool = False,\n    reply_received: bool = False\n) -> None:\n    \"\"\"\n    ARCHANGEL v2: Update enrichment metrics for a source.\n    \n    Args:\n        session: Database session\n        source_type: Signal source type (news, craigslist, etc.)\n        enriched: Whether enrichment succeeded\n        domain_discovered: Whether domain was found\n        email_discovered: Whether email was found\n        phone_discovered: Whether phone was found\n        unenrichable_reason: Reason if marked unenrichable\n        outbound_sent: Whether outbound was sent\n        reply_received: Whether reply was received\n    \"\"\"\n    today = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)\n    \n    metrics = session.exec(\n        select(EnrichmentMetrics)\n        .where(EnrichmentMetrics.source_type == source_type)\n        .where(EnrichmentMetrics.period_start == today)\n    ).first()\n    \n    if not metrics:\n        metrics = EnrichmentMetrics(\n            source_type=source_type,\n            period_start=today\n        )\n        session.add(metrics)\n    \n    metrics.total_leads += 1\n    if enriched:\n        metrics.enriched_leads += 1\n    if domain_discovered:\n        metrics.domains_discovered += 1\n    if email_discovered:\n        metrics.emails_discovered += 1\n    if phone_discovered:\n        metrics.phones_discovered += 1\n    if outbound_sent:\n        metrics.outbound_sent += 1\n    if reply_received:\n        metrics.replies_received += 1\n    \n    if unenrichable_reason:\n        if unenrichable_reason == UNENRICHABLE_REASON_NO_DOMAIN:\n            metrics.unenrichable_no_domain += 1\n        elif unenrichable_reason == UNENRICHABLE_REASON_NO_CONTACT_INFO:\n            metrics.unenrichable_no_contact += 1\n        elif unenrichable_reason == UNENRICHABLE_REASON_NO_OSINT_PRESENCE:\n            metrics.unenrichable_no_osint += 1\n        elif unenrichable_reason == UNENRICHABLE_REASON_BLOCKED_DOMAIN:\n            metrics.unenrichable_blocked += 1\n        elif unenrichable_reason == UNENRICHABLE_REASON_INVALID_COMPANY:\n            metrics.unenrichable_invalid_company += 1\n    \n    if metrics.total_leads > 0:\n        metrics.enrichment_rate = (metrics.enriched_leads / metrics.total_leads) * 100\n    if metrics.outbound_sent > 0:\n        metrics.reply_rate = (metrics.replies_received / metrics.outbound_sent) * 100\n    \n    metrics.last_updated_at = datetime.utcnow()\n    session.add(metrics)\n    session.commit()\n\n\n@dataclass\nclass EnrichmentResult:\n    \"\"\"Result of an enrichment attempt - ARCHANGEL Enhanced.\"\"\"\n    success: bool\n    source: str  # 'hunter', 'clearbit', 'scrape', 'none'\n    email: Optional[str] = None\n    phone: Optional[str] = None\n    contact_name: Optional[str] = None\n    company_name: Optional[str] = None\n    social_links: dict = field(default_factory=dict)\n    error: Optional[str] = None\n    email_confidence: float = 0.0  # ARCHANGEL: email confidence score 0-1.0\n\n\ndef extract_domain_from_url(url: str) -> Optional[str]:\n    \"\"\"\n    Extract clean domain from URL.\n    \n    Args:\n        url: Full URL or partial domain\n        \n    Returns:\n        Clean domain without protocol/path, or None if invalid\n    \"\"\"\n    if not url:\n        return None\n    \n    url = url.strip().lower()\n    if not url.startswith((\"http://\", \"https://\")):\n        url = \"https://\" + url\n    \n    try:\n        parsed = urlparse(url)\n        domain = parsed.netloc or parsed.path.split(\"/\")[0]\n        domain = domain.replace(\"www.\", \"\")\n        if \".\" in domain and len(domain) > 3:\n            return domain\n    except Exception:\n        pass\n    \n    return None\n\n\ndef try_hunter_enrichment(domain: str) -> Optional[dict]:\n    \"\"\"\n    Use Hunter.io domain search API (free tier) to find emails.\n    \n    Free tier: 25 searches/month\n    API: https://api.hunter.io/v2/domain-search\n    \n    Args:\n        domain: Target domain to search\n        \n    Returns:\n        Dict with emails and contact info, or None if failed/unavailable\n    \"\"\"\n    if not HUNTER_API_KEY:\n        log_enrichment(\"skip\", domain=domain, source=\"hunter\", \n                       details={\"reason\": \"HUNTER_API_KEY not set\"})\n        return None\n    \n    if ENRICHMENT_DRY_RUN:\n        log_enrichment(\"dry_run\", domain=domain, source=\"hunter\",\n                       details={\"would_call\": f\"{HUNTER_API_BASE}/domain-search\"})\n        return {\n            \"email\": f\"contact@{domain}\",\n            \"contact_name\": \"Mock Contact\",\n            \"source\": \"hunter_mock\"\n        }\n    \n    try:\n        log_enrichment(\"attempt\", domain=domain, source=\"hunter\")\n        \n        response = requests.get(\n            f\"{HUNTER_API_BASE}/domain-search\",\n            params={\n                \"domain\": domain,\n                \"api_key\": HUNTER_API_KEY,\n                \"limit\": 5\n            },\n            timeout=REQUEST_TIMEOUT\n        )\n        \n        if response.status_code == 429:\n            log_enrichment(\"rate_limit\", domain=domain, source=\"hunter\",\n                           error=\"Rate limit exceeded\")\n            return None\n        \n        if response.status_code == 402:\n            log_enrichment(\"quota_exceeded\", domain=domain, source=\"hunter\",\n                           error=\"Monthly quota exceeded (free tier: 25/month)\")\n            return None\n        \n        if response.status_code != 200:\n            log_enrichment(\"failure\", domain=domain, source=\"hunter\",\n                           error=f\"HTTP {response.status_code}\")\n            return None\n        \n        data = response.json()\n        \n        if not data.get(\"data\"):\n            log_enrichment(\"no_results\", domain=domain, source=\"hunter\")\n            return None\n        \n        result_data = data[\"data\"]\n        emails = result_data.get(\"emails\", [])\n        \n        if not emails:\n            log_enrichment(\"no_emails\", domain=domain, source=\"hunter\")\n            return None\n        \n        best_email = emails[0]\n        \n        result = {\n            \"email\": best_email.get(\"value\"),\n            \"contact_name\": None,\n            \"company_name\": result_data.get(\"organization\"),\n            \"source\": \"hunter\"\n        }\n        \n        first_name = best_email.get(\"first_name\")\n        last_name = best_email.get(\"last_name\")\n        if first_name or last_name:\n            result[\"contact_name\"] = f\"{first_name or ''} {last_name or ''}\".strip()\n        \n        social_links = {}\n        if result_data.get(\"facebook\"):\n            social_links[\"facebook\"] = result_data[\"facebook\"]\n        if result_data.get(\"twitter\"):\n            social_links[\"twitter\"] = result_data[\"twitter\"]\n        if result_data.get(\"linkedin\"):\n            social_links[\"linkedin\"] = result_data[\"linkedin\"]\n        if result_data.get(\"instagram\"):\n            social_links[\"instagram\"] = result_data[\"instagram\"]\n        \n        if social_links:\n            result[\"social_links\"] = social_links\n        \n        log_enrichment(\"success\", domain=domain, source=\"hunter\",\n                       details={\"email_found\": result[\"email\"]})\n        \n        return result\n        \n    except requests.Timeout:\n        log_enrichment(\"timeout\", domain=domain, source=\"hunter\",\n                       error=\"Request timed out\")\n        return None\n    except requests.RequestException as e:\n        log_enrichment(\"error\", domain=domain, source=\"hunter\",\n                       error=str(e))\n        return None\n    except Exception as e:\n        log_enrichment(\"error\", domain=domain, source=\"hunter\",\n                       error=f\"Unexpected: {str(e)}\")\n        return None\n\n\ndef try_clearbit_enrichment(domain: str) -> Optional[dict]:\n    \"\"\"\n    Use Clearbit company lookup for company info and social links.\n    \n    Free tier available with limited requests.\n    API: https://company.clearbit.com/v2/companies/find\n    \n    Args:\n        domain: Target domain to lookup\n        \n    Returns:\n        Dict with company info and social links, or None if failed/unavailable\n    \"\"\"\n    if not CLEARBIT_API_KEY:\n        log_enrichment(\"skip\", domain=domain, source=\"clearbit\",\n                       details={\"reason\": \"CLEARBIT_API_KEY not set\"})\n        return None\n    \n    if ENRICHMENT_DRY_RUN:\n        log_enrichment(\"dry_run\", domain=domain, source=\"clearbit\",\n                       details={\"would_call\": f\"{CLEARBIT_API_BASE}/companies/find\"})\n        return {\n            \"company_name\": f\"Mock Company ({domain})\",\n            \"description\": \"Mock company description\",\n            \"social_links\": {\n                \"linkedin\": f\"https://linkedin.com/company/{domain.split('.')[0]}\",\n                \"twitter\": f\"https://twitter.com/{domain.split('.')[0]}\"\n            },\n            \"source\": \"clearbit_mock\"\n        }\n    \n    try:\n        log_enrichment(\"attempt\", domain=domain, source=\"clearbit\")\n        \n        response = requests.get(\n            f\"{CLEARBIT_API_BASE}/companies/find\",\n            params={\"domain\": domain},\n            headers={\"Authorization\": f\"Bearer {CLEARBIT_API_KEY}\"},\n            timeout=REQUEST_TIMEOUT\n        )\n        \n        if response.status_code == 429:\n            log_enrichment(\"rate_limit\", domain=domain, source=\"clearbit\",\n                           error=\"Rate limit exceeded\")\n            return None\n        \n        if response.status_code == 402:\n            log_enrichment(\"quota_exceeded\", domain=domain, source=\"clearbit\",\n                           error=\"Quota exceeded\")\n            return None\n        \n        if response.status_code == 404:\n            log_enrichment(\"not_found\", domain=domain, source=\"clearbit\")\n            return None\n        \n        if response.status_code != 200:\n            log_enrichment(\"failure\", domain=domain, source=\"clearbit\",\n                           error=f\"HTTP {response.status_code}\")\n            return None\n        \n        data = response.json()\n        \n        if not data:\n            log_enrichment(\"no_data\", domain=domain, source=\"clearbit\")\n            return None\n        \n        result = {\n            \"company_name\": data.get(\"name\"),\n            \"description\": data.get(\"description\"),\n            \"source\": \"clearbit\"\n        }\n        \n        social_links = {}\n        if data.get(\"facebook\", {}).get(\"handle\"):\n            social_links[\"facebook\"] = f\"https://facebook.com/{data['facebook']['handle']}\"\n        if data.get(\"twitter\", {}).get(\"handle\"):\n            social_links[\"twitter\"] = f\"https://twitter.com/{data['twitter']['handle']}\"\n        if data.get(\"linkedin\", {}).get(\"handle\"):\n            social_links[\"linkedin\"] = f\"https://linkedin.com/company/{data['linkedin']['handle']}\"\n        \n        if social_links:\n            result[\"social_links\"] = social_links\n        \n        log_enrichment(\"success\", domain=domain, source=\"clearbit\",\n                       details={\"company\": result.get(\"company_name\")})\n        \n        return result\n        \n    except requests.Timeout:\n        log_enrichment(\"timeout\", domain=domain, source=\"clearbit\",\n                       error=\"Request timed out\")\n        return None\n    except requests.RequestException as e:\n        log_enrichment(\"error\", domain=domain, source=\"clearbit\",\n                       error=str(e))\n        return None\n    except Exception as e:\n        log_enrichment(\"error\", domain=domain, source=\"clearbit\",\n                       error=f\"Unexpected: {str(e)}\")\n        return None\n\n\ndef extract_social_links(html: str) -> dict:\n    \"\"\"\n    Extract social media profile URLs from HTML content.\n    \n    Finds Facebook, Instagram, LinkedIn, and Twitter/X URLs.\n    \n    Args:\n        html: Raw HTML content to search\n        \n    Returns:\n        Dict of social platform -> URL mappings\n    \"\"\"\n    if not html:\n        return {}\n    \n    social_links = {}\n    \n    for platform, pattern in SOCIAL_PATTERNS.items():\n        matches = pattern.findall(html)\n        if matches:\n            url = matches[0].rstrip(\"/\")\n            if not any(skip in url.lower() for skip in [\"share\", \"sharer\", \"intent\", \"dialog\"]):\n                social_links[platform] = url\n    \n    return social_links\n\n\ndef _extract_emails_from_html(html: str, domain: str) -> List[str]:\n    \"\"\"\n    Extract email addresses from HTML using multiple patterns, prioritizing domain matches.\n    \n    Args:\n        html: Raw HTML content\n        domain: Target domain for prioritization\n        \n    Returns:\n        List of unique email addresses\n    \"\"\"\n    if not html:\n        return []\n    \n    emails = set()\n    \n    for pattern in EMAIL_PATTERNS:\n        matches = pattern.findall(html)\n        for match in matches:\n            if isinstance(match, tuple):\n                match = match[1] if len(match) > 1 else match[0]\n            emails.add(match)\n    \n    emails = list(emails)\n    \n    skip_patterns = [\"example.com\", \"domain.com\", \"email.com\", \"yoursite.com\", \n                     \"placeholder\", \"test@\", \"noreply\", \"no-reply\", \".png\", \".jpg\", \".gif\",\n                     \"facebook.com\", \"instagram.com\", \"twitter.com\", \"linkedin.com\"]\n    emails = [e for e in emails if not any(skip in e.lower() for skip in skip_patterns) and \"@\" in e]\n    \n    domain_emails = [e for e in emails if domain in e.lower()]\n    other_emails = [e for e in emails if domain not in e.lower()]\n    \n    return domain_emails + other_emails\n\n\ndef _extract_phones_from_html(html: str) -> List[str]:\n    \"\"\"\n    Extract phone numbers from HTML content.\n    \n    Args:\n        html: Raw HTML content\n        \n    Returns:\n        List of unique phone numbers\n    \"\"\"\n    if not html:\n        return []\n    \n    phones = PHONE_REGEX.findall(html)\n    \n    phones = list(set(phones))\n    \n    phones = [p for p in phones if len(re.sub(r'\\D', '', p)) >= 10]\n    \n    return phones[:5]\n\n\ndef _fetch_page(url: str, timeout: int = REQUEST_TIMEOUT) -> Optional[str]:\n    \"\"\"Fetch a page with browser headers, return HTML or None.\"\"\"\n    try:\n        response = requests.get(\n            url,\n            headers=BROWSER_HEADERS,\n            timeout=timeout,\n            allow_redirects=True\n        )\n        if response.status_code == 200:\n            return response.text\n    except Exception:\n        pass\n    return None\n\n\ndef _extract_mailto_emails(html: str) -> List[str]:\n    \"\"\"Extract emails from mailto: links - these are high confidence.\"\"\"\n    if not html:\n        return []\n    matches = MAILTO_REGEX.findall(html)\n    return list(set(matches))\n\n\ndef _guess_domain_emails(domain: str) -> List[str]:\n    \"\"\"\n    Generate common domain-based email guesses.\n    \n    Common patterns: info@, contact@, hello@, support@, sales@, admin@, etc.\n    \n    Args:\n        domain: Target domain\n        \n    Returns:\n        List of guessed email addresses\n    \"\"\"\n    domain_clean = domain.replace(\"www.\", \"\")\n    prefixes = [\n        \"info\", \"contact\", \"hello\", \"support\", \"sales\", \"admin\",\n        \"office\", \"team\", \"inquiry\", \"inquiries\", \"business\", \"hr\",\n        \"marketing\", \"partnerships\", \"press\", \"feedback\", \"hello\"\n    ]\n    \n    guessed = []\n    for prefix in prefixes:\n        guessed.append(f\"{prefix}@{domain_clean}\")\n    \n    return guessed\n\n\ndef _discover_contact_links(html: str, base_url: str) -> List[str]:\n    \"\"\"Find links on a page that look like they lead to contact info.\"\"\"\n    discovered = set()\n    for pattern in CONTACT_LINK_PATTERNS:\n        matches = re.findall(pattern, html, re.IGNORECASE)\n        for match in matches:\n            if match.startswith(\"http\"):\n                discovered.add(match)\n            elif match.startswith(\"/\"):\n                discovered.add(urljoin(base_url, match))\n            elif not match.startswith(\"#\") and not match.startswith(\"javascript\"):\n                discovered.add(urljoin(base_url, \"/\" + match))\n    return list(discovered)[:10]\n\n\ndef scrape_contact_page(domain: str) -> Optional[dict]:\n    \"\"\"\n    AGGRESSIVE web scraper to find contact info from company websites.\n    \n    Strategy (tenacious multi-phase approach):\n    1. Fetch homepage - extract mailto: links (highest confidence)\n    2. Try common contact page paths (expanded list)\n    3. Discover contact links from homepage and follow them\n    4. Extract from footer sections\n    5. Try both www and non-www versions\n    6. Parse any emails found in page body\n    \n    Args:\n        domain: Target domain to scrape\n        \n    Returns:\n        Dict with extracted contact info, or None if nothing found\n    \"\"\"\n    if ENRICHMENT_DRY_RUN:\n        log_enrichment(\"dry_run\", domain=domain, source=\"scrape\",\n                       details={\"strategy\": \"aggressive_multi_phase\"})\n        return {\n            \"email\": f\"info@{domain}\",\n            \"phone\": \"(555) 123-4567\",\n            \"social_links\": {\"facebook\": f\"https://facebook.com/{domain.split('.')[0]}\"},\n            \"source\": \"scrape_mock\"\n        }\n    \n    log_enrichment(\"attempt\", domain=domain, source=\"scrape\",\n                   details={\"strategy\": \"aggressive\"})\n    \n    all_emails = []\n    all_phones = []\n    all_social = {}\n    pages_tried = 0\n    pages_success = 0\n    discovered_links = []\n    \n    base_urls = [f\"https://{domain}\", f\"https://www.{domain}\"]\n    if domain.startswith(\"www.\"):\n        base_urls = [f\"https://{domain}\", f\"https://{domain[4:]}\"]\n    \n    homepage_html = None\n    working_base = None\n    \n    for base_url in base_urls:\n        html = _fetch_page(base_url)\n        pages_tried += 1\n        if html:\n            homepage_html = html\n            working_base = base_url\n            pages_success += 1\n            \n            mailto_emails = _extract_mailto_emails(html)\n            if mailto_emails:\n                all_emails.extend(mailto_emails)\n                log_enrichment(\"mailto_found\", domain=domain, source=\"scrape\",\n                               details={\"count\": len(mailto_emails), \"source\": \"homepage\"})\n            \n            discovered_links = _discover_contact_links(html, base_url)\n            \n            social = extract_social_links(html)\n            all_social.update(social)\n            \n            emails = _extract_emails_from_html(html, domain)\n            all_emails.extend(emails)\n            \n            phones = _extract_phones_from_html(html)\n            all_phones.extend(phones)\n            \n            break\n        time.sleep(0.3)\n    \n    if not working_base:\n        working_base = base_urls[0]\n    \n    if not all_emails:\n        for path in CONTACT_PAGE_PATHS:\n            url = urljoin(working_base, path)\n            pages_tried += 1\n            \n            html = _fetch_page(url)\n            if html:\n                pages_success += 1\n                \n                mailto_emails = _extract_mailto_emails(html)\n                all_emails.extend(mailto_emails)\n                \n                emails = _extract_emails_from_html(html, domain)\n                all_emails.extend(emails)\n                \n                phones = _extract_phones_from_html(html)\n                all_phones.extend(phones)\n                \n                social = extract_social_links(html)\n                all_social.update(social)\n                \n                if all_emails:\n                    log_enrichment(\"found_on_path\", domain=domain, source=\"scrape\",\n                                   details={\"path\": path, \"emails\": len(all_emails)})\n                    break\n            \n            time.sleep(0.3)\n    \n    if not all_emails and discovered_links:\n        log_enrichment(\"following_discovered\", domain=domain, source=\"scrape\",\n                       details={\"links_count\": len(discovered_links)})\n        \n        for link_url in discovered_links[:5]:\n            pages_tried += 1\n            html = _fetch_page(link_url)\n            if html:\n                pages_success += 1\n                \n                mailto_emails = _extract_mailto_emails(html)\n                all_emails.extend(mailto_emails)\n                \n                emails = _extract_emails_from_html(html, domain)\n                all_emails.extend(emails)\n                \n                phones = _extract_phones_from_html(html)\n                all_phones.extend(phones)\n                \n                if all_emails:\n                    log_enrichment(\"found_via_discovery\", domain=domain, source=\"scrape\",\n                                   details={\"url\": link_url[:50]})\n                    break\n            \n            time.sleep(0.3)\n    \n    all_emails = list(dict.fromkeys(all_emails))\n    \n    skip_patterns = [\n        \"example.com\", \"domain.com\", \"email.com\", \"yoursite.com\",\n        \"placeholder\", \"test@\", \"noreply\", \"no-reply\", \n        \".png\", \".jpg\", \".gif\", \".svg\", \".webp\",\n        \"wixpress.com\", \"sentry.io\", \"cloudflare\", \"google.com\",\n        \"facebook.com\", \"twitter.com\", \"schema.org\"\n    ]\n    all_emails = [e for e in all_emails if not any(skip in e.lower() for skip in skip_patterns)]\n    \n    domain_root = domain.replace(\"www.\", \"\").split(\".\")[0].lower()\n    domain_emails = [e for e in all_emails if domain_root in e.lower() or domain in e.lower()]\n    generic_good = [e for e in all_emails if any(p in e.lower() for p in [\"info@\", \"contact@\", \"hello@\", \"sales@\", \"support@\", \"admin@\", \"office@\", \"team@\", \"mail@\", \"enquiries@\", \"inquiries@\"])]\n    other_emails = [e for e in all_emails if e not in domain_emails and e not in generic_good]\n    \n    all_emails = domain_emails + generic_good + other_emails\n    all_emails = all_emails[:5]\n    \n    all_phones = list(dict.fromkeys(all_phones))[:3]\n    \n    if not all_emails:\n        guessed_emails = _guess_domain_emails(domain)\n        if guessed_emails:\n            log_enrichment(\"guessed_emails\", domain=domain, source=\"scrape\",\n                           details={\"guessed\": guessed_emails[:3]})\n            all_emails = guessed_emails[:3]\n    \n    if not all_emails and not all_phones and not all_social:\n        log_enrichment(\"no_data\", domain=domain, source=\"scrape\",\n                       details={\"pages_tried\": pages_tried, \"pages_success\": pages_success,\n                                \"discovered_links\": len(discovered_links)})\n        return None\n    \n    result: Dict[str, Any] = {\"source\": \"scrape\"}\n    \n    if all_emails:\n        result[\"email\"] = all_emails[0]\n        if len(all_emails) > 1:\n            result[\"additional_emails\"] = all_emails[1:]\n    \n    if all_phones:\n        result[\"phone\"] = all_phones[0]\n        if len(all_phones) > 1:\n            result[\"additional_phones\"] = all_phones[1:]\n    \n    if all_social:\n        result[\"social_links\"] = all_social\n    \n    log_enrichment(\"success\", domain=domain, source=\"scrape\",\n                   details={\n                       \"emails_found\": len(all_emails),\n                       \"phones_found\": len(all_phones),\n                       \"social_found\": len(all_social),\n                       \"pages_tried\": pages_tried,\n                       \"pages_success\": pages_success\n                   })\n    \n    return result\n\n\nUSELESS_DOMAINS = [\n    \"news.google.com\", \"google.com\", \"reddit.com\", \"facebook.com\",\n    \"twitter.com\", \"x.com\", \"linkedin.com\", \"instagram.com\",\n    \"youtube.com\", \"tiktok.com\", \"yelp.com\", \"bbb.org\",\n    \"bizjournals.com\", \"prnewswire.com\", \"businesswire.com\",\n    \"globenewswire.com\", \"reuters.com\", \"bloomberg.com\",\n    \"yahoo.com\", \"msn.com\", \"cnn.com\", \"foxnews.com\",\n    \"local10.com\", \"wsvn.com\", \"nbcmiami.com\", \"cbsmiami.com\",\n    \"miamiherald.com\", \"sun-sentinel.com\", \"palmbeachpost.com\",\n]\n\n\ndef _extract_company_domain_from_name(company_name: str) -> Optional[str]:\n    \"\"\"Try to guess a domain from company name.\"\"\"\n    if not company_name:\n        return None\n    \n    name = company_name.lower().strip()\n    name = re.sub(r'\\s+(inc|llc|corp|co|ltd|llp|pllc|pc|pa)\\.?$', '', name, flags=re.IGNORECASE)\n    name = re.sub(r'[^a-z0-9\\s]', '', name)\n    name = name.strip()\n    \n    if not name or len(name) < 2:\n        return None\n    \n    slug = name.replace(' ', '')\n    \n    return f\"{slug}.com\"\n\n\ndef _get_domain_for_enrichment(lead_event: LeadEvent, session: Session) -> Optional[str]:\n    \"\"\"\n    Smart domain extraction that avoids news/aggregator sites.\n    \n    Priority:\n    1. lead_domain if it's a real company domain\n    2. Lead.website if linked\n    3. Guess from lead_company name\n    4. Extract from summary text\n    \"\"\"\n    if lead_event.lead_domain:\n        domain = lead_event.lead_domain.lower().replace(\"www.\", \"\")\n        if domain and domain not in USELESS_DOMAINS and not any(u in domain for u in USELESS_DOMAINS):\n            return domain\n    \n    if lead_event.lead_id:\n        lead = session.exec(\n            select(Lead).where(Lead.id == lead_event.lead_id)\n        ).first()\n        if lead and lead.website:\n            domain = extract_domain_from_url(lead.website)\n            if domain and domain not in USELESS_DOMAINS:\n                return domain\n    \n    if lead_event.lead_company:\n        guessed = _extract_company_domain_from_name(lead_event.lead_company)\n        if guessed:\n            return guessed\n    \n    if lead_event.summary:\n        url_match = re.search(r\"https?://(?:www\\.)?([a-zA-Z0-9-]+\\.[a-zA-Z0-9.-]+)\", lead_event.summary)\n        if url_match:\n            domain = url_match.group(1).lower()\n            if domain not in USELESS_DOMAINS:\n                return domain\n    \n    return None\n\n\nasync def enrich_lead_event(lead_event: LeadEvent, session: Session) -> EnrichmentResult:\n    \"\"\"\n    Main entry point for enriching a LeadEvent with contact information.\n    \n    Email-first approach: If lead_email is already set from signal extraction, skip scraping.\n    Otherwise, uses smart domain extraction to find real company domains (not news sites).\n    Then tries enrichment sources in order:\n    1. Web scraping (always available, no API key needed)\n    2. Hunter.io (if API key set)\n    3. Clearbit (if API key set)\n    \n    Args:\n        lead_event: LeadEvent to enrich\n        session: Database session for Lead lookup\n        \n    Returns:\n        EnrichmentResult with status and data\n    \"\"\"\n    log_enrichment(\"start\", lead_event_id=lead_event.id,\n                   details={\"company\": lead_event.lead_company, \"domain\": lead_event.lead_domain, \n                             \"has_email\": bool(lead_event.lead_email)})\n    \n    if lead_event.lead_email:\n        log_enrichment(\"email_first\", lead_event_id=lead_event.id,\n                       details={\"email\": lead_event.lead_email, \"source\": \"signal\"})\n        return EnrichmentResult(\n            success=True,\n            source=\"signal\",\n            email=lead_event.lead_email\n        )\n    \n    domain = _get_domain_for_enrichment(lead_event, session)\n    \n    if not domain:\n        log_enrichment(\"skip\", lead_event_id=lead_event.id,\n                       details={\"reason\": \"No usable domain\", \"lead_domain\": lead_event.lead_domain})\n        return EnrichmentResult(\n            success=False,\n            source=\"none\",\n            error=\"No usable domain available for enrichment\"\n        )\n    \n    result = EnrichmentResult(success=False, source=\"none\")\n    \n    log_enrichment(\"scrape_first\", domain=domain, lead_event_id=lead_event.id,\n                   details={\"strategy\": \"web_scrape_primary\"})\n    \n    scrape_data = scrape_contact_page(domain)\n    \n    if scrape_data:\n        if scrape_data.get(\"email\"):\n            result.email = scrape_data[\"email\"]\n            result.success = True\n            result.source = \"scrape\"\n        \n        if scrape_data.get(\"phone\"):\n            result.phone = scrape_data[\"phone\"]\n        \n        if scrape_data.get(\"social_links\"):\n            result.social_links = scrape_data[\"social_links\"]\n    \n    if not result.email and HUNTER_API_KEY:\n        time.sleep(RATE_LIMIT_DELAY)\n        hunter_data = try_hunter_enrichment(domain)\n        \n        if hunter_data and hunter_data.get(\"email\"):\n            result.success = True\n            result.source = \"hunter\"\n            result.email = hunter_data.get(\"email\")\n            result.contact_name = hunter_data.get(\"contact_name\")\n            result.company_name = hunter_data.get(\"company_name\")\n            if hunter_data.get(\"social_links\"):\n                for platform, url in hunter_data[\"social_links\"].items():\n                    if platform not in result.social_links:\n                        result.social_links[platform] = url\n    \n    if (not result.company_name or not result.social_links) and CLEARBIT_API_KEY:\n        time.sleep(RATE_LIMIT_DELAY)\n        clearbit_data = try_clearbit_enrichment(domain)\n        \n        if clearbit_data:\n            if not result.company_name:\n                result.company_name = clearbit_data.get(\"company_name\")\n            \n            if clearbit_data.get(\"social_links\"):\n                for platform, url in clearbit_data[\"social_links\"].items():\n                    if platform not in result.social_links:\n                        result.social_links[platform] = url\n    \n    if result.success:\n        log_enrichment(\"complete\", domain=domain, lead_event_id=lead_event.id,\n                       source=result.source,\n                       details={\n                           \"has_email\": bool(result.email),\n                           \"has_phone\": bool(result.phone),\n                           \"has_contact\": bool(result.contact_name),\n                           \"social_count\": len(result.social_links)\n                       })\n    else:\n        result.error = \"No contact information found from any source\"\n        log_enrichment(\"failed\", domain=domain, lead_event_id=lead_event.id,\n                       error=result.error)\n    \n    return result\n\n\ndef _apply_phone_enrichment(lead_event: LeadEvent, session: Session) -> bool:\n    \"\"\"\n    PHONESTORM: Apply phone enrichment to LeadEvent.\n    \n    Attempts to discover phone numbers from the lead's domain.\n    Phone data is stored regardless of email status for future use.\n    \n    Args:\n        lead_event: LeadEvent to update\n        session: Database session\n        \n    Returns:\n        True if phone was discovered, False otherwise\n    \"\"\"\n    if not lead_event.lead_domain:\n        return False\n    \n    if lead_event.lead_phone_e164:\n        return True\n    \n    try:\n        phone_result = discover_phones(lead_event.lead_domain)\n        \n        if phone_result.success and phone_result.best_phone:\n            best = phone_result.best_phone\n            lead_event.lead_phone_raw = best.raw_number\n            lead_event.lead_phone_e164 = best.e164_number\n            lead_event.phone_confidence = best.confidence\n            lead_event.phone_source = best.source\n            lead_event.phone_type = best.phone_type\n            \n            log_enrichment(\"PHONESTORM_FOUND\", lead_event_id=lead_event.id,\n                           details={\n                               \"phone\": best.e164_number,\n                               \"confidence\": best.confidence,\n                               \"source\": best.source,\n                               \"phone_type\": best.phone_type\n                           })\n            \n            return True\n        else:\n            log_enrichment(\"PHONESTORM_NOT_FOUND\", lead_event_id=lead_event.id,\n                           details={\"domain\": lead_event.lead_domain})\n            return False\n            \n    except Exception as e:\n        log_enrichment(\"PHONESTORM_ERROR\", lead_event_id=lead_event.id,\n                       error=str(e))\n        return False\n\n\ndef _apply_enrichment_to_lead_event(\n    lead_event: LeadEvent,\n    result: EnrichmentResult,\n    session: Session,\n    domain_discovered: bool = False\n) -> str:\n    \"\"\"\n    Apply enrichment results to LeadEvent and persist to database.\n    \n    Uses new lifecycle states:\n    - UNENRICHED: No domain discovered yet\n    - WITH_DOMAIN_NO_EMAIL: Domain found but no email discovered  \n    - WITH_PHONE_ONLY: Phone found but no email (PHONESTORM)\n    - ENRICHED_NO_OUTBOUND: Email found, awaiting outbound\n    - OUTBOUND_SENT: Outbound email sent (set by BizDev cycle)\n    \n    Args:\n        lead_event: LeadEvent to update\n        result: EnrichmentResult with data\n        session: Database session\n        domain_discovered: True if domain was just discovered this cycle\n        \n    Returns:\n        New enrichment status string\n    \"\"\"\n    lead_event.enrichment_attempts = (lead_event.enrichment_attempts or 0) + 1\n    lead_event.last_enrichment_at = datetime.utcnow()\n    \n    has_phone = _apply_phone_enrichment(lead_event, session)\n    \n    if result.success and result.email:\n        lead_event.enrichment_status = ENRICHMENT_STATUS_ENRICHED_NO_OUTBOUND\n        lead_event.enrichment_source = result.source\n        lead_event.enriched_email = result.email\n        lead_event.enriched_phone = result.phone\n        lead_event.enriched_contact_name = result.contact_name\n        lead_event.enriched_company_name = result.company_name\n        lead_event.enriched_social_links = json.dumps(result.social_links) if result.social_links else None\n        lead_event.enriched_at = datetime.utcnow()\n        \n        # ARCHANGEL: Set email confidence from result\n        lead_event.email_confidence = result.email_confidence if result.email_confidence > 0 else 0.75\n        \n        if not lead_event.lead_email:\n            lead_event.lead_email = result.email\n            log_enrichment(\"ARCHANGEL_EMAIL_SET\", lead_event_id=lead_event.id,\n                           details={\"lead_email\": result.email, \"source\": result.source, \n                                    \"email_confidence\": lead_event.email_confidence})\n        \n        if result.contact_name and not lead_event.lead_name:\n            lead_event.lead_name = result.contact_name\n            \n        log_enrichment(\"ARCHANGEL_STATUS_ENRICHED\", lead_event_id=lead_event.id,\n                       details={\"new_status\": ENRICHMENT_STATUS_ENRICHED_NO_OUTBOUND,\n                                \"email_confidence\": lead_event.email_confidence})\n        \n        session.add(lead_event)\n        session.commit()\n        \n        send_result = send_lead_event_immediate(session, lead_event, commit=True)\n        log_enrichment(\"ARCHANGEL_IMMEDIATE_SEND\", lead_event_id=lead_event.id,\n                       details={\"action\": send_result.action, \"success\": send_result.success,\n                                \"reason\": send_result.reason})\n        \n        return lead_event.enrichment_status\n    \n    elif lead_event.lead_domain:\n        if has_phone and lead_event.lead_phone_e164:\n            lead_event.enrichment_status = ENRICHMENT_STATUS_WITH_PHONE_ONLY\n            lead_event.enrichment_source = result.source if result else \"phonestorm\"\n            log_enrichment(\"PHONESTORM_STATUS_WITH_PHONE\", lead_event_id=lead_event.id,\n                           details={\"new_status\": ENRICHMENT_STATUS_WITH_PHONE_ONLY,\n                                    \"domain\": lead_event.lead_domain,\n                                    \"phone\": lead_event.lead_phone_e164,\n                                    \"phone_confidence\": lead_event.phone_confidence})\n        else:\n            lead_event.enrichment_status = ENRICHMENT_STATUS_WITH_DOMAIN_NO_EMAIL\n            lead_event.enrichment_source = result.source if result else \"none\"\n            log_enrichment(\"ARCHANGEL_STATUS_TRANSITION\", lead_event_id=lead_event.id,\n                           details={\"new_status\": ENRICHMENT_STATUS_WITH_DOMAIN_NO_EMAIL, \n                                    \"domain\": lead_event.lead_domain,\n                                    \"domain_confidence\": lead_event.domain_confidence})\n    \n    else:\n        lead_event.enrichment_status = ENRICHMENT_STATUS_UNENRICHED\n        log_enrichment(\"status_transition\", lead_event_id=lead_event.id,\n                       details={\"new_status\": ENRICHMENT_STATUS_UNENRICHED, \"reason\": \"no_domain\"})\n    \n    session.add(lead_event)\n    session.commit()\n    \n    return lead_event.enrichment_status\n\n\nMAX_ENRICHMENT_PER_CYCLE = int(os.environ.get(\"MAX_ENRICHMENT_PER_CYCLE\", \"25\"))\n\n\ndef _get_source_url_for_event(lead_event: LeadEvent, session: Session) -> Optional[str]:\n    \"\"\"Get source URL from the Signal associated with a LeadEvent.\"\"\"\n    if not lead_event.signal_id:\n        return None\n    \n    signal = session.exec(\n        select(Signal).where(Signal.id == lead_event.signal_id)\n    ).first()\n    \n    if not signal or not signal.raw_payload:\n        return None\n    \n    try:\n        payload = json.loads(signal.raw_payload)\n        return payload.get(\"url\") or payload.get(\"source_url\") or payload.get(\"link\")\n    except (json.JSONDecodeError, TypeError):\n        return None\n\n\nasync def run_enrichment_pipeline(session: Session, max_events: Optional[int] = None) -> dict:\n    \"\"\"\n    Domain-first enrichment pipeline for LeadEvents with PHONESTORM integration.\n    \n    Three-phase pipeline:\n    1. Domain Discovery: For UNENRICHED events, attempt to discover a domain\n    2. Phone Extraction: During enrichment, extract phone numbers (PHONESTORM)\n    3. Email Enrichment: For WITH_DOMAIN_NO_EMAIL/WITH_PHONE_ONLY events, scrape for emails\n    \n    State transitions:\n    - UNENRICHED + domain found  WITH_DOMAIN_NO_EMAIL or WITH_PHONE_ONLY\n    - WITH_DOMAIN_NO_EMAIL/WITH_PHONE_ONLY + email found  ENRICHED_NO_OUTBOUND\n    - ENRICHED_NO_OUTBOUND  OUTBOUND_SENT (by BizDev cycle)\n    \n    PHONESTORM: Phone numbers are extracted alongside email enrichment.\n    WITH_PHONE_ONLY leads have phone but no email - prioritize for retry.\n    \n    Args:\n        session: Database session\n        max_events: Maximum events to process per cycle\n        \n    Returns:\n        Summary dict with enrichment stats\n    \"\"\"\n    if max_events is None:\n        max_events = MAX_ENRICHMENT_PER_CYCLE\n        \n    log_enrichment(\"pipeline_start\", details={\"status\": \"starting\", \"max_events\": max_events})\n    \n    unenriched_events = list(session.exec(\n        select(LeadEvent)\n        .where(LeadEvent.enrichment_status == ENRICHMENT_STATUS_UNENRICHED)\n        .order_by(LeadEvent.created_at.desc())\n        .limit(max_events // 2)\n    ).all())\n    \n    with_domain_events = list(session.exec(\n        select(LeadEvent)\n        .where(LeadEvent.enrichment_status.in_([\n            ENRICHMENT_STATUS_WITH_DOMAIN_NO_EMAIL,\n            ENRICHMENT_STATUS_WITH_PHONE_ONLY\n        ]))\n        .order_by(LeadEvent.created_at.desc())\n        .limit(max_events // 2)\n    ).all())\n    \n    legacy_events = session.exec(\n        select(LeadEvent)\n        .where(LeadEvent.enrichment_status.in_([\n            ENRICHMENT_STATUS_SKIPPED, \n            ENRICHMENT_STATUS_FAILED,\n            ENRICHMENT_STATUS_ENRICHED\n        ]))\n        .limit(max_events // 4)\n    ).all()\n    \n    for le in legacy_events:\n        if le.enrichment_status in [ENRICHMENT_STATUS_SKIPPED, ENRICHMENT_STATUS_FAILED]:\n            le.enrichment_status = ENRICHMENT_STATUS_UNENRICHED\n        elif le.enrichment_status == ENRICHMENT_STATUS_ENRICHED:\n            if le.lead_email:\n                le.enrichment_status = ENRICHMENT_STATUS_ENRICHED_NO_OUTBOUND\n            elif le.lead_domain:\n                le.enrichment_status = ENRICHMENT_STATUS_WITH_DOMAIN_NO_EMAIL\n            else:\n                le.enrichment_status = ENRICHMENT_STATUS_UNENRICHED\n        session.add(le)\n    if legacy_events:\n        session.commit()\n        log_enrichment(\"legacy_migration\", details={\"migrated\": len(legacy_events)})\n    \n    total_unenriched = len(session.exec(\n        select(LeadEvent).where(LeadEvent.enrichment_status == ENRICHMENT_STATUS_UNENRICHED)\n    ).all())\n    \n    total_with_domain = len(session.exec(\n        select(LeadEvent).where(LeadEvent.enrichment_status == ENRICHMENT_STATUS_WITH_DOMAIN_NO_EMAIL)\n    ).all())\n    \n    log_enrichment(\"pipeline_load\", details={\n        \"unenriched_batch\": len(unenriched_events),\n        \"with_domain_batch\": len(with_domain_events),\n        \"total_unenriched\": total_unenriched,\n        \"total_with_domain\": total_with_domain\n    })\n    \n    total_with_phone = len(session.exec(\n        select(LeadEvent).where(LeadEvent.enrichment_status == ENRICHMENT_STATUS_WITH_PHONE_ONLY)\n    ).all())\n    \n    stats = {\n        \"processed\": 0,\n        \"domains_discovered\": 0,\n        \"phones_discovered\": 0,\n        \"enriched\": 0,\n        \"with_domain_no_email\": 0,\n        \"with_phone_only\": 0,\n        \"still_unenriched\": 0,\n        \"archived_unenrichable\": 0,\n        \"companies_created\": 0,\n        \"pending_unenriched\": total_unenriched - len(unenriched_events),\n        \"pending_with_domain\": total_with_domain - len(with_domain_events),\n        \"pending_with_phone\": total_with_phone,\n        \"by_source\": {\n            \"domain_discovery\": 0,\n            \"scrape\": 0,\n            \"phonestorm\": 0,\n            \"signal\": 0,\n            \"none\": 0\n        }\n    }\n    \n    for i, lead_event in enumerate(unenriched_events):\n        mission_log = get_mission_log(lead_event)\n        \n        if not check_enrichment_budget(lead_event):\n            mark_as_unenrichable(\n                lead_event, \n                UNENRICHABLE_REASON_NO_DOMAIN, \n                session, \n                mission_log\n            )\n            stats[\"archived_unenrichable\"] += 1\n            log_enrichment(\"ARCHANGEL_BUDGET_EXHAUSTED\", lead_event_id=lead_event.id,\n                           details={\"attempts\": lead_event.enrichment_attempts,\n                                    \"max\": lead_event.max_enrichment_attempts})\n            continue\n        \n        mission_log.start_new_pass()\n        \n        source_url = _get_source_url_for_event(lead_event, session)\n        \n        effective_company = lead_event.lead_company\n        extraction_source = \"existing\"\n        \n        if not effective_company:\n            signal = None\n            if lead_event.signal_id:\n                signal = session.exec(select(Signal).where(Signal.id == lead_event.signal_id)).first()\n            \n            signal_title = signal.headline if signal and hasattr(signal, 'headline') else None\n            if not signal_title and signal and hasattr(signal, 'context_summary'):\n                signal_title = signal.context_summary\n            \n            namestorm_result = extract_company_candidates(\n                title=signal_title,\n                summary=lead_event.summary,\n                source_url=source_url,\n                lead_event_id=lead_event.id,\n                fetch_page=True\n            )\n            \n            if namestorm_result.success and namestorm_result.best_candidate:\n                effective_company = namestorm_result.best_candidate.name\n                extraction_source = f\"namestorm_{namestorm_result.best_candidate.source}\"\n                \n                lead_event.lead_company = effective_company\n                lead_event.company_name_candidate = effective_company\n                \n                if namestorm_result.all_candidates:\n                    lead_event.candidate_company_names = json.dumps([\n                        c.to_dict() for c in namestorm_result.all_candidates[:5]\n                    ])\n                \n                mission_log.add_entry(\n                    phase=\"NAMESTORM\",\n                    action=\"company_extracted\",\n                    result=\"success\",\n                    notes=f\"Company: {effective_company}, Candidates: {len(namestorm_result.all_candidates)}\"\n                )\n                \n                log_enrichment(\"NAMESTORM_EXTRACTED\", lead_event_id=lead_event.id,\n                               details={\"company\": effective_company, \n                                        \"source\": namestorm_result.best_candidate.source,\n                                        \"confidence\": f\"{namestorm_result.best_candidate.confidence:.2f}\",\n                                        \"candidates\": len(namestorm_result.all_candidates)})\n            else:\n                effective_company = extract_company_name_from_summary(lead_event.summary)\n                extraction_source = \"summary_fallback\"\n                \n                if not effective_company and source_url and 'news.google.com' not in source_url:\n                    effective_company = _extract_company_from_article_body(source_url)\n                    extraction_source = \"article_body_fallback\"\n                \n                if effective_company:\n                    lead_event.lead_company = effective_company\n                    lead_event.company_name_candidate = effective_company\n                    \n                    mission_log.add_entry(\n                        phase=\"NAMESTORM\",\n                        action=\"fallback_extraction\",\n                        result=\"success\",\n                        notes=f\"Company: {effective_company}, Source: {extraction_source}\"\n                    )\n                    \n                    log_enrichment(\"ARCHANGEL_COMPANY_EXTRACTED\", lead_event_id=lead_event.id,\n                                   details={\"company\": effective_company, \"source\": extraction_source})\n                else:\n                    mission_log.add_entry(\n                        phase=\"NAMESTORM\",\n                        action=\"company_extraction\",\n                        result=\"no_result\",\n                        notes=\"No company name found in title, summary, or article body\"\n                    )\n                    \n                    log_enrichment(\"NAMESTORM_SKIP\", lead_event_id=lead_event.id,\n                                   details={\"reason\": \"no_company_found\", \n                                            \"summary\": lead_event.summary[:80] if lead_event.summary else None})\n        \n        domain_result = discover_domain_for_lead_event(\n            lead_event_id=lead_event.id,\n            lead_domain=lead_event.lead_domain,\n            lead_email=lead_event.lead_email,\n            lead_company=effective_company,\n            source_url=source_url,\n            geography=None,\n            niche=None,\n            summary=lead_event.summary\n        )\n        \n        stats[\"processed\"] += 1\n        \n        if domain_result.success and domain_result.domain:\n            lead_event.lead_domain = domain_result.domain\n            lead_event.enrichment_status = ENRICHMENT_STATUS_WITH_DOMAIN_NO_EMAIL\n            lead_event.enrichment_source = domain_result.source\n            lead_event.last_enrichment_at = datetime.utcnow()\n            lead_event.domain_confidence = domain_result.confidence\n            \n            mission_log.add_entry(\n                phase=\"DOMAINSTORM\",\n                action=\"domain_discovered\",\n                query=effective_company,\n                result=\"success\",\n                notes=f\"Domain: {domain_result.domain}, Method: {domain_result.discovery_method}\"\n            )\n            \n            if not lead_event.company_name_candidate:\n                lead_event.company_name_candidate = extract_company_name_from_summary(lead_event.summary)\n            \n            phone_data = []\n            try:\n                phone_result = discover_phones(domain_result.domain)\n                if phone_result.success and phone_result.best_phone:\n                    lead_event.lead_phone_raw = phone_result.best_phone.raw_number\n                    lead_event.lead_phone_e164 = phone_result.best_phone.e164_number\n                    lead_event.phone_confidence = phone_result.best_phone.confidence\n                    lead_event.phone_source = phone_result.best_phone.source\n                    lead_event.phone_type = phone_result.best_phone.phone_type\n                    \n                    phone_data.append({\n                        \"number\": phone_result.best_phone.e164_number,\n                        \"type\": phone_result.best_phone.phone_type,\n                        \"confidence\": phone_result.best_phone.confidence,\n                        \"source\": phone_result.best_phone.source\n                    })\n                    \n                    mission_log.add_entry(\n                        phase=\"PHONESTORM\",\n                        action=\"phone_discovered\",\n                        result=\"success\",\n                        notes=f\"Phone: {phone_result.best_phone.e164_number}\"\n                    )\n                    \n                    stats[\"phones_discovered\"] += 1\n                    stats[\"by_source\"][\"phonestorm\"] = stats[\"by_source\"].get(\"phonestorm\", 0) + 1\n                    \n                    log_enrichment(\"PHONESTORM_FOUND\", lead_event_id=lead_event.id,\n                                   details={\"phone\": phone_result.best_phone.e164_number,\n                                            \"type\": phone_result.best_phone.phone_type,\n                                            \"confidence\": f\"{phone_result.best_phone.confidence:.2f}\",\n                                            \"source\": phone_result.best_phone.source})\n                else:\n                    mission_log.add_entry(\n                        phase=\"PHONESTORM\",\n                        action=\"phone_search\",\n                        result=\"no_result\",\n                        notes=\"No phone found\"\n                    )\n                    log_enrichment(\"PHONESTORM_NONE\", lead_event_id=lead_event.id,\n                                   details={\"domain\": domain_result.domain, \"reason\": \"no_phone_found\"})\n            except Exception as e:\n                mission_log.add_entry(\n                    phase=\"PHONESTORM\",\n                    action=\"phone_search\",\n                    result=\"error\",\n                    notes=str(e)[:100]\n                )\n                log_enrichment(\"PHONESTORM_ERROR\", lead_event_id=lead_event.id,\n                               details={\"error\": str(e)[:50]})\n            \n            if effective_company:\n                try:\n                    company = upsert_company(\n                        session=session,\n                        name=effective_company,\n                        domain=domain_result.domain,\n                        geography=None,\n                        source_type=\"news\",\n                        source_signal_id=lead_event.signal_id,\n                        phones=phone_data if phone_data else None\n                    )\n                    link_lead_to_company(lead_event, company, session)\n                    stats[\"companies_created\"] += 1\n                except Exception as e:\n                    log_enrichment(\"COMPANY_UPSERT_ERROR\", lead_event_id=lead_event.id,\n                                   error=str(e)[:50])\n            \n            save_mission_log(lead_event, mission_log)\n            session.add(lead_event)\n            session.commit()\n            \n            stats[\"domains_discovered\"] += 1\n            stats[\"by_source\"][\"domain_discovery\"] += 1\n            \n            log_enrichment(\"ARCHANGEL_DOMAIN_DISCOVERED\", lead_event_id=lead_event.id,\n                           domain=domain_result.domain,\n                           details={\"method\": domain_result.discovery_method, \n                                    \"confidence\": domain_result.confidence,\n                                    \"company_candidate\": lead_event.company_name_candidate,\n                                    \"has_phone\": bool(lead_event.lead_phone_e164)})\n            \n            with_domain_events.append(lead_event)\n        else:\n            lead_event.enrichment_attempts = (lead_event.enrichment_attempts or 0) + 1\n            lead_event.last_enrichment_at = datetime.utcnow()\n            \n            mission_log.add_entry(\n                phase=\"DOMAINSTORM\",\n                action=\"domain_search\",\n                query=effective_company,\n                result=\"no_result\",\n                notes=f\"No domain found for: {effective_company or 'unknown company'}\"\n            )\n            \n            if not check_enrichment_budget(lead_event):\n                mark_as_unenrichable(\n                    lead_event,\n                    UNENRICHABLE_REASON_NO_DOMAIN if not effective_company else UNENRICHABLE_REASON_NO_OSINT_PRESENCE,\n                    session,\n                    mission_log\n                )\n                stats[\"archived_unenrichable\"] += 1\n            else:\n                save_mission_log(lead_event, mission_log)\n                session.add(lead_event)\n                session.commit()\n                stats[\"still_unenriched\"] += 1\n            \n            stats[\"by_source\"][\"none\"] += 1\n        \n        if i < len(unenriched_events) - 1:\n            await asyncio.sleep(0.5)\n    \n    for i, lead_event in enumerate(with_domain_events):\n        if lead_event.lead_email:\n            lead_event.enrichment_status = ENRICHMENT_STATUS_ENRICHED_NO_OUTBOUND\n            lead_event.email_confidence = 0.95 if \"@\" in lead_event.lead_email else 0.0\n            session.add(lead_event)\n            session.commit()\n            \n            send_result = send_lead_event_immediate(session, lead_event, commit=True)\n            \n            stats[\"enriched\"] += 1\n            stats[\"by_source\"][\"signal\"] += 1\n            if send_result.email_sent:\n                stats[\"immediate_sent\"] = stats.get(\"immediate_sent\", 0) + 1\n            elif send_result.queued_for_review:\n                stats[\"immediate_queued\"] = stats.get(\"immediate_queued\", 0) + 1\n            \n            log_enrichment(\"ARCHANGEL_IMMEDIATE_SEND\", lead_event_id=lead_event.id,\n                           details={\"email\": lead_event.lead_email,\n                                    \"action\": send_result.action, \n                                    \"success\": send_result.success,\n                                    \"reason\": send_result.reason})\n            continue\n        \n        result = await enrich_lead_event(lead_event, session)\n        stats[\"processed\"] += 1\n        \n        new_status = _apply_enrichment_to_lead_event(lead_event, result, session, domain_discovered=False)\n        \n        if new_status == ENRICHMENT_STATUS_ENRICHED_NO_OUTBOUND:\n            stats[\"enriched\"] += 1\n            stats[\"by_source\"][\"scrape\"] += 1\n        else:\n            stats[\"with_domain_no_email\"] += 1\n        \n        if (i + 1) % 5 == 0:\n            log_enrichment(\"pipeline_progress\", details={\n                \"phase\": \"email_enrichment\",\n                \"processed\": i + 1,\n                \"enriched\": stats[\"enriched\"]\n            })\n        \n        if i < len(with_domain_events) - 1:\n            await asyncio.sleep(RATE_LIMIT_DELAY)\n    \n    archival_result = archive_stale_leads(session, max_to_archive=25)\n    stats[\"archived\"] = archival_result.get(\"archived\", 0)\n    \n    log_enrichment(\"pipeline_complete\", details=stats)\n    \n    return stats\n\n\ndef get_enrichment_status() -> dict:\n    \"\"\"\n    Get current enrichment pipeline status and API availability.\n    \n    Returns:\n        Dict with status info:\n        - hunter_available: Whether Hunter API key is set\n        - clearbit_available: Whether Clearbit API key is set\n        - scrape_only_mode: True if no API keys set\n        - dry_run: Whether dry run mode is enabled\n    \"\"\"\n    return {\n        \"hunter_available\": bool(HUNTER_API_KEY),\n        \"clearbit_available\": bool(CLEARBIT_API_KEY),\n        \"scrape_only_mode\": not HUNTER_API_KEY and not CLEARBIT_API_KEY,\n        \"dry_run\": ENRICHMENT_DRY_RUN\n    }\n\n\nSTALE_LEAD_AGE_DAYS = int(os.environ.get(\"STALE_LEAD_AGE_DAYS\", \"30\"))\n\n\ndef archive_stale_leads(session: Session, max_to_archive: int = 50) -> dict:\n    \"\"\"\n    Archive LeadEvents that have been stuck in non-actionable states for too long.\n    \n    Criteria for archival:\n    - Status is UNENRICHED or WITH_DOMAIN_NO_EMAIL (non-actionable)\n    - Created more than STALE_LEAD_AGE_DAYS days ago (default: 30)\n    \n    This prevents the pipeline from repeatedly trying to enrich stale leads\n    that are unlikely to ever be enriched successfully.\n    \n    Args:\n        session: Database session\n        max_to_archive: Maximum leads to archive per call\n        \n    Returns:\n        Dict with archival stats\n    \"\"\"\n    from datetime import timedelta\n    \n    cutoff_date = datetime.utcnow() - timedelta(days=STALE_LEAD_AGE_DAYS)\n    \n    stale_events = session.exec(\n        select(LeadEvent)\n        .where(LeadEvent.enrichment_status.in_([\n            ENRICHMENT_STATUS_UNENRICHED,\n            ENRICHMENT_STATUS_WITH_DOMAIN_NO_EMAIL\n        ]))\n        .where(LeadEvent.created_at < cutoff_date)\n        .order_by(LeadEvent.created_at.asc())\n        .limit(max_to_archive)\n    ).all()\n    \n    if not stale_events:\n        return {\"archived\": 0, \"message\": \"No stale leads to archive\"}\n    \n    archived_count = 0\n    archived_by_status = {\"UNENRICHED\": 0, \"WITH_DOMAIN_NO_EMAIL\": 0}\n    \n    for event in stale_events:\n        old_status = event.enrichment_status\n        event.enrichment_status = ENRICHMENT_STATUS_ARCHIVED\n        event.last_enrichment_at = datetime.utcnow()\n        session.add(event)\n        \n        if old_status == ENRICHMENT_STATUS_UNENRICHED:\n            archived_by_status[\"UNENRICHED\"] += 1\n        else:\n            archived_by_status[\"WITH_DOMAIN_NO_EMAIL\"] += 1\n        \n        archived_count += 1\n    \n    session.commit()\n    \n    log_enrichment(\"stale_leads_archived\", details={\n        \"archived\": archived_count,\n        \"cutoff_days\": STALE_LEAD_AGE_DAYS,\n        \"by_status\": archived_by_status\n    })\n    \n    return {\n        \"archived\": archived_count,\n        \"cutoff_days\": STALE_LEAD_AGE_DAYS,\n        \"by_status\": archived_by_status,\n        \"message\": f\"Archived {archived_count} stale leads (>{STALE_LEAD_AGE_DAYS} days old)\"\n    }\n\n\nprint(f\"{_get_dry_run_prefix()}[ENRICHMENT][STARTUP] Hunter: {'enabled' if HUNTER_API_KEY else 'disabled'}, Clearbit: {'enabled' if CLEARBIT_API_KEY else 'disabled'}, DRY_RUN: {ENRICHMENT_DRY_RUN}\")\n","path":null,"size_bytes":73798,"size_tokens":null},"signal_sources.py":{"content":"\"\"\"\nSignalNet Framework for HossAgent\n\nA modular signal ingestion system that transforms HossAgent from generic lead gen\ninto a context-aware intelligence engine. Signal sources detect timing/context\nsignals, not contact data - HossNative handles lead discovery via web scraping.\n\n============================================================================\nSIGNAL_MODE CONFIGURATION\n============================================================================\nEnvironment variable SIGNAL_MODE controls signal pipeline behavior:\n\n  PRODUCTION: Run real sources, create LeadEvents for high-scoring signals (default)\n  SANDBOX: Run sources and score signals, but don't create LeadEvents\n  OFF: Skip signal ingestion entirely\n\nDefault: PRODUCTION (creates LeadEvents for signals scoring >= 60)\nThreshold: LEADEVENT_SCORE_THRESHOLD = 60\n\n============================================================================\nDRY_RUN MODE\n============================================================================\nEnvironment variable SIGNAL_DRY_RUN controls API call behavior:\n\n  True/1/yes: Sources log what they WOULD fetch, generate mock data instead\n  False/0/no: Sources make real API calls (default)\n\nWhen DRY_RUN is enabled:\n  - All log messages are prefixed with [DRY_RUN]\n  - No external API calls are made\n  - Mock/sample signals are generated for testing\n  - Useful for development and testing without hitting rate limits\n\n============================================================================\nARCHITECTURE\n============================================================================\n\n  SignalSource (ABC)         - Abstract base class for signal sources\n       |\n  SignalRegistry             - Manages and registers sources, handles cooldowns\n       |\n  SignalPipeline            - Orchestrates fetch -> parse -> score -> persist\n       |\n  score_signal()            - Scoring utility with weighted factors\n       |\n  log_signal_activity()     - Structured logging for debugging\n\n============================================================================\nERROR HANDLING & AUTO-DISABLE\n============================================================================\nPer-source error tracking:\n  - error_count: Consecutive error count\n  - MAX_CONSECUTIVE_ERRORS: 5 (default)\n  - Sources with > 5 consecutive errors are auto-disabled\n  - Use reset_source() to re-enable disabled sources\n\n============================================================================\nMIAMI-FIRST TARGETING\n============================================================================\nVia LEAD_GEOGRAPHY and LEAD_NICHE environment variables:\n  - Geography match: +15 score boost\n  - Niche match: +10 score boost\n  - Miami-tuned urgency categories (HURRICANE=95, etc.)\n============================================================================\n\"\"\"\n\nimport json\nimport os\nimport random\nimport requests\nimport time\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\nfrom typing import Any, Dict, List, Optional, Tuple, Type\nfrom sqlmodel import Session, select\n\nfrom models import Signal, LeadEvent, SignalLog, Customer, BusinessProfile, ENRICHMENT_STATUS_UNENRICHED, ENRICHMENT_STATUS_ENRICHED\n\n\nOPENWEATHER_API_KEY = os.environ.get(\"OPENWEATHER_API_KEY\", \"\")\nNEWS_API_KEY = os.environ.get(\"NEWS_API_KEY\", \"\")\n\n\nSIGNAL_MODE = os.environ.get(\"SIGNAL_MODE\", \"PRODUCTION\").upper()  # Changed from SANDBOX\nSIGNAL_DRY_RUN = os.environ.get(\"SIGNAL_DRY_RUN\", \"false\").lower() in (\"true\", \"1\", \"yes\")\nLEAD_GEOGRAPHY = os.environ.get(\"LEAD_GEOGRAPHY\", \"Miami, Broward, South Florida\")\nLEAD_NICHE = os.environ.get(\"LEAD_NICHE\", \"HVAC, Roofing, Med Spa, Immigration Attorney\")\n\nLEAD_GEOGRAPHY_LIST = [g.strip().lower() for g in LEAD_GEOGRAPHY.split(\",\")]\nLEAD_NICHE_LIST = [n.strip().lower() for n in LEAD_NICHE.split(\",\")]\n\nLEADEVENT_SCORE_THRESHOLD = 60  # Changed from 65\nMAX_CONSECUTIVE_ERRORS = 5\n\nURGENCY_CATEGORY_WEIGHTS = {\n    \"HURRICANE\": 95,\n    \"HURRICANE_SEASON\": 95,\n    \"GROWTH_SIGNAL\": 80,\n    \"REVIEW\": 70,\n    \"REPUTATION_CHANGE\": 70,\n    \"COMPETITOR_SHIFT\": 75,\n    \"MIAMI_PRICE_MOVE\": 70,\n    \"BILINGUAL_OPPORTUNITY\": 65,\n    \"NEWS\": 60,\n    \"PERMIT\": 55,\n    \"JOB_POSTING\": 55,\n    \"OPPORTUNITY\": 50,\n    \"DEFAULT\": 50,\n}\n\n_log_session: Optional[Session] = None\n\n\ndef _get_dry_run_prefix() -> str:\n    \"\"\"Get log prefix for dry run mode.\"\"\"\n    return \"[DRY_RUN]\" if SIGNAL_DRY_RUN else \"\"\n\n\ndef log_signal_activity(\n    source_name: str,\n    action: str,\n    details: Optional[Dict] = None,\n    signal_count: int = 0,\n    error: Optional[str] = None,\n    session: Optional[Session] = None\n) -> None:\n    \"\"\"\n    Log signal activity for debugging with structured format.\n    \n    Logs to console and optionally persists to database for admin visibility.\n    \n    Args:\n        source_name: Name of the signal source (e.g., 'weather_openweather')\n        action: Action being performed (fetch, parse, score, persist, error, dry_run, auto_disable, reset)\n        details: Optional dict with relevant context data\n        signal_count: Number of signals processed (if applicable)\n        error: Error message (if any)\n        session: Optional database session for persistence\n    \"\"\"\n    prefix = _get_dry_run_prefix()\n    timestamp = datetime.utcnow().isoformat()\n    \n    details_str = json.dumps(details) if details else \"{}\"\n    \n    log_level = \"ERROR\" if error else \"INFO\"\n    error_part = f\" | Error: {error}\" if error else \"\"\n    count_part = f\" | Count: {signal_count}\" if signal_count > 0 else \"\"\n    \n    console_msg = f\"{prefix}[SIGNALNET][{source_name.upper()}][{action.upper()}] {details_str[:200]}{count_part}{error_part}\"\n    print(console_msg)\n    \n    if session:\n        try:\n            log_entry = SignalLog(\n                timestamp=datetime.utcnow(),\n                source_name=source_name,\n                action=action,\n                details=details_str,\n                signal_count=signal_count,\n                error_message=error,\n                dry_run=SIGNAL_DRY_RUN,\n            )\n            session.add(log_entry)\n            session.commit()\n        except Exception as e:\n            print(f\"[SIGNALNET][LOG] Failed to persist log entry: {e}\")\n\n\ndef set_log_session(session: Optional[Session]) -> None:\n    \"\"\"Set the global session for logging persistence.\"\"\"\n    global _log_session\n    _log_session = session\n\n\ndef get_log_session() -> Optional[Session]:\n    \"\"\"Get the global session for logging persistence.\"\"\"\n    return _log_session\n\n\nprint(f\"{_get_dry_run_prefix()}[SIGNALNET][STARTUP] Mode: {SIGNAL_MODE} (default: PRODUCTION), Threshold: {LEADEVENT_SCORE_THRESHOLD}, DRY_RUN: {SIGNAL_DRY_RUN}, Geography: {LEAD_GEOGRAPHY}, Niche: {LEAD_NICHE}\")\n\n\n@dataclass\nclass RawSignal:\n    \"\"\"\n    Raw signal data fetched from a source before parsing.\n    \n    This intermediate representation allows sources to return\n    unprocessed data that gets standardized during parsing.\n    \"\"\"\n    source_name: str\n    source_type: str\n    raw_data: Dict[str, Any]\n    fetched_at: datetime = field(default_factory=datetime.utcnow)\n    geography: Optional[str] = None\n    company_hint: Optional[str] = None\n    lead_id_hint: Optional[int] = None\n    company_id_hint: Optional[int] = None\n\n\n@dataclass\nclass ParsedSignal:\n    \"\"\"\n    Parsed signal ready for scoring and persistence.\n    \n    Represents a standardized signal after source-specific parsing.\n    \"\"\"\n    source_type: str\n    raw_payload: str\n    context_summary: str\n    geography: Optional[str] = None\n    lead_id: Optional[int] = None\n    company_id: Optional[int] = None\n    category_hint: Optional[str] = None\n    niche_hint: Optional[str] = None\n    created_at: datetime = field(default_factory=datetime.utcnow)\n\n\n@dataclass\nclass ScoredSignal:\n    \"\"\"\n    Signal with computed score and explanation.\n    \"\"\"\n    parsed_signal: ParsedSignal\n    score: int\n    score_explanation: str\n    should_create_event: bool\n\n\nclass SignalSource(ABC):\n    \"\"\"\n    Abstract base class for signal sources.\n    \n    Each signal source represents a data feed that provides context/timing\n    information about companies and markets. Signal sources do NOT provide\n    contact data - HossNative handles lead discovery via autonomous web scraping.\n    \n    Subclasses must implement:\n      - fetch() -> List[RawSignal]: Get raw signals from the source\n      - parse(raw: RawSignal) -> ParsedSignal: Convert raw to parsed format\n    \n    Source lifecycle is tracked via:\n      - last_run: When the source was last executed\n      - last_error: Most recent error message (if any)\n      - items_last_run: Number of signals fetched in last run\n      - error_count: Consecutive error count (for auto-disable)\n    \n    Cooldown and rate limiting:\n      - cooldown_seconds: Minimum time between runs\n      - max_items_per_run: Cap on signals per execution\n    \n    Auto-disable:\n      - Sources with > MAX_CONSECUTIVE_ERRORS (5) are auto-disabled\n      - Use reset_source() to re-enable\n    \n    DRY_RUN mode:\n      - When SIGNAL_DRY_RUN is True, sources generate mock data instead of API calls\n      - Override _generate_mock_signals() for source-specific mock data\n    \"\"\"\n    \n    def __init__(self):\n        self._last_run: Optional[datetime] = None\n        self._next_eligible: Optional[datetime] = None\n        self._last_error: Optional[str] = None\n        self._items_last_run: int = 0\n        self._error_count: int = 0\n        self._auto_disabled: bool = False\n        self._disabled_reason: Optional[str] = None\n    \n    @property\n    @abstractmethod\n    def name(self) -> str:\n        \"\"\"Unique name for this source (e.g., 'google_reviews', 'indeed_jobs').\"\"\"\n        ...\n    \n    @property\n    @abstractmethod\n    def source_type(self) -> str:\n        \"\"\"Category of signals this source provides (e.g., 'review', 'job_posting').\"\"\"\n        ...\n    \n    @property\n    def enabled(self) -> bool:\n        \"\"\"Whether this source is active. Override to add conditional logic.\"\"\"\n        return True\n    \n    @property\n    def cooldown_seconds(self) -> int:\n        \"\"\"Minimum seconds between runs. Override for source-specific cooldowns.\"\"\"\n        return 300\n    \n    @property\n    def max_items_per_run(self) -> int:\n        \"\"\"Maximum signals to fetch per run. Override for rate limiting.\"\"\"\n        return 50\n    \n    @property\n    def last_run(self) -> Optional[datetime]:\n        \"\"\"When this source was last executed.\"\"\"\n        return self._last_run\n    \n    @property\n    def next_eligible(self) -> Optional[datetime]:\n        \"\"\"When this source will next be eligible to run.\"\"\"\n        return self._next_eligible\n    \n    @property\n    def last_error(self) -> Optional[str]:\n        \"\"\"Most recent error message, if any.\"\"\"\n        return self._last_error\n    \n    @property\n    def items_last_run(self) -> int:\n        \"\"\"Number of signals fetched in last run.\"\"\"\n        return self._items_last_run\n    \n    @property\n    def error_count(self) -> int:\n        \"\"\"Consecutive error count.\"\"\"\n        return self._error_count\n    \n    @property\n    def is_auto_disabled(self) -> bool:\n        \"\"\"Whether source was auto-disabled due to errors.\"\"\"\n        return self._auto_disabled\n    \n    @property\n    def disabled_reason(self) -> Optional[str]:\n        \"\"\"Reason for auto-disable, if applicable.\"\"\"\n        return self._disabled_reason\n    \n    @property\n    def is_dry_run(self) -> bool:\n        \"\"\"Check if DRY_RUN mode is enabled globally.\"\"\"\n        return SIGNAL_DRY_RUN\n    \n    def is_eligible(self) -> bool:\n        \"\"\"\n        Check if this source is eligible to run.\n        \n        Returns True if:\n          - Source is enabled\n          - Source is not auto-disabled\n          - Cooldown period has elapsed since last_run\n        \"\"\"\n        if not self.enabled:\n            return False\n        \n        if self._auto_disabled:\n            return False\n        \n        if self._last_run is None:\n            return True\n        \n        elapsed = (datetime.utcnow() - self._last_run).total_seconds()\n        return elapsed >= self.cooldown_seconds\n    \n    def record_run(self, items_count: int, error: Optional[str] = None):\n        \"\"\"Record the results of a run and update error tracking.\"\"\"\n        self._last_run = datetime.utcnow()\n        self._next_eligible = self._last_run + timedelta(seconds=self.cooldown_seconds)\n        self._items_last_run = items_count\n        self._last_error = error\n        \n        if error:\n            self._error_count += 1\n            if self._error_count >= MAX_CONSECUTIVE_ERRORS:\n                self._auto_disabled = True\n                self._disabled_reason = f\"Auto-disabled after {self._error_count} consecutive errors: {error}\"\n                log_signal_activity(\n                    self.name,\n                    \"auto_disable\",\n                    {\"error_count\": self._error_count, \"last_error\": error},\n                    error=self._disabled_reason,\n                    session=get_log_session()\n                )\n        else:\n            self._error_count = 0\n    \n    def reset(self) -> bool:\n        \"\"\"\n        Reset source error state and re-enable if auto-disabled.\n        \n        Returns:\n            True if source was reset, False if no reset needed\n        \"\"\"\n        was_disabled = self._auto_disabled\n        self._error_count = 0\n        self._auto_disabled = False\n        self._disabled_reason = None\n        self._last_error = None\n        \n        if was_disabled:\n            log_signal_activity(\n                self.name,\n                \"reset\",\n                {\"previously_disabled\": True, \"reason\": \"Manual reset\"},\n                session=get_log_session()\n            )\n        \n        return was_disabled\n    \n    def _generate_mock_signals(self) -> List[RawSignal]:\n        \"\"\"\n        Generate mock signals for DRY_RUN mode.\n        \n        Override in subclasses for source-specific mock data.\n        Default implementation returns 1-3 generic mock signals.\n        \"\"\"\n        num_signals = random.randint(1, 3)\n        signals = []\n        \n        for i in range(num_signals):\n            signals.append(RawSignal(\n                source_name=self.name,\n                source_type=self.source_type,\n                raw_data={\n                    \"mock\": True,\n                    \"index\": i,\n                    \"generated_at\": datetime.utcnow().isoformat(),\n                    \"description\": f\"Mock signal #{i+1} from {self.name}\",\n                },\n                geography=\"Miami\",\n            ))\n        \n        return signals\n    \n    def fetch_with_dry_run(self) -> List[RawSignal]:\n        \"\"\"\n        Wrapper for fetch() that handles DRY_RUN mode.\n        \n        In DRY_RUN mode, logs what would be fetched and returns mock data.\n        In normal mode, calls the actual fetch() implementation.\n        \"\"\"\n        if self.is_dry_run:\n            log_signal_activity(\n                self.name,\n                \"dry_run\",\n                {\"action\": \"would_fetch\", \"source_type\": self.source_type},\n                session=get_log_session()\n            )\n            return self._generate_mock_signals()\n        else:\n            return self.fetch()\n    \n    @abstractmethod\n    def fetch(self) -> List[RawSignal]:\n        \"\"\"\n        Fetch raw signals from the source.\n        \n        Returns:\n            List of RawSignal objects containing unprocessed source data.\n            \n        Raises:\n            Exception: On fetch failure (will be captured by pipeline)\n        \"\"\"\n        ...\n    \n    @abstractmethod\n    def parse(self, raw: RawSignal) -> ParsedSignal:\n        \"\"\"\n        Parse a raw signal into standardized format.\n        \n        Args:\n            raw: RawSignal from fetch()\n            \n        Returns:\n            ParsedSignal ready for scoring and persistence\n        \"\"\"\n        ...\n    \n    def get_status(self) -> Dict[str, Any]:\n        \"\"\"Get current status of this source.\"\"\"\n        return {\n            \"name\": self.name,\n            \"source_type\": self.source_type,\n            \"enabled\": self.enabled,\n            \"cooldown_seconds\": self.cooldown_seconds,\n            \"max_items_per_run\": self.max_items_per_run,\n            \"last_run\": self._last_run.isoformat() if self._last_run else None,\n            \"next_eligible\": self._next_eligible.isoformat() if self._next_eligible else None,\n            \"last_error\": self._last_error,\n            \"items_last_run\": self._items_last_run,\n            \"error_count\": self._error_count,\n            \"is_auto_disabled\": self._auto_disabled,\n            \"disabled_reason\": self._disabled_reason,\n            \"is_eligible\": self.is_eligible(),\n            \"dry_run\": self.is_dry_run,\n        }\n    \n    def get_throttle_status(self) -> Dict[str, Any]:\n        \"\"\"Get throttle and error tracking status for this source.\"\"\"\n        now = datetime.utcnow()\n        time_until_eligible = None\n        \n        if self._next_eligible and self._next_eligible > now:\n            time_until_eligible = (self._next_eligible - now).total_seconds()\n        \n        return {\n            \"name\": self.name,\n            \"last_run\": self._last_run.isoformat() if self._last_run else None,\n            \"next_eligible\": self._next_eligible.isoformat() if self._next_eligible else None,\n            \"seconds_until_eligible\": time_until_eligible,\n            \"error_count\": self._error_count,\n            \"max_errors_before_disable\": MAX_CONSECUTIVE_ERRORS,\n            \"is_auto_disabled\": self._auto_disabled,\n            \"disabled_reason\": self._disabled_reason,\n        }\n\n\nclass SignalRegistry:\n    \"\"\"\n    Registry for managing SignalSource instances.\n    \n    Handles:\n      - Registration of source classes and instances\n      - Eligibility checking based on cooldowns and enabled flags\n      - Retrieval of sources for pipeline execution\n    \"\"\"\n    \n    def __init__(self):\n        self._sources: Dict[str, SignalSource] = {}\n        self._source_classes: Dict[str, Type[SignalSource]] = {}\n    \n    def register_class(self, source_class: Type[SignalSource]) -> None:\n        \"\"\"\n        Register a SignalSource class for lazy instantiation.\n        \n        Args:\n            source_class: A SignalSource subclass (not an instance)\n        \"\"\"\n        temp_instance = source_class()\n        self._source_classes[temp_instance.name] = source_class\n        print(f\"[SIGNALNET][REGISTRY] Registered source class: {temp_instance.name}\")\n    \n    def register(self, source: SignalSource) -> None:\n        \"\"\"\n        Register an instantiated SignalSource.\n        \n        Args:\n            source: A SignalSource instance\n        \"\"\"\n        self._sources[source.name] = source\n        print(f\"[SIGNALNET][REGISTRY] Registered source: {source.name} ({source.source_type})\")\n    \n    def unregister(self, name: str) -> bool:\n        \"\"\"\n        Remove a source from the registry.\n        \n        Args:\n            name: Name of the source to remove\n            \n        Returns:\n            True if removed, False if not found\n        \"\"\"\n        if name in self._sources:\n            del self._sources[name]\n            print(f\"[SIGNALNET][REGISTRY] Unregistered source: {name}\")\n            return True\n        if name in self._source_classes:\n            del self._source_classes[name]\n            return True\n        return False\n    \n    def get_source(self, name: str) -> Optional[SignalSource]:\n        \"\"\"Get a specific source by name.\"\"\"\n        if name in self._sources:\n            return self._sources[name]\n        if name in self._source_classes:\n            self._sources[name] = self._source_classes[name]()\n            return self._sources[name]\n        return None\n    \n    def get_all_sources(self) -> List[SignalSource]:\n        \"\"\"Get all registered sources (instantiated).\"\"\"\n        for name, cls in self._source_classes.items():\n            if name not in self._sources:\n                self._sources[name] = cls()\n        return list(self._sources.values())\n    \n    def get_eligible_sources(self) -> List[SignalSource]:\n        \"\"\"\n        Get sources eligible to run in the current cycle.\n        \n        Returns sources that:\n          - Are enabled\n          - Have passed their cooldown period\n        \"\"\"\n        all_sources = self.get_all_sources()\n        eligible = [s for s in all_sources if s.is_eligible()]\n        \n        print(f\"[SIGNALNET][REGISTRY] {len(eligible)}/{len(all_sources)} sources eligible\")\n        return eligible\n    \n    def get_status(self) -> Dict[str, Any]:\n        \"\"\"Get status of all registered sources.\"\"\"\n        all_sources = self.get_all_sources()\n        return {\n            \"total_sources\": len(all_sources),\n            \"eligible_sources\": len([s for s in all_sources if s.is_eligible()]),\n            \"sources\": [s.get_status() for s in all_sources],\n        }\n\n\ndef _matches_lead_geography(geography: Optional[str]) -> bool:\n    \"\"\"Check if geography matches configured LEAD_GEOGRAPHY.\"\"\"\n    if not geography:\n        return False\n    geo_lower = geography.lower()\n    return any(target in geo_lower for target in LEAD_GEOGRAPHY_LIST)\n\n\ndef _matches_lead_niche(niche: Optional[str]) -> bool:\n    \"\"\"Check if niche matches configured LEAD_NICHE.\"\"\"\n    if not niche:\n        return False\n    niche_lower = niche.lower()\n    return any(target in niche_lower for target in LEAD_NICHE_LIST)\n\n\ndef _calculate_recency_score(created_at: datetime, max_age_hours: int = 72) -> int:\n    \"\"\"\n    Calculate recency score (0-100) based on signal age.\n    \n    Newer signals score higher:\n      - 0-6 hours: 100\n      - 6-24 hours: 80-99\n      - 24-48 hours: 60-79\n      - 48-72 hours: 40-59\n      - 72+ hours: 20-39\n    \"\"\"\n    now = datetime.utcnow()\n    age_hours = (now - created_at).total_seconds() / 3600\n    \n    if age_hours <= 6:\n        return 100\n    elif age_hours <= 24:\n        return int(80 + (24 - age_hours) / 18 * 19)\n    elif age_hours <= 48:\n        return int(60 + (48 - age_hours) / 24 * 19)\n    elif age_hours <= max_age_hours:\n        return int(40 + (max_age_hours - age_hours) / 24 * 19)\n    else:\n        return max(20, int(40 - (age_hours - max_age_hours) / 24 * 10))\n\n\ndef score_signal(\n    parsed_signal: ParsedSignal,\n    category: Optional[str] = None,\n    bypass_niche_filter: bool = False,\n) -> ScoredSignal:\n    \"\"\"\n    Score a parsed signal based on weighted factors.\n    \n    Scoring components (0-100 final range):\n      1. Urgency category weight (30% of score)\n         - HURRICANE: 95 base\n         - GROWTH_SIGNAL: 80 base\n         - REVIEW: 70 base\n         - etc.\n      \n      2. Recency decay (25% of score)\n         - Newer signals score higher\n         - Decays over 72 hours\n      \n      3. Geography match boost (25% of score)\n         - +25 if matches LEAD_GEOGRAPHY\n         - 0 otherwise\n      \n      4. Niche match boost (20% of score)\n         - +20 if matches LEAD_NICHE\n         - 0 otherwise (bypassed if bypass_niche_filter=True)\n    \n    Args:\n        parsed_signal: The ParsedSignal to score\n        category: Optional category override (inferred if not provided)\n        bypass_niche_filter: If True, skip niche matching and allow all signals in target geography\n        \n    Returns:\n        ScoredSignal with score, explanation, and event creation flag\n    \"\"\"\n    explanation_parts = []\n    \n    if category is None:\n        category = parsed_signal.category_hint or _infer_category(\n            parsed_signal.source_type,\n            parsed_signal.context_summary\n        )\n    \n    category_upper = category.upper()\n    category_base = URGENCY_CATEGORY_WEIGHTS.get(\n        category_upper,\n        URGENCY_CATEGORY_WEIGHTS[\"DEFAULT\"]\n    )\n    category_score = int(category_base * 0.30)\n    explanation_parts.append(f\"Category {category}: {category_base}0.30 = {category_score}\")\n    \n    recency_base = _calculate_recency_score(parsed_signal.created_at)\n    recency_score = int(recency_base * 0.25)\n    age_hours = (datetime.utcnow() - parsed_signal.created_at).total_seconds() / 3600\n    explanation_parts.append(f\"Recency ({age_hours:.1f}h old): {recency_base}0.25 = {recency_score}\")\n    \n    geo_match = _matches_lead_geography(parsed_signal.geography)\n    geo_score = 25 if geo_match else 0\n    geo_status = f\"MATCH ({parsed_signal.geography})\" if geo_match else f\"no match ({parsed_signal.geography or 'none'})\"\n    explanation_parts.append(f\"Geography {geo_status}: {geo_score}\")\n    \n    if bypass_niche_filter:\n        niche_score = 20\n        niche_status = \"BYPASS (all signals allowed)\"\n        explanation_parts.append(f\"Niche {niche_status}: {niche_score}\")\n    else:\n        niche_match = _matches_lead_niche(parsed_signal.niche_hint)\n        niche_score = 20 if niche_match else 0\n        niche_status = f\"MATCH ({parsed_signal.niche_hint})\" if niche_match else f\"no match ({parsed_signal.niche_hint or 'none'})\"\n        explanation_parts.append(f\"Niche {niche_status}: {niche_score}\")\n    \n    total_score = category_score + recency_score + geo_score + niche_score\n    total_score = max(0, min(100, total_score))\n    \n    should_create_event = total_score >= LEADEVENT_SCORE_THRESHOLD\n    \n    explanation = f\"Total: {total_score}/100 | \" + \" | \".join(explanation_parts)\n    \n    return ScoredSignal(\n        parsed_signal=parsed_signal,\n        score=total_score,\n        score_explanation=explanation,\n        should_create_event=should_create_event,\n    )\n\n\ndef _infer_category(source_type: str, context: str) -> str:\n    \"\"\"\n    Infer signal category from source type and context.\n    \n    Miami-tuned categories:\n      - HURRICANE_SEASON: Storm/hurricane signals\n      - BILINGUAL_OPPORTUNITY: Spanish/bilingual signals\n      - MIAMI_PRICE_MOVE: Local pricing changes\n      - COMPETITOR_SHIFT: Competitor positioning\n      - GROWTH_SIGNAL: Hiring/expansion\n      - REPUTATION_CHANGE: Review signals\n      - OPPORTUNITY: General opportunity\n    \"\"\"\n    context_lower = context.lower()\n    \n    if \"hurricane\" in context_lower or \"storm\" in context_lower:\n        return \"HURRICANE_SEASON\"\n    elif \"bilingual\" in context_lower or \"spanish\" in context_lower:\n        return \"BILINGUAL_OPPORTUNITY\"\n    elif \"price\" in context_lower and (\"miami\" in context_lower or \"local\" in context_lower):\n        return \"MIAMI_PRICE_MOVE\"\n    elif \"competitor\" in context_lower or \"pricing\" in context_lower:\n        return \"COMPETITOR_SHIFT\"\n    elif \"hiring\" in context_lower or \"job\" in context_lower or \"growth\" in context_lower:\n        return \"GROWTH_SIGNAL\"\n    elif \"review\" in context_lower:\n        return \"REPUTATION_CHANGE\"\n    elif source_type == \"job_posting\":\n        return \"GROWTH_SIGNAL\"\n    elif source_type == \"review\":\n        return \"REPUTATION_CHANGE\"\n    elif source_type == \"competitor_update\":\n        return \"COMPETITOR_SHIFT\"\n    elif source_type == \"weather\":\n        return \"HURRICANE_SEASON\"\n    elif source_type == \"permit\":\n        return \"GROWTH_SIGNAL\"\n    else:\n        return \"OPPORTUNITY\"\n\n\ndef _generate_recommended_action(category: str, context: str) -> str:\n    \"\"\"Generate recommended action based on category.\"\"\"\n    actions = {\n        \"HURRICANE_SEASON\": \"Offer hurricane-season discount bundle or preparedness package\",\n        \"COMPETITOR_SHIFT\": \"Send competitive analysis snapshot highlighting your differentiators\",\n        \"GROWTH_SIGNAL\": \"Propose partnership or capacity-building services\",\n        \"BILINGUAL_OPPORTUNITY\": \"Highlight bilingual staff on homepage - big ROI in Miami market\",\n        \"REPUTATION_CHANGE\": \"Offer reputation management or customer experience audit\",\n        \"MIAMI_PRICE_MOVE\": \"Prepare market pricing comparison and value proposition\",\n        \"OPPORTUNITY\": \"Send contextual outreach with relevant service offer\",\n    }\n    return actions.get(category.upper(), \"Prepare contextual outreach based on signal\")\n\n\ndef _extract_company_from_context(context_summary: str) -> Optional[str]:\n    \"\"\"\n    Extract company name from signal context_summary.\n    \n    Uses strict validation to extract real company names from news headlines.\n    Returns None if no high-confidence company name can be extracted.\n    \"\"\"\n    if not context_summary:\n        return None\n    \n    import re\n    \n    text = context_summary.strip()\n    if text.startswith(\"News: \"):\n        text = text[6:]\n    \n    verb_words = [\n        \"opens\", \"expands\", \"announces\", \"acquires\", \"hires\", \"launches\",\n        \"adds\", \"reveals\", \"unveils\", \"introduces\", \"offers\", \"receives\",\n        \"gets\", \"wins\", \"reports\", \"seeks\", \"files\", \"gives\", \"breaks\",\n        \"evolves\", \"working\", \"works\", \"calls\", \"booming\", \"ramps\",\n    ]\n    \n    skip_words = [\n        \"miami\", \"florida\", \"south florida\", \"fort lauderdale\", \"broward\", \n        \"palm beach\", \"pompano\", \"boca raton\", \"hialeah\", \"orlando\", \"tampa\",\n        \"local\", \"area\", \"regional\", \"downtown\", \"new\", \"west\", \"east\",\n        \"the\", \"a\", \"an\", \"this\", \"that\", \"first\", \"best\", \"top\",\n        \"breaking\", \"update\", \"latest\", \"today\", \"now\", \"here\", \"see\",\n        \"south\", \"north\", \"christmas\", \"holiday\", \"fortune\", \"major\",\n        \"business\", \"businesses\", \"company\", \"companies\", \"firm\", \"firms\",\n        \"store\", \"stores\", \"owner\", \"owners\", \"partnership\", \"trump\",\n        \"global\", \"national\", \"international\", \"biggest\", \"largest\",\n        \"development\", \"developments\", \"project\", \"projects\", \"sites\",\n        \"vets\", \"veterans\", \"billionaire\", \"ceo\", \"executive\", \"over\",\n        \"architecture\", \"architectural\", \"real estate\", \"realty\", \"realtor\",\n    ]\n    \n    bad_words = [\n        \"roofing\", \"roofs\", \"roof\", \"plumbing\", \"hvac\", \"air\", \"ac\",\n        \"housing\", \"authority\", \"association\", \"contractors\", \"boom\",\n        \"permits\", \"permit\", \"zoning\", \"construction\", \"building\",\n        \"homes\", \"home\", \"house\", \"properties\", \"services\", \"service\",\n        \"repair\", \"repairs\", \"cooling\", \"heating\", \"furnaces\", \"supporter\",\n    ]\n    \n    reject_phrases = [\n        \"breaks down\", \"supporter\", \"evolves\", \"boom\", \"gives away\",\n        \"vets receive\", \"florida home\", \"fortune 500\", \"amid\", \"after\",\n        \"between miami\", \"across south\", \"works to\", \"files complaint\",\n        \"hurricane\", \"preparedness\", \"advisory\", \"demand expected\",\n        \"competitor\", \"pricing\", \"updated\", \"call florida\", \"call home\",\n        \"south florida\", \"new businesses\", \"stores opening\", \"ice takes\",\n        \"development sites\", \"major projects\", \"broke ground\", \"bankruptcy\",\n    ]\n    \n    generic_descriptors = [\n        \"firm\", \"firms\", \"agency\", \"agencies\", \"restaurant\", \"bar\", \"hotel\",\n        \"clinic\", \"store\", \"shop\", \"office\", \"group\", \"corporation\", \"corp\",\n        \"architecture\", \"architectural\", \"global\", \"national\", \"local\",\n    ]\n    all_blocked = verb_words + bad_words + skip_words + generic_descriptors\n    \n    multi_word_pattern = r\"^([A-Z][a-zA-Z0-9]+(?:\\s+[A-Z]?[a-zA-Z0-9&'.-]+){1,4})\\s+(?:Announces?|Opens?|Expands?|Launches?|Acquires?|Hires?|Adds?|Reveals?|Reports?)\"\n    match = re.search(multi_word_pattern, text)\n    if match:\n        company = match.group(1).strip()\n        words = company.split()\n        \n        if len(words) >= 2 and len(words) <= 5:\n            has_blocked = any(w.lower() in all_blocked for w in words)\n            if not has_blocked:\n                if not any(phrase in company.lower() for phrase in reject_phrases):\n                    if len(company) >= 5 and len(company) <= 60:\n                        return company\n    \n    branded_pattern = r\"^([A-Z][a-zA-Z]+(?:\\s+[A-Z]?[a-zA-Z]+)*\\s+(?:Air|Roofing|Plumbing|HVAC|Electric|Cleaning|Landscaping|Construction|Realty|Properties|Solutions|Services|Group|Partners))\\s+(?:Announces?|Opens?|Expands?|Acquires?|Launches?|Hires?|Adds?)\"\n    match = re.search(branded_pattern, text)\n    if match:\n        company = match.group(1).strip()\n        if len(company.split()) >= 2 and len(company.split()) <= 5:\n            return company\n    \n    name_dash_verb = r\"^([A-Z][a-zA-Z&']+(?:\\s+[A-Z]?[a-zA-Z&']+){0,3})\\s+[-]\\s+(?:[A-Z]|Business|PR|Markets|The)\"\n    match = re.search(name_dash_verb, text)\n    if match:\n        company = match.group(1).strip()\n        if len(company.split()) >= 2 and len(company) <= 40:\n            if company.lower() not in skip_words + bad_words:\n                return company\n    \n    all_caps = r\"^([A-Z]{2,}(?:\\s+[A-Z]{2,})*)\\s+(?:opens|expands|announces|launches)\"\n    match = re.search(all_caps, text, re.IGNORECASE)\n    if match:\n        company = match.group(1).strip()\n        if len(company) >= 3 and len(company) <= 30:\n            if company.lower() not in skip_words + bad_words:\n                return company\n    \n    simple_pattern = r\"^([A-Z][a-zA-Z]+(?:\\s+[A-Z]?[a-zA-Z]+){1,3})\\s+(?:Expands?|Opens?|Announces?|Acquires?|Launches?)\"\n    match = re.search(simple_pattern, text)\n    if match:\n        company = match.group(1).strip()\n        words = company.split()\n        if len(words) >= 2 and len(words) <= 4:\n            has_skip = sum(1 for w in words if w.lower() in skip_words)\n            if has_skip < len(words):\n                return company\n    \n    return None\n\n\ndef _extract_domain_from_context(context_summary: str, raw_payload: str) -> Optional[str]:\n    \"\"\"\n    Extract domain from signal context or raw payload.\n    \n    Prioritizes:\n    1. Real article URLs from news sources (not news aggregators)\n    2. Source/publisher domains from RSS metadata\n    3. Explicit URLs in context\n    \n    Filters out news aggregator domains (news.google.com, etc.)\n    \"\"\"\n    import re\n    import json\n    \n    AGGREGATOR_DOMAINS = [\n        \"news.google.com\", \"google.com\", \"yahoo.com\", \"msn.com\", \n        \"flipboard.com\", \"feedly.com\", \"apple.news\", \"smartnews.com\"\n    ]\n    \n    PUBLISHER_TO_DOMAIN = {\n        \"miami herald\": \"miamiherald.com\",\n        \"sun sentinel\": \"sun-sentinel.com\",\n        \"south florida business journal\": \"bizjournals.com/southflorida\",\n        \"local 10\": \"local10.com\",\n        \"wsvn\": \"wsvn.com\",\n        \"nbc 6\": \"nbcmiami.com\",\n        \"palm beach post\": \"palmbeachpost.com\",\n        \"openpr\": \"openpr.com\",\n        \"pr newswire\": \"prnewswire.com\",\n        \"business wire\": \"businesswire.com\",\n        \"globenewswire\": \"globenewswire.com\",\n    }\n    \n    if raw_payload:\n        try:\n            data = json.loads(raw_payload)\n            \n            source = data.get(\"source\", \"\").lower()\n            for publisher, domain in PUBLISHER_TO_DOMAIN.items():\n                if publisher in source:\n                    return domain\n            \n            link = data.get(\"link\", \"\")\n            if link and \"news.google.com\" not in link:\n                url_match = re.search(r'https?://(?:www\\.)?([a-zA-Z0-9-]+\\.[a-zA-Z0-9.-]+)', link)\n                if url_match:\n                    domain = url_match.group(1)\n                    if not any(agg in domain for agg in AGGREGATOR_DOMAINS):\n                        return domain\n        except (json.JSONDecodeError, TypeError):\n            pass\n    \n    url_pattern = r'https?://(?:www\\.)?([a-zA-Z0-9-]+(?:\\.[a-zA-Z0-9-]+)+)'\n    domain_pattern = r'(?:www\\.)?([a-zA-Z0-9-]+\\.(?:com|io|net|org|co|biz|info))'\n    \n    for text in [context_summary, raw_payload]:\n        if not text:\n            continue\n        \n        url_match = re.search(url_pattern, text)\n        if url_match:\n            domain = url_match.group(1)\n            if not any(agg in domain for agg in AGGREGATOR_DOMAINS):\n                return domain\n        \n        domain_match = re.search(domain_pattern, text.lower())\n        if domain_match:\n            domain = domain_match.group(1)\n            if not any(agg in domain for agg in AGGREGATOR_DOMAINS):\n                return domain\n    \n    return None\n\n\ndef _has_contact_info(context_summary: str, raw_payload: str) -> bool:\n    \"\"\"\n    Check if signal contains contact information (email, phone).\n    \n    Returns True if contact info is detected, False otherwise.\n    \"\"\"\n    import re\n    \n    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n    phone_pattern = r'(?:\\+1)?[\\s.-]?\\(?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}'\n    \n    for text in [context_summary, raw_payload]:\n        if not text:\n            continue\n        \n        if re.search(email_pattern, text):\n            return True\n        if re.search(phone_pattern, text):\n            return True\n    \n    return False\n\n\ndef is_self_signal(parsed_signal: ParsedSignal, session: Session) -> Tuple[bool, Optional[str]]:\n    \"\"\"\n    Check if a signal refers to the customer's own company.\n    \n    Returns (is_self, reason) tuple.\n    If is_self is True, the signal should be ignored and no LeadEvent created.\n    \"\"\"\n    context = (parsed_signal.context_summary or \"\").lower()\n    company_mentioned = None\n    \n    from models import Customer, BusinessProfile\n    customers = session.exec(select(Customer)).all()\n    \n    for customer in customers:\n        if customer.company and customer.company.lower() in context:\n            company_mentioned = customer.company\n            break\n            \n        if customer.contact_email:\n            domain = customer.contact_email.split(\"@\")[-1].lower()\n            domain_name = domain.split(\".\")[0]\n            if domain_name in context and len(domain_name) > 3:\n                company_mentioned = customer.company\n                break\n        \n        profile = session.exec(\n            select(BusinessProfile).where(BusinessProfile.customer_id == customer.id)\n        ).first()\n        if profile:\n            if profile.short_description and customer.company and customer.company.lower() in context:\n                company_mentioned = customer.company\n                break\n    \n    if company_mentioned:\n        return True, f\"Signal mentions customer company: {company_mentioned}\"\n    return False, None\n\n\ndef _get_primary_customer(session: Session) -> Optional[int]:\n    \"\"\"\n    Get the primary active customer to assign new LeadEvents to.\n    \n    Priority:\n    1. First paid customer (subscription_status='active')\n    2. First customer with autopilot enabled\n    3. First customer overall (fallback for demos)\n    \n    Returns customer_id or None if no customers exist.\n    \"\"\"\n    paid = session.exec(\n        select(Customer).where(Customer.subscription_status == \"active\").limit(1)\n    ).first()\n    if paid:\n        return paid.id\n    \n    autopilot = session.exec(\n        select(Customer).where(Customer.autopilot_enabled == True).limit(1)\n    ).first()\n    if autopilot:\n        return autopilot.id\n    \n    first = session.exec(select(Customer).limit(1)).first()\n    if first:\n        return first.id\n    \n    return None\n\n\ndef create_lead_event_from_signal(\n    scored_signal: ScoredSignal,\n    session: Session,\n    signal: Optional[Signal] = None\n) -> Optional[LeadEvent]:\n    \"\"\"\n    Create a LeadEvent from a high-scoring signal.\n    \n    This function handles the creation of LeadEvents from signals that score\n    above the LEADEVENT_SCORE_THRESHOLD. It includes duplicate checking,\n    company/domain extraction, and proper enrichment status handling.\n    \n    Extracts contact info from signal metadata (URLs, emails, phones extracted at source).\n    Auto-assigns to the primary active customer if no company_id is specified.\n    \n    Args:\n        scored_signal: The scored signal containing parsed data and score\n        session: Database session for persistence\n        signal: Optional persisted Signal object (if already created)\n        \n    Returns:\n        LeadEvent if created successfully, None if duplicate or error\n    \"\"\"\n    parsed = scored_signal.parsed_signal\n    \n    if signal and signal.id:\n        existing = session.exec(\n            select(LeadEvent).where(LeadEvent.signal_id == signal.id)\n        ).first()\n        if existing:\n            log_signal_activity(\n                \"pipeline\",\n                \"skip_duplicate\",\n                {\"signal_id\": signal.id, \"existing_event_id\": existing.id, \"reason\": \"same_signal_id\"},\n                session=session\n            )\n            return None\n    \n    assigned_company_id = parsed.company_id\n    if not assigned_company_id:\n        assigned_company_id = _get_primary_customer(session)\n    \n    existing_by_summary = session.exec(\n        select(LeadEvent).where(\n            LeadEvent.summary == parsed.context_summary,\n            LeadEvent.company_id == assigned_company_id\n        )\n    ).first()\n    if existing_by_summary:\n        log_signal_activity(\n            \"pipeline\",\n            \"skip_duplicate\",\n            {\"existing_event_id\": existing_by_summary.id, \"reason\": \"same_summary\"},\n            session=session\n        )\n        return None\n    \n    category = parsed.category_hint or _infer_category(\n        parsed.source_type,\n        parsed.context_summary\n    )\n    \n    company_name = _extract_company_from_context(parsed.context_summary)\n    domain = _extract_domain_from_context(parsed.context_summary, parsed.raw_payload)\n    \n    lead_email = None\n    contact_info = getattr(parsed, 'extracted_contact_info', {}) or {}\n    extracted_urls = contact_info.get('extracted_urls', [])\n    extracted_emails = contact_info.get('extracted_emails', [])\n    \n    if extracted_urls and not domain:\n        from lead_enrichment import extract_domain_from_url\n        domain = extract_domain_from_url(extracted_urls[0])\n    \n    if extracted_emails:\n        lead_email = extracted_emails[0]\n    \n    enrichment_status = ENRICHMENT_STATUS_ENRICHED if lead_email else ENRICHMENT_STATUS_UNENRICHED\n    \n    recommended_action = _generate_recommended_action(category, parsed.context_summary)\n    \n    event = LeadEvent(\n        company_id=assigned_company_id,\n        lead_id=parsed.lead_id,\n        signal_id=signal.id if signal else None,\n        lead_email=lead_email,\n        lead_domain=domain,\n        lead_name=None,\n        lead_company=company_name,\n        summary=parsed.context_summary,\n        category=category,\n        urgency_score=scored_signal.score,\n        status=\"NEW\",\n        recommended_action=recommended_action,\n        enrichment_status=enrichment_status,\n        enriched_company_name=company_name,\n    )\n    \n    session.add(event)\n    session.commit()\n    session.refresh(event)\n    \n    print(f\"[SIGNALNET][LEADEVENT] Created event {event.id} from signal (score={scored_signal.score})\")\n    \n    log_signal_activity(\n        \"pipeline\",\n        \"create_event\",\n        {\n            \"event_id\": event.id,\n            \"signal_id\": signal.id if signal else None,\n            \"category\": category,\n            \"urgency_score\": scored_signal.score,\n            \"enrichment_status\": enrichment_status,\n            \"company_name\": company_name,\n            \"domain\": domain,\n        },\n        session=session\n    )\n    \n    return event\n\n\nclass SignalPipeline:\n    \"\"\"\n    Orchestrates the signal ingestion pipeline with structured logging.\n    \n    Pipeline stages:\n      1. Get eligible sources from registry\n      2. Fetch raw signals from each source (or mock in DRY_RUN mode)\n      3. Parse raw signals into standardized format\n      4. Score each signal\n      5. Persist signals to database\n      6. Generate LeadEvents for signals scoring >= 60 (PRODUCTION mode only)\n    \n    Mode behavior (via SIGNAL_MODE env var):\n      - PRODUCTION: Full pipeline including LeadEvent creation\n      - SANDBOX: Fetch, parse, score, persist signals - skip LeadEvent creation\n      - OFF: Skip signal ingestion entirely\n    \n    DRY_RUN behavior (via SIGNAL_DRY_RUN env var):\n      - When True: Sources generate mock data instead of API calls\n      - All operations are logged with [DRY_RUN] prefix\n    \n    Error handling:\n      - Each source is processed independently\n      - Sources with > 5 consecutive errors are auto-disabled\n      - Structured logging captures all actions for debugging\n    \"\"\"\n    \n    def __init__(self, registry: SignalRegistry, session: Session):\n        self.registry = registry\n        self.session = session\n        self.mode = SIGNAL_MODE\n        self.dry_run = SIGNAL_DRY_RUN\n        set_log_session(session)\n    \n    def run(self) -> Dict[str, Any]:\n        \"\"\"\n        Execute the signal pipeline.\n        \n        Returns:\n            Dict with pipeline execution results\n        \"\"\"\n        prefix = _get_dry_run_prefix()\n        \n        if self.mode == \"OFF\":\n            log_signal_activity(\n                \"pipeline\",\n                \"skip\",\n                {\"reason\": \"SIGNAL_MODE is OFF\"},\n                session=self.session\n            )\n            return {\n                \"mode\": \"OFF\",\n                \"dry_run\": self.dry_run,\n                \"skipped\": True,\n                \"signals_fetched\": 0,\n                \"signals_persisted\": 0,\n                \"events_created\": 0,\n            }\n        \n        log_signal_activity(\n            \"pipeline\",\n            \"start\",\n            {\"mode\": self.mode, \"dry_run\": self.dry_run},\n            session=self.session\n        )\n        \n        eligible_sources = self.registry.get_eligible_sources()\n        \n        if not eligible_sources:\n            log_signal_activity(\n                \"pipeline\",\n                \"no_sources\",\n                {\"reason\": \"All sources on cooldown or disabled\"},\n                session=self.session\n            )\n            return {\n                \"mode\": self.mode,\n                \"dry_run\": self.dry_run,\n                \"skipped\": False,\n                \"sources_checked\": len(self.registry.get_all_sources()),\n                \"sources_eligible\": 0,\n                \"signals_fetched\": 0,\n                \"signals_persisted\": 0,\n                \"events_created\": 0,\n            }\n        \n        results = {\n            \"mode\": self.mode,\n            \"dry_run\": self.dry_run,\n            \"skipped\": False,\n            \"sources_checked\": len(self.registry.get_all_sources()),\n            \"sources_eligible\": len(eligible_sources),\n            \"sources_run\": [],\n            \"signals_fetched\": 0,\n            \"signals_parsed\": 0,\n            \"signals_scored\": 0,\n            \"signals_persisted\": 0,\n            \"events_created\": 0,\n            \"errors\": [],\n        }\n        \n        for source in eligible_sources:\n            source_result = self._run_source(source)\n            results[\"sources_run\"].append(source_result)\n            results[\"signals_fetched\"] += source_result.get(\"fetched\", 0)\n            results[\"signals_parsed\"] += source_result.get(\"parsed\", 0)\n            results[\"signals_scored\"] += source_result.get(\"scored\", 0)\n            results[\"signals_persisted\"] += source_result.get(\"persisted\", 0)\n            results[\"events_created\"] += source_result.get(\"events_created\", 0)\n            if source_result.get(\"error\"):\n                results[\"errors\"].append({\n                    \"source\": source.name,\n                    \"error\": source_result[\"error\"],\n                })\n        \n        log_signal_activity(\n            \"pipeline\",\n            \"complete\",\n            {\n                \"signals_persisted\": results[\"signals_persisted\"],\n                \"events_created\": results[\"events_created\"],\n                \"errors_count\": len(results[\"errors\"]),\n            },\n            signal_count=results[\"signals_persisted\"],\n            session=self.session\n        )\n        \n        return results\n    \n    def _run_source(self, source: SignalSource) -> Dict[str, Any]:\n        \"\"\"Run a single source through the pipeline with structured logging.\"\"\"\n        result = {\n            \"source\": source.name,\n            \"source_type\": source.source_type,\n            \"dry_run\": source.is_dry_run,\n            \"fetched\": 0,\n            \"parsed\": 0,\n            \"scored\": 0,\n            \"persisted\": 0,\n            \"events_created\": 0,\n            \"error\": None,\n        }\n        \n        primary_customer = self.session.exec(select(Customer).where(Customer.id == 1)).first()\n        bypass_niche_filter = primary_customer is not None\n        \n        try:\n            log_signal_activity(\n                source.name,\n                \"fetch\",\n                {\"source_type\": source.source_type, \"dry_run\": source.is_dry_run},\n                session=self.session\n            )\n            \n            raw_signals = source.fetch_with_dry_run()\n            result[\"fetched\"] = len(raw_signals)\n            \n            if len(raw_signals) > source.max_items_per_run:\n                raw_signals = raw_signals[:source.max_items_per_run]\n                log_signal_activity(\n                    source.name,\n                    \"throttle\",\n                    {\"capped_at\": source.max_items_per_run, \"original\": result[\"fetched\"]},\n                    session=self.session\n                )\n            \n            for raw_signal in raw_signals:\n                try:\n                    parsed = source.parse(raw_signal)\n                    result[\"parsed\"] += 1\n                    \n                    scored = score_signal(parsed, bypass_niche_filter=bypass_niche_filter)\n                    result[\"scored\"] += 1\n                    \n                    log_signal_activity(\n                        source.name,\n                        \"score\",\n                        {\"score\": scored.score, \"should_create_event\": scored.should_create_event},\n                        session=self.session\n                    )\n                    \n                    signal = self._persist_signal(parsed, source.name)\n                    result[\"persisted\"] += 1\n                    \n                    if scored.score >= LEADEVENT_SCORE_THRESHOLD and self.mode == \"PRODUCTION\":\n                        is_self, self_reason = is_self_signal(parsed, self.session)\n                        if is_self:\n                            log_signal_activity(\n                                source.name,\n                                \"skip_self_signal\",\n                                {\"score\": scored.score, \"reason\": self_reason},\n                                session=self.session\n                            )\n                            print(f\"[SIGNALNET][SELF_SIGNAL] Skipping: {self_reason}\")\n                            continue\n                        \n                        lead_event = create_lead_event_from_signal(scored, self.session, signal)\n                        if lead_event:\n                            result[\"events_created\"] += 1\n                    elif scored.score >= LEADEVENT_SCORE_THRESHOLD and self.mode == \"SANDBOX\":\n                        log_signal_activity(\n                            source.name,\n                            \"sandbox_skip_event\",\n                            {\"score\": scored.score, \"threshold\": LEADEVENT_SCORE_THRESHOLD, \"reason\": \"SANDBOX mode\"},\n                            session=self.session\n                        )\n                    \n                except ValueError as ve:\n                    log_signal_activity(\n                        source.name,\n                        \"error\",\n                        {\"stage\": \"parse\", \"error_type\": \"ValueError\"},\n                        error=str(ve),\n                        session=self.session\n                    )\n                except TypeError as te:\n                    log_signal_activity(\n                        source.name,\n                        \"error\",\n                        {\"stage\": \"parse\", \"error_type\": \"TypeError\"},\n                        error=str(te),\n                        session=self.session\n                    )\n                except Exception as parse_err:\n                    log_signal_activity(\n                        source.name,\n                        \"error\",\n                        {\"stage\": \"parse\", \"error_type\": type(parse_err).__name__},\n                        error=str(parse_err),\n                        session=self.session\n                    )\n            \n            source.record_run(result[\"fetched\"])\n            \n            log_signal_activity(\n                source.name,\n                \"complete\",\n                {\n                    \"fetched\": result[\"fetched\"],\n                    \"parsed\": result[\"parsed\"],\n                    \"persisted\": result[\"persisted\"],\n                    \"events_created\": result[\"events_created\"],\n                },\n                signal_count=result[\"persisted\"],\n                session=self.session\n            )\n            \n        except requests.exceptions.ConnectionError as ce:\n            error_msg = f\"Connection error: {str(ce)}\"\n            result[\"error\"] = error_msg\n            source.record_run(0, error=error_msg)\n            log_signal_activity(\n                source.name,\n                \"error\",\n                {\"stage\": \"fetch\", \"error_type\": \"ConnectionError\"},\n                error=error_msg,\n                session=self.session\n            )\n        except requests.exceptions.Timeout as te:\n            error_msg = f\"Timeout error: {str(te)}\"\n            result[\"error\"] = error_msg\n            source.record_run(0, error=error_msg)\n            log_signal_activity(\n                source.name,\n                \"error\",\n                {\"stage\": \"fetch\", \"error_type\": \"Timeout\"},\n                error=error_msg,\n                session=self.session\n            )\n        except requests.exceptions.HTTPError as he:\n            error_msg = f\"HTTP error: {str(he)}\"\n            result[\"error\"] = error_msg\n            source.record_run(0, error=error_msg)\n            log_signal_activity(\n                source.name,\n                \"error\",\n                {\"stage\": \"fetch\", \"error_type\": \"HTTPError\", \"status_code\": getattr(he.response, 'status_code', None)},\n                error=error_msg,\n                session=self.session\n            )\n        except Exception as fetch_err:\n            error_msg = str(fetch_err)\n            result[\"error\"] = error_msg\n            source.record_run(0, error=error_msg)\n            log_signal_activity(\n                source.name,\n                \"error\",\n                {\"stage\": \"fetch\", \"error_type\": type(fetch_err).__name__},\n                error=error_msg,\n                session=self.session\n            )\n        \n        return result\n    \n    def _persist_signal(self, parsed: ParsedSignal, source_name: str) -> Signal:\n        \"\"\"Persist a parsed signal to the database with structured logging.\"\"\"\n        contact_info_obj = getattr(parsed, 'extracted_contact_info', None)\n        contact_info_json = json.dumps(contact_info_obj) if contact_info_obj else None\n        \n        signal = Signal(\n            company_id=parsed.company_id,\n            lead_id=parsed.lead_id,\n            source_type=parsed.source_type,\n            raw_payload=parsed.raw_payload,\n            context_summary=parsed.context_summary,\n            geography=parsed.geography,\n            extracted_contact_info=contact_info_json,\n        )\n        self.session.add(signal)\n        self.session.commit()\n        self.session.refresh(signal)\n        \n        log_signal_activity(\n            source_name,\n            \"persist\",\n            {\n                \"signal_id\": signal.id,\n                \"source_type\": parsed.source_type,\n                \"geography\": parsed.geography,\n                \"summary_preview\": parsed.context_summary[:60] if parsed.context_summary else None,\n            },\n            signal_count=1,\n            session=self.session\n        )\n        \n        return signal\n    \n    def _create_lead_event(self, signal: Signal, scored: ScoredSignal, source_name: str) -> Optional[LeadEvent]:\n        \"\"\"\n        Create a LeadEvent from a high-scoring signal with structured logging.\n        \n        This method now delegates to create_lead_event_from_signal() for proper\n        duplicate checking and enrichment status handling.\n        \n        Returns:\n            LeadEvent if created, None if duplicate or error\n        \"\"\"\n        return create_lead_event_from_signal(scored, self.session, signal)\n\n\n_global_registry = SignalRegistry()\n\n\ndef get_registry() -> SignalRegistry:\n    \"\"\"Get the global signal registry.\"\"\"\n    return _global_registry\n\n\ndef register_source(source: SignalSource) -> None:\n    \"\"\"Register a source with the global registry.\"\"\"\n    _global_registry.register(source)\n\n\ndef register_source_class(source_class: Type[SignalSource]) -> None:\n    \"\"\"Register a source class with the global registry.\"\"\"\n    _global_registry.register_class(source_class)\n\n\ndef run_signal_pipeline(session: Session) -> Dict[str, Any]:\n    \"\"\"\n    Run the signal pipeline with the global registry.\n    \n    Args:\n        session: SQLModel database session\n        \n    Returns:\n        Dict with pipeline execution results\n    \"\"\"\n    pipeline = SignalPipeline(_global_registry, session)\n    return pipeline.run()\n\n\ndef get_signal_mode() -> str:\n    \"\"\"Get current SIGNAL_MODE setting.\"\"\"\n    return SIGNAL_MODE\n\n\ndef get_signal_status() -> Dict[str, Any]:\n    \"\"\"Get comprehensive status of the SignalNet system.\"\"\"\n    return {\n        \"mode\": SIGNAL_MODE,\n        \"dry_run\": SIGNAL_DRY_RUN,\n        \"lead_geography\": LEAD_GEOGRAPHY,\n        \"lead_niche\": LEAD_NICHE,\n        \"leadevent_threshold\": LEADEVENT_SCORE_THRESHOLD,\n        \"max_consecutive_errors\": MAX_CONSECUTIVE_ERRORS,\n        \"registry\": _global_registry.get_status(),\n    }\n\n\ndef get_source_throttle_status(source_name: Optional[str] = None) -> Dict[str, Any]:\n    \"\"\"\n    Get throttle and error tracking status for sources.\n    \n    Args:\n        source_name: Optional specific source name to get status for.\n                     If None, returns status for all sources.\n    \n    Returns:\n        Dict with throttle status for requested source(s)\n    \"\"\"\n    if source_name:\n        source = _global_registry.get_source(source_name)\n        if source:\n            return source.get_throttle_status()\n        else:\n            return {\"error\": f\"Source '{source_name}' not found\"}\n    \n    sources = _global_registry.get_all_sources()\n    return {\n        \"sources\": [s.get_throttle_status() for s in sources],\n        \"auto_disabled_count\": sum(1 for s in sources if s.is_auto_disabled),\n        \"total_sources\": len(sources),\n    }\n\n\ndef reset_source(source_name: str) -> Dict[str, Any]:\n    \"\"\"\n    Reset a source's error state and re-enable if auto-disabled.\n    \n    Args:\n        source_name: Name of the source to reset\n        \n    Returns:\n        Dict with reset result\n    \"\"\"\n    source = _global_registry.get_source(source_name)\n    if not source:\n        return {\"success\": False, \"error\": f\"Source '{source_name}' not found\"}\n    \n    was_disabled = source.reset()\n    \n    return {\n        \"success\": True,\n        \"source\": source_name,\n        \"was_disabled\": was_disabled,\n        \"current_status\": source.get_status(),\n    }\n\n\ndef reset_all_sources() -> Dict[str, Any]:\n    \"\"\"\n    Reset all sources' error states and re-enable any auto-disabled sources.\n    \n    Returns:\n        Dict with reset results for all sources\n    \"\"\"\n    sources = _global_registry.get_all_sources()\n    reset_results = []\n    \n    for source in sources:\n        was_disabled = source.reset()\n        reset_results.append({\n            \"source\": source.name,\n            \"was_disabled\": was_disabled,\n        })\n    \n    return {\n        \"success\": True,\n        \"sources_reset\": len(sources),\n        \"previously_disabled\": sum(1 for r in reset_results if r[\"was_disabled\"]),\n        \"results\": reset_results,\n    }\n\n\ndef is_dry_run() -> bool:\n    \"\"\"Check if DRY_RUN mode is enabled.\"\"\"\n    return SIGNAL_DRY_RUN\n\n\nclass WeatherSignalSource(SignalSource):\n    \"\"\"\n    Weather alerts signal source for South Florida.\n    \n    Uses OpenWeatherMap API (free tier) to detect:\n    - Hurricanes and tropical storms\n    - Extreme heat (>95F) driving HVAC demand\n    - Cold fronts driving heating demand  \n    - Heavy rain/storms driving roofing/water damage demand\n    \n    API Key: OPENWEATHER_API_KEY environment variable (optional)\n    If no API key, source is disabled and logs a warning.\n    \n    DRY_RUN mode: Generates mock weather signals without API calls.\n    \"\"\"\n    \n    SOUTH_FLORIDA_LOCATIONS = [\n        {\"name\": \"Miami\", \"lat\": 25.7617, \"lon\": -80.1918},\n        {\"name\": \"Fort Lauderdale\", \"lat\": 26.1224, \"lon\": -80.1373},\n        {\"name\": \"Palm Beach\", \"lat\": 26.7056, \"lon\": -80.0364},\n    ]\n    \n    HEAT_THRESHOLD_F = 95\n    COLD_THRESHOLD_F = 50\n    HEAVY_RAIN_THRESHOLD_MM = 25\n    \n    STORM_KEYWORDS = [\n        \"hurricane\", \"tropical storm\", \"tropical depression\", \n        \"thunderstorm\", \"severe\", \"flood\", \"warning\", \"watch\"\n    ]\n    \n    @property\n    def name(self) -> str:\n        return \"weather_openweather\"\n    \n    @property\n    def source_type(self) -> str:\n        return \"weather\"\n    \n    @property\n    def enabled(self) -> bool:\n        if self.is_dry_run:\n            return SIGNAL_MODE in (\"SANDBOX\", \"PRODUCTION\")\n        if not OPENWEATHER_API_KEY:\n            return False\n        return SIGNAL_MODE in (\"SANDBOX\", \"PRODUCTION\")\n    \n    @property\n    def cooldown_seconds(self) -> int:\n        return 3600\n    \n    @property\n    def max_items_per_run(self) -> int:\n        return 15\n    \n    def _generate_mock_signals(self) -> List[RawSignal]:\n        \"\"\"Generate mock weather signals for DRY_RUN mode.\"\"\"\n        mock_events = [\n            {\n                \"event_type\": \"extreme_heat\",\n                \"temp_f\": 98,\n                \"feels_like_f\": 105,\n                \"humidity\": 75,\n                \"description\": \"Extreme heat alert: 98F (feels like 105F)\",\n                \"business_impact\": \"HVAC demand surge expected\",\n                \"niche_opportunities\": [\"HVAC\", \"pool service\", \"landscaping\"],\n            },\n            {\n                \"event_type\": \"hurricane_alert\",\n                \"alert_event\": \"Tropical Storm Warning\",\n                \"description\": \"Tropical Storm approaching South Florida coast\",\n                \"business_impact\": \"Hurricane preparation and post-storm services\",\n                \"niche_opportunities\": [\"roofing\", \"restoration\", \"generators\", \"tree service\"],\n            },\n            {\n                \"event_type\": \"heavy_rain\",\n                \"rain_mm\": 35,\n                \"description\": \"Heavy rainfall expected throughout the day\",\n                \"business_impact\": \"Roofing and water damage service demand\",\n                \"niche_opportunities\": [\"roofing\", \"water damage restoration\", \"plumbing\"],\n            },\n        ]\n        \n        signals = []\n        num_signals = random.randint(1, 3)\n        \n        for i in range(num_signals):\n            event = random.choice(mock_events)\n            location = random.choice(self.SOUTH_FLORIDA_LOCATIONS)\n            \n            signals.append(RawSignal(\n                source_name=self.name,\n                source_type=\"weather\",\n                raw_data={\n                    **event,\n                    \"location\": location[\"name\"],\n                    \"mock\": True,\n                    \"generated_at\": datetime.utcnow().isoformat(),\n                },\n                geography=location[\"name\"],\n            ))\n        \n        log_signal_activity(\n            self.name,\n            \"dry_run\",\n            {\"action\": \"generated_mock_weather\", \"count\": len(signals)},\n            signal_count=len(signals),\n            session=get_log_session()\n        )\n        \n        return signals\n    \n    def fetch(self) -> List[RawSignal]:\n        \"\"\"Fetch weather data from OpenWeatherMap for South Florida locations.\"\"\"\n        if not OPENWEATHER_API_KEY:\n            log_signal_activity(\n                self.name,\n                \"skip\",\n                {\"reason\": \"No OPENWEATHER_API_KEY set\"},\n                session=get_log_session()\n            )\n            return []\n        \n        signals = []\n        \n        for location in self.SOUTH_FLORIDA_LOCATIONS:\n            try:\n                current_data = self._fetch_current_weather(location)\n                if current_data:\n                    signals.extend(self._analyze_current_weather(current_data, location))\n                \n                alerts_data = self._fetch_weather_alerts(location)\n                if alerts_data:\n                    signals.extend(self._analyze_weather_alerts(alerts_data, location))\n                    \n                time.sleep(0.25)\n                \n            except requests.exceptions.RequestException as e:\n                log_signal_activity(\n                    self.name,\n                    \"error\",\n                    {\"stage\": \"fetch_location\", \"location\": location[\"name\"], \"error_type\": type(e).__name__},\n                    error=str(e),\n                    session=get_log_session()\n                )\n                continue\n            except Exception as e:\n                log_signal_activity(\n                    self.name,\n                    \"error\",\n                    {\"stage\": \"fetch_location\", \"location\": location[\"name\"], \"error_type\": type(e).__name__},\n                    error=str(e),\n                    session=get_log_session()\n                )\n                continue\n        \n        log_signal_activity(\n            self.name,\n            \"fetch_complete\",\n            {\"locations_checked\": len(self.SOUTH_FLORIDA_LOCATIONS), \"signals_found\": len(signals)},\n            signal_count=len(signals),\n            session=get_log_session()\n        )\n        \n        return signals\n    \n    def _fetch_current_weather(self, location: Dict) -> Optional[Dict]:\n        \"\"\"Fetch current weather for a location.\"\"\"\n        try:\n            url = \"https://api.openweathermap.org/data/2.5/weather\"\n            params = {\n                \"lat\": location[\"lat\"],\n                \"lon\": location[\"lon\"],\n                \"appid\": OPENWEATHER_API_KEY,\n                \"units\": \"imperial\"\n            }\n            response = requests.get(url, params=params, timeout=10)\n            response.raise_for_status()\n            return response.json()\n        except requests.RequestException as e:\n            print(f\"[SIGNALNET][WEATHER] Current weather API error: {e}\")\n            return None\n    \n    def _fetch_weather_alerts(self, location: Dict) -> Optional[Dict]:\n        \"\"\"Fetch weather alerts using One Call API (if available).\"\"\"\n        try:\n            url = \"https://api.openweathermap.org/data/2.5/onecall\"\n            params = {\n                \"lat\": location[\"lat\"],\n                \"lon\": location[\"lon\"],\n                \"appid\": OPENWEATHER_API_KEY,\n                \"exclude\": \"minutely,hourly,daily\",\n                \"units\": \"imperial\"\n            }\n            response = requests.get(url, params=params, timeout=10)\n            if response.status_code == 401:\n                return None\n            response.raise_for_status()\n            return response.json()\n        except requests.RequestException:\n            return None\n    \n    def _analyze_current_weather(self, data: Dict, location: Dict) -> List[RawSignal]:\n        \"\"\"Analyze current weather for business-relevant signals.\"\"\"\n        signals = []\n        \n        main = data.get(\"main\", {})\n        weather = data.get(\"weather\", [{}])[0]\n        rain = data.get(\"rain\", {})\n        \n        temp_f = main.get(\"temp\", 70)\n        feels_like_f = main.get(\"feels_like\", 70)\n        humidity = main.get(\"humidity\", 50)\n        description = weather.get(\"description\", \"\").lower()\n        weather_main = weather.get(\"main\", \"\").lower()\n        \n        if temp_f >= self.HEAT_THRESHOLD_F or feels_like_f >= self.HEAT_THRESHOLD_F:\n            signals.append(RawSignal(\n                source_name=self.name,\n                source_type=\"weather\",\n                raw_data={\n                    \"event_type\": \"extreme_heat\",\n                    \"temp_f\": temp_f,\n                    \"feels_like_f\": feels_like_f,\n                    \"humidity\": humidity,\n                    \"location\": location[\"name\"],\n                    \"description\": f\"Extreme heat alert: {temp_f}F (feels like {feels_like_f}F)\",\n                    \"business_impact\": \"HVAC demand surge expected\",\n                    \"niche_opportunities\": [\"HVAC\", \"pool service\", \"landscaping\"],\n                },\n                geography=location[\"name\"],\n            ))\n        \n        if temp_f <= self.COLD_THRESHOLD_F:\n            signals.append(RawSignal(\n                source_name=self.name,\n                source_type=\"weather\",\n                raw_data={\n                    \"event_type\": \"cold_front\",\n                    \"temp_f\": temp_f,\n                    \"feels_like_f\": feels_like_f,\n                    \"location\": location[\"name\"],\n                    \"description\": f\"Cold front: {temp_f}F (feels like {feels_like_f}F)\",\n                    \"business_impact\": \"Heating and winterization demand\",\n                    \"niche_opportunities\": [\"HVAC\", \"plumbing\", \"landscaping\"],\n                },\n                geography=location[\"name\"],\n            ))\n        \n        rain_1h = rain.get(\"1h\", 0)\n        if rain_1h >= self.HEAVY_RAIN_THRESHOLD_MM or \"heavy rain\" in description:\n            signals.append(RawSignal(\n                source_name=self.name,\n                source_type=\"weather\",\n                raw_data={\n                    \"event_type\": \"heavy_rain\",\n                    \"rain_mm\": rain_1h,\n                    \"location\": location[\"name\"],\n                    \"description\": f\"Heavy rain in {location['name']}: {rain_1h}mm/hour\",\n                    \"business_impact\": \"Roofing and water damage service demand\",\n                    \"niche_opportunities\": [\"roofing\", \"water damage restoration\", \"plumbing\"],\n                },\n                geography=location[\"name\"],\n            ))\n        \n        is_storm = any(kw in description or kw in weather_main for kw in [\"storm\", \"thunder\", \"severe\"])\n        if is_storm:\n            signals.append(RawSignal(\n                source_name=self.name,\n                source_type=\"weather\",\n                raw_data={\n                    \"event_type\": \"storm\",\n                    \"weather_main\": weather_main,\n                    \"description\": f\"Storm conditions in {location['name']}: {description}\",\n                    \"location\": location[\"name\"],\n                    \"business_impact\": \"Storm damage and emergency services demand\",\n                    \"niche_opportunities\": [\"roofing\", \"tree service\", \"restoration\"],\n                },\n                geography=location[\"name\"],\n            ))\n        \n        return signals\n    \n    def _analyze_weather_alerts(self, data: Dict, location: Dict) -> List[RawSignal]:\n        \"\"\"Analyze weather alerts for hurricane/tropical storm signals.\"\"\"\n        signals = []\n        \n        alerts = data.get(\"alerts\", [])\n        for alert in alerts:\n            event = alert.get(\"event\", \"\").lower()\n            description = alert.get(\"description\", \"\")\n            \n            is_tropical = any(kw in event for kw in [\"hurricane\", \"tropical\", \"storm warning\"])\n            \n            if is_tropical:\n                signals.append(RawSignal(\n                    source_name=self.name,\n                    source_type=\"weather\",\n                    raw_data={\n                        \"event_type\": \"hurricane_alert\",\n                        \"alert_event\": alert.get(\"event\"),\n                        \"description\": description[:500],\n                        \"sender\": alert.get(\"sender_name\"),\n                        \"location\": location[\"name\"],\n                        \"start\": alert.get(\"start\"),\n                        \"end\": alert.get(\"end\"),\n                        \"business_impact\": \"Hurricane preparation and post-storm services\",\n                        \"niche_opportunities\": [\"roofing\", \"restoration\", \"generators\", \"tree service\"],\n                    },\n                    geography=location[\"name\"],\n                ))\n        \n        return signals\n    \n    def parse(self, raw: RawSignal) -> ParsedSignal:\n        \"\"\"Parse weather signal into standardized format.\"\"\"\n        event_type = raw.raw_data.get(\"event_type\", \"weather\")\n        \n        category_map = {\n            \"hurricane_alert\": \"HURRICANE_SEASON\",\n            \"storm\": \"HURRICANE_SEASON\",\n            \"extreme_heat\": \"OPPORTUNITY\",\n            \"cold_front\": \"OPPORTUNITY\",\n            \"heavy_rain\": \"HURRICANE_SEASON\",\n        }\n        category = category_map.get(event_type, \"OPPORTUNITY\")\n        \n        niche_opportunities = raw.raw_data.get(\"niche_opportunities\", [])\n        niche_hint = niche_opportunities[0] if niche_opportunities else None\n        \n        context = raw.raw_data.get(\"description\", \"Weather event detected\")\n        business_impact = raw.raw_data.get(\"business_impact\", \"\")\n        if business_impact:\n            context = f\"{context} | Impact: {business_impact}\"\n        \n        return ParsedSignal(\n            source_type=\"weather\",\n            raw_payload=json.dumps(raw.raw_data),\n            context_summary=context,\n            geography=raw.geography,\n            category_hint=category,\n            niche_hint=niche_hint,\n        )\n\n\nclass NewsSearchSignalSource(SignalSource):\n    \"\"\"\n    Business news signal source for South Florida.\n    \n    Uses Google News RSS feeds (free, no API key required) to detect:\n    - New business openings\n    - Business expansions\n    - Commercial developments\n    - Industry news for target niches\n    \n    DRY_RUN mode: Generates mock news signals without API calls.\n    \"\"\"\n    \n    SEARCH_QUERIES = [\n        \"Miami new business opening\",\n        \"Fort Lauderdale business expansion\",\n        \"South Florida commercial development\",\n        \"Miami HVAC company\",\n        \"South Florida roofing contractor\",\n        \"Miami med spa opening\",\n        \"Broward County new business\",\n    ]\n    \n    GOOGLE_NEWS_RSS_BASE = \"https://news.google.com/rss/search\"\n    \n    @property\n    def name(self) -> str:\n        return \"news_search\"\n    \n    @property\n    def source_type(self) -> str:\n        return \"news\"\n    \n    @property\n    def enabled(self) -> bool:\n        return SIGNAL_MODE in (\"SANDBOX\", \"PRODUCTION\")\n    \n    @property\n    def cooldown_seconds(self) -> int:\n        return 7200\n    \n    @property\n    def max_items_per_run(self) -> int:\n        return 25\n    \n    def _generate_mock_signals(self) -> List[RawSignal]:\n        \"\"\"Generate mock news signals for DRY_RUN mode.\"\"\"\n        mock_articles = [\n            {\n                \"title\": \"New HVAC company opens in Coral Gables, promises 24/7 service\",\n                \"source\": \"Miami Herald\",\n                \"query\": \"Miami HVAC company\",\n                \"geography\": \"Miami\",\n            },\n            {\n                \"title\": \"South Florida roofing contractor expands operations after hurricane season\",\n                \"source\": \"Sun Sentinel\",\n                \"query\": \"South Florida roofing contractor\",\n                \"geography\": \"Fort Lauderdale\",\n            },\n            {\n                \"title\": \"Med spa franchise opening 3 new locations in Broward County\",\n                \"source\": \"Brickell Magazine\",\n                \"query\": \"Miami med spa opening\",\n                \"geography\": \"Fort Lauderdale\",\n            },\n            {\n                \"title\": \"New commercial development project approved for downtown Miami\",\n                \"source\": \"Miami Today\",\n                \"query\": \"South Florida commercial development\",\n                \"geography\": \"Miami\",\n            },\n        ]\n        \n        signals = []\n        num_signals = random.randint(2, 4)\n        \n        for i in range(num_signals):\n            article = random.choice(mock_articles)\n            \n            signals.append(RawSignal(\n                source_name=self.name,\n                source_type=\"news\",\n                raw_data={\n                    \"title\": article[\"title\"],\n                    \"link\": f\"https://example.com/mock-news-{i}\",\n                    \"published\": datetime.utcnow().isoformat(),\n                    \"source\": article[\"source\"],\n                    \"query\": article[\"query\"],\n                    \"mock\": True,\n                },\n                geography=article[\"geography\"],\n            ))\n        \n        log_signal_activity(\n            self.name,\n            \"dry_run\",\n            {\"action\": \"generated_mock_news\", \"count\": len(signals)},\n            signal_count=len(signals),\n            session=get_log_session()\n        )\n        \n        return signals\n    \n    def fetch(self) -> List[RawSignal]:\n        \"\"\"Fetch news from Google News RSS feeds.\"\"\"\n        signals = []\n        \n        for query in self.SEARCH_QUERIES:\n            try:\n                articles = self._fetch_google_news_rss(query)\n                for article in articles[:5]:\n                    signals.append(RawSignal(\n                        source_name=self.name,\n                        source_type=\"news\",\n                        raw_data={\n                            \"title\": article.get(\"title\", \"\"),\n                            \"link\": article.get(\"link\", \"\"),\n                            \"published\": article.get(\"published\", \"\"),\n                            \"source\": article.get(\"source\", \"\"),\n                            \"query\": query,\n                        },\n                        geography=self._extract_geography(article.get(\"title\", \"\") + \" \" + query),\n                    ))\n                \n                time.sleep(0.5)\n                \n            except requests.exceptions.RequestException as e:\n                log_signal_activity(\n                    self.name,\n                    \"error\",\n                    {\"stage\": \"fetch_query\", \"query\": query, \"error_type\": type(e).__name__},\n                    error=str(e),\n                    session=get_log_session()\n                )\n                continue\n            except Exception as e:\n                log_signal_activity(\n                    self.name,\n                    \"error\",\n                    {\"stage\": \"fetch_query\", \"query\": query, \"error_type\": type(e).__name__},\n                    error=str(e),\n                    session=get_log_session()\n                )\n                continue\n        \n        log_signal_activity(\n            self.name,\n            \"fetch_complete\",\n            {\"queries_checked\": len(self.SEARCH_QUERIES), \"signals_found\": len(signals)},\n            signal_count=len(signals),\n            session=get_log_session()\n        )\n        \n        return signals\n    \n    def _fetch_google_news_rss(self, query: str) -> List[Dict]:\n        \"\"\"Fetch news articles from Google News RSS feed.\"\"\"\n        import urllib.parse\n        import xml.etree.ElementTree as ET\n        \n        try:\n            encoded_query = urllib.parse.quote(query)\n            url = f\"{self.GOOGLE_NEWS_RSS_BASE}?q={encoded_query}&hl=en-US&gl=US&ceid=US:en\"\n            \n            headers = {\n                \"User-Agent\": \"Mozilla/5.0 (compatible; HossAgent/1.0; +https://hossagent.net)\"\n            }\n            \n            response = requests.get(url, headers=headers, timeout=15)\n            response.raise_for_status()\n            \n            root = ET.fromstring(response.content)\n            \n            articles = []\n            for item in root.findall(\".//item\"):\n                title = item.find(\"title\")\n                link = item.find(\"link\")\n                pub_date = item.find(\"pubDate\")\n                source = item.find(\"source\")\n                \n                articles.append({\n                    \"title\": title.text if title is not None else \"\",\n                    \"link\": link.text if link is not None else \"\",\n                    \"published\": pub_date.text if pub_date is not None else \"\",\n                    \"source\": source.text if source is not None else \"\",\n                })\n            \n            return articles\n            \n        except Exception as e:\n            print(f\"[SIGNALNET][NEWS] RSS parse error: {e}\")\n            return []\n    \n    def _extract_geography(self, text: str) -> Optional[str]:\n        \"\"\"Extract geography from text.\"\"\"\n        text_lower = text.lower()\n        \n        if \"miami\" in text_lower or \"dade\" in text_lower:\n            return \"Miami\"\n        elif \"fort lauderdale\" in text_lower or \"broward\" in text_lower:\n            return \"Fort Lauderdale\"\n        elif \"palm beach\" in text_lower:\n            return \"Palm Beach\"\n        elif \"south florida\" in text_lower:\n            return \"South Florida\"\n        else:\n            return \"South Florida\"\n    \n    def _infer_category(self, title: str, query: str) -> str:\n        \"\"\"Infer signal category from news content.\"\"\"\n        text_lower = (title + \" \" + query).lower()\n        \n        if any(kw in text_lower for kw in [\"opening\", \"new business\", \"launches\", \"expands\"]):\n            return \"GROWTH_SIGNAL\"\n        elif any(kw in text_lower for kw in [\"competitor\", \"rivalry\", \"market share\"]):\n            return \"COMPETITOR_SHIFT\"\n        elif any(kw in text_lower for kw in [\"development\", \"construction\", \"project\"]):\n            return \"GROWTH_SIGNAL\"\n        else:\n            return \"OPPORTUNITY\"\n    \n    def _infer_niche(self, title: str, query: str) -> Optional[str]:\n        \"\"\"Infer niche from news content.\"\"\"\n        text_lower = (title + \" \" + query).lower()\n        \n        niche_keywords = {\n            \"hvac\": [\"hvac\", \"air conditioning\", \"heating\", \"cooling\"],\n            \"roofing\": [\"roof\", \"roofing\", \"roofer\"],\n            \"med spa\": [\"med spa\", \"medspa\", \"medical spa\", \"aesthetics\", \"botox\"],\n            \"plumbing\": [\"plumb\", \"plumber\", \"plumbing\"],\n            \"landscaping\": [\"landscape\", \"landscaping\", \"lawn\"],\n            \"restaurant\": [\"restaurant\", \"dining\", \"food service\"],\n            \"legal\": [\"attorney\", \"lawyer\", \"law firm\", \"legal\"],\n        }\n        \n        for niche, keywords in niche_keywords.items():\n            if any(kw in text_lower for kw in keywords):\n                return niche\n        \n        return None\n    \n    def parse(self, raw: RawSignal) -> ParsedSignal:\n        \"\"\"Parse news signal into standardized format and extract contact info.\"\"\"\n        import re\n        \n        title = raw.raw_data.get(\"title\", \"News article\")\n        query = raw.raw_data.get(\"query\", \"\")\n        source = raw.raw_data.get(\"source\", \"Unknown\")\n        link = raw.raw_data.get(\"link\", \"\")\n        full_text = f\"{title} {query} {source} {link}\"\n        \n        category = self._infer_category(title, query)\n        niche = self._infer_niche(title, query)\n        \n        context = f\"News: {title}\"\n        if source:\n            context = f\"{context} (via {source})\"\n        \n        extracted_urls = []\n        extracted_emails = []\n        extracted_phones = []\n        \n        url_pattern = r'https?://[^\\s<>\"{}|\\\\^`\\[\\]]+'\n        urls = re.findall(url_pattern, full_text)\n        extracted_urls = list(set([u.rstrip('.,;:)') for u in urls if u]))[:5]\n        \n        email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n        emails = re.findall(email_pattern, full_text)\n        extracted_emails = list(set([e.lower() for e in emails if '@' in e]))[:3]\n        \n        phone_pattern = r'(?:\\+?1[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}'\n        phones = re.findall(phone_pattern, full_text)\n        extracted_phones = list(set(phones))[:2]\n        \n        metadata = {\n            \"extracted_urls\": extracted_urls,\n            \"extracted_emails\": extracted_emails,\n            \"extracted_phones\": extracted_phones,\n            \"source_confidence\": 0.85 if extracted_urls or extracted_emails else 0.5,\n        }\n        \n        parsed = ParsedSignal(\n            source_type=\"news\",\n            raw_payload=json.dumps(raw.raw_data),\n            context_summary=context[:500],\n            geography=raw.geography,\n            category_hint=category,\n            niche_hint=niche,\n        )\n        parsed.extracted_contact_info = metadata\n        \n        return parsed\n\n\nclass RedditSignalSource(SignalSource):\n    \"\"\"\n    Reddit signal source for South Florida local business discussions.\n    \n    Uses Reddit's public JSON API (no authentication required) to monitor:\n    - r/Miami, r/FortLauderdale, r/southflorida subreddits\n    - Service recommendation requests\n    - Business-related discussions\n    - \"Looking for\" and \"need help with\" posts\n    \n    Note: Reddit may block automated requests (403 errors). The source\n    auto-disables after repeated failures to avoid noisy logs.\n    \n    DRY_RUN mode: Generates mock Reddit posts without API calls.\n    \"\"\"\n    \n    SUBREDDITS = [\"Miami\", \"FortLauderdale\", \"southflorida\"]\n    \n    _blocked = False\n    _consecutive_failures = 0\n    \n    SERVICE_KEYWORDS = [\n        \"recommend\", \"recommendation\", \"looking for\",\n        \"need help\", \"anyone know\", \"best\", \"who do you use\",\n        \"contractor\", \"plumber\", \"hvac\", \"ac\", \"air conditioning\",\n        \"roofer\", \"roofing\", \"lawyer\", \"attorney\", \"doctor\",\n        \"mechanic\", \"electrician\", \"handyman\", \"moving company\",\n    ]\n    \n    REDDIT_BASE_URL = \"https://www.reddit.com\"\n    \n    @property\n    def name(self) -> str:\n        return \"reddit_local\"\n    \n    @property\n    def source_type(self) -> str:\n        return \"social\"\n    \n    @property\n    def enabled(self) -> bool:\n        if self.is_dry_run:\n            return SIGNAL_MODE in (\"SANDBOX\", \"PRODUCTION\")\n        if RedditSignalSource._blocked:\n            return False\n        return SIGNAL_MODE in (\"SANDBOX\", \"PRODUCTION\")\n    \n    def _generate_mock_signals(self) -> List[RawSignal]:\n        \"\"\"Generate mock Reddit signals for DRY_RUN mode.\"\"\"\n        mock_posts = [\n            {\n                \"title\": \"Looking for a reliable HVAC company in Miami - AC stopped working\",\n                \"selftext\": \"My AC unit is making weird noises and isn't cooling. Anyone know a good, honest HVAC technician in the Miami area?\",\n                \"subreddit\": \"Miami\",\n                \"score\": 15,\n                \"num_comments\": 23,\n            },\n            {\n                \"title\": \"Recommend a good roofing contractor in Fort Lauderdale?\",\n                \"selftext\": \"Need some roof repairs after the last storm. Looking for recommendations for a licensed roofer.\",\n                \"subreddit\": \"FortLauderdale\",\n                \"score\": 8,\n                \"num_comments\": 12,\n            },\n            {\n                \"title\": \"Best immigration attorney in South Florida?\",\n                \"selftext\": \"Looking for an experienced immigration lawyer. Need help with visa process. Any recommendations?\",\n                \"subreddit\": \"southflorida\",\n                \"score\": 22,\n                \"num_comments\": 45,\n            },\n            {\n                \"title\": \"Need help finding a plumber in Brickell area\",\n                \"selftext\": \"Have a leak under my kitchen sink. Anyone know a good plumber who does same-day service?\",\n                \"subreddit\": \"Miami\",\n                \"score\": 5,\n                \"num_comments\": 8,\n            },\n        ]\n        \n        signals = []\n        num_signals = random.randint(2, 4)\n        \n        for i in range(num_signals):\n            post = random.choice(mock_posts)\n            geography = self._subreddit_to_geography(post[\"subreddit\"])\n            \n            signals.append(RawSignal(\n                source_name=self.name,\n                source_type=\"social\",\n                raw_data={\n                    \"title\": post[\"title\"],\n                    \"selftext\": post[\"selftext\"],\n                    \"subreddit\": post[\"subreddit\"],\n                    \"author\": f\"mock_user_{i}\",\n                    \"score\": post[\"score\"],\n                    \"num_comments\": post[\"num_comments\"],\n                    \"permalink\": f\"/r/{post['subreddit']}/comments/mock{i}/\",\n                    \"created_utc\": datetime.utcnow().timestamp(),\n                    \"url\": f\"https://www.reddit.com/r/{post['subreddit']}/comments/mock{i}/\",\n                    \"mock\": True,\n                },\n                geography=geography,\n            ))\n        \n        log_signal_activity(\n            self.name,\n            \"dry_run\",\n            {\"action\": \"generated_mock_reddit\", \"count\": len(signals)},\n            signal_count=len(signals),\n            session=get_log_session()\n        )\n        \n        return signals\n    \n    @property\n    def cooldown_seconds(self) -> int:\n        return 3600\n    \n    @property\n    def max_items_per_run(self) -> int:\n        return 30\n    \n    def fetch(self) -> List[RawSignal]:\n        \"\"\"Fetch relevant posts from South Florida subreddits.\"\"\"\n        if RedditSignalSource._blocked:\n            print(\"[SIGNALNET][REDDIT] Source auto-disabled due to API blocking (403)\")\n            return []\n        \n        signals = []\n        blocked_count = 0\n        \n        for subreddit in self.SUBREDDITS:\n            try:\n                posts = self._fetch_subreddit_posts(subreddit)\n                for post in posts:\n                    if self._is_relevant_post(post):\n                        geography = self._subreddit_to_geography(subreddit)\n                        signals.append(RawSignal(\n                            source_name=self.name,\n                            source_type=\"social\",\n                            raw_data={\n                                \"title\": post.get(\"title\", \"\"),\n                                \"selftext\": post.get(\"selftext\", \"\")[:500],\n                                \"subreddit\": subreddit,\n                                \"author\": post.get(\"author\", \"\"),\n                                \"score\": post.get(\"score\", 0),\n                                \"num_comments\": post.get(\"num_comments\", 0),\n                                \"permalink\": post.get(\"permalink\", \"\"),\n                                \"created_utc\": post.get(\"created_utc\", 0),\n                                \"url\": f\"{self.REDDIT_BASE_URL}{post.get('permalink', '')}\",\n                            },\n                            geography=geography,\n                        ))\n                \n                time.sleep(2)\n                \n            except requests.HTTPError as e:\n                if e.response is not None and e.response.status_code == 403:\n                    blocked_count += 1\n                    print(f\"[SIGNALNET][REDDIT] Blocked by Reddit (403) for r/{subreddit}\")\n                else:\n                    print(f\"[SIGNALNET][REDDIT] HTTP error for r/{subreddit}: {e}\")\n                continue\n            except Exception as e:\n                print(f\"[SIGNALNET][REDDIT] Error fetching r/{subreddit}: {e}\")\n                continue\n        \n        if blocked_count >= len(self.SUBREDDITS):\n            RedditSignalSource._blocked = True\n            RedditSignalSource._consecutive_failures += 1\n            print(f\"[SIGNALNET][REDDIT] All subreddits blocked - source auto-disabled\")\n        else:\n            RedditSignalSource._consecutive_failures = 0\n        \n        print(f\"[SIGNALNET][REDDIT] Fetched {len(signals)} relevant posts from {len(self.SUBREDDITS)} subreddits\")\n        return signals\n    \n    def _fetch_subreddit_posts(self, subreddit: str, limit: int = 50) -> List[Dict]:\n        \"\"\"Fetch recent posts from a subreddit using public JSON API.\"\"\"\n        url = f\"{self.REDDIT_BASE_URL}/r/{subreddit}/new.json\"\n        params = {\"limit\": limit}\n        headers = {\n            \"User-Agent\": \"HossAgent/1.0 (Business Signal Detection; +https://hossagent.net)\"\n        }\n        \n        response = requests.get(url, params=params, headers=headers, timeout=15)\n        \n        if response.status_code == 403:\n            print(f\"[SIGNALNET][REDDIT] Blocked by Reddit (403) for r/{subreddit}\")\n            raise requests.HTTPError(\"403 Blocked\", response=response)\n        \n        response.raise_for_status()\n        \n        data = response.json()\n        posts = []\n        \n        for child in data.get(\"data\", {}).get(\"children\", []):\n            posts.append(child.get(\"data\", {}))\n        \n        return posts\n    \n    def _is_relevant_post(self, post: Dict) -> bool:\n        \"\"\"Check if a post is relevant for business signals.\"\"\"\n        title = post.get(\"title\", \"\").lower()\n        selftext = post.get(\"selftext\", \"\").lower()\n        content = title + \" \" + selftext\n        \n        return any(keyword in content for keyword in self.SERVICE_KEYWORDS)\n    \n    def _subreddit_to_geography(self, subreddit: str) -> str:\n        \"\"\"Map subreddit to geography.\"\"\"\n        mapping = {\n            \"Miami\": \"Miami\",\n            \"FortLauderdale\": \"Fort Lauderdale\",\n            \"southflorida\": \"South Florida\",\n        }\n        return mapping.get(subreddit, \"South Florida\")\n    \n    def _extract_niche(self, title: str, selftext: str) -> Optional[str]:\n        \"\"\"Extract potential business niche from post content.\"\"\"\n        content = (title + \" \" + selftext).lower()\n        \n        niche_patterns = {\n            \"HVAC\": [\"hvac\", \"ac \", \"a/c\", \"air conditioning\", \"heating\", \"cooling\"],\n            \"Roofing\": [\"roof\", \"roofing\", \"roofer\", \"shingles\"],\n            \"Plumbing\": [\"plumber\", \"plumbing\", \"pipe\", \"drain\", \"water heater\"],\n            \"Electrical\": [\"electrician\", \"electrical\", \"wiring\"],\n            \"Legal\": [\"lawyer\", \"attorney\", \"legal\"],\n            \"Medical\": [\"doctor\", \"clinic\", \"medical\", \"dentist\"],\n            \"Automotive\": [\"mechanic\", \"auto\", \"car repair\"],\n            \"Home Services\": [\"handyman\", \"contractor\", \"renovation\", \"remodel\"],\n            \"Moving\": [\"moving company\", \"movers\", \"relocation\"],\n        }\n        \n        for niche, keywords in niche_patterns.items():\n            if any(kw in content for kw in keywords):\n                return niche\n        \n        return None\n    \n    def _infer_category(self, post: Dict) -> str:\n        \"\"\"Infer signal category from post content.\"\"\"\n        title = post.get(\"title\", \"\").lower()\n        selftext = post.get(\"selftext\", \"\").lower()\n        content = title + \" \" + selftext\n        \n        if any(kw in content for kw in [\"recommend\", \"looking for\", \"anyone know\", \"who do you use\"]):\n            return \"OPPORTUNITY\"\n        elif any(kw in content for kw in [\"need help\", \"urgent\", \"emergency\"]):\n            return \"OPPORTUNITY\"\n        elif any(kw in content for kw in [\"new business\", \"opening\", \"just opened\"]):\n            return \"GROWTH_SIGNAL\"\n        else:\n            return \"OPPORTUNITY\"\n    \n    def parse(self, raw: RawSignal) -> ParsedSignal:\n        \"\"\"Parse Reddit post into standardized format.\"\"\"\n        title = raw.raw_data.get(\"title\", \"Reddit post\")\n        selftext = raw.raw_data.get(\"selftext\", \"\")\n        subreddit = raw.raw_data.get(\"subreddit\", \"\")\n        score = raw.raw_data.get(\"score\", 0)\n        num_comments = raw.raw_data.get(\"num_comments\", 0)\n        \n        category = self._infer_category(raw.raw_data)\n        niche = self._extract_niche(title, selftext)\n        \n        context = f\"Reddit r/{subreddit}: {title}\"\n        engagement = f\"(Score: {score}, Comments: {num_comments})\"\n        context = f\"{context} {engagement}\"\n        \n        return ParsedSignal(\n            source_type=\"social\",\n            raw_payload=json.dumps(raw.raw_data),\n            context_summary=context[:500],\n            geography=raw.geography,\n            category_hint=category,\n            niche_hint=niche,\n        )\n\n\nclass JobBoardSignalSource(SignalSource):\n    \"\"\"\n    Job Board signal source for SMB hiring signals (EPIC 3.2).\n    \n    Ingests job postings from public job boards to identify SMBs that are hiring.\n    Hiring signals indicate business growth and potential need for B2B services.\n    \"\"\"\n    \n    name = \"job_board\"\n    source_type = \"job_board\"\n    \n    NICHES = [\"hvac\", \"plumbing\", \"roofing\", \"electrical\", \"landscaping\"]\n    REGIONS = [\"miami\", \"fort_lauderdale\", \"west_palm\"]\n    \n    def is_enabled(self) -> bool:\n        \"\"\"Enable in both SANDBOX and PRODUCTION.\"\"\"\n        return SIGNAL_MODE in (\"SANDBOX\", \"PRODUCTION\")\n    \n    def fetch(self) -> List[RawSignal]:\n        \"\"\"Fetch job postings from job boards.\"\"\"\n        try:\n            from job_board_connector import fetch_job_postings, REGIONS as JB_REGIONS\n            \n            print(f\"[SIGNALNET][JOB_BOARD][FETCH] Starting job board search\")\n            \n            jobs = fetch_job_postings(\n                niches=self.NICHES[:3],\n                regions=self.REGIONS,\n                max_per_niche=3\n            )\n            \n            signals = []\n            for job in jobs:\n                geography = job.geography or \"South Florida\"\n                niche = job.niche or \"\"\n                \n                raw_signal = RawSignal(\n                    source_name=self.name,\n                    source_type=self.source_type,\n                    geography=geography,\n                    raw_data={\n                        \"title\": job.title,\n                        \"company_name\": job.company_name,\n                        \"location\": job.location,\n                        \"description\": job.description or \"\",\n                        \"url\": job.url or \"\",\n                        \"posted_date\": job.posted_date or \"\",\n                        \"niche\": niche,\n                        \"geography\": geography,\n                        \"source\": job.source,\n                    }\n                )\n                signals.append(raw_signal)\n            \n            print(f\"[SIGNALNET][JOB_BOARD][FETCH_COMPLETE] Found {len(signals)} job postings\")\n            return signals\n            \n        except Exception as e:\n            print(f\"[SIGNALNET][JOB_BOARD][ERROR] Fetch failed: {str(e)}\")\n            return []\n    \n    def parse(self, raw: RawSignal) -> ParsedSignal:\n        \"\"\"Parse job posting into standardized format.\"\"\"\n        data = raw.raw_data\n        company_name = data.get(\"company_name\", \"Unknown Company\")\n        title = data.get(\"title\", \"Job Posting\")\n        location = data.get(\"location\", \"South Florida\")\n        niche = data.get(\"niche\", \"trades\")\n        \n        summary = f\"{company_name} is hiring: {title} in {location}\"\n        if niche:\n            summary += f\" ({niche.upper()} sector)\"\n        \n        return ParsedSignal(\n            source_type=\"job_board\",\n            raw_payload=json.dumps(data),\n            context_summary=summary[:500],\n            geography=raw.geography,\n            category_hint=\"JOB_POSTING\",\n            niche_hint=niche,\n        )\n\n\nregister_source_class(WeatherSignalSource)\nregister_source_class(NewsSearchSignalSource)\nregister_source_class(RedditSignalSource)\nregister_source_class(JobBoardSignalSource)\n\nprint(\"[SIGNALNET][STARTUP] Signal sources registered: weather_openweather, news_search, reddit_local, job_board\")\n","path":null,"size_bytes":98674,"size_tokens":null},"conversation_engine.py":{"content":"\"\"\"\nHossAgent Conversation Engine\n\nThe full inbound-outbound loop where strangers become leads, leads become\nconversations, and conversations become revenue.\n\nComponents:\n- Inbound email parsing and storage\n- Thread management (message linking, status tracking)\n- Draft reply generation with LLM\n- Guardrails engine (pricing, scheduling, sensitive content detection)\n- Suppression/opt-out handling\n- Human-in-the-loop priority enforcement\n- Conversation metrics tracking\n\nEnvironment Variables:\n- INBOUND_EMAIL_SECRET: Secret token for inbound webhook validation\n- AUTO_REPLY_LEVEL: NONE | SAFE_ONLY | AGGRESSIVE (default: SAFE_ONLY)\n- OPENAI_API_KEY: Required for AI draft generation\n\"\"\"\n\nimport os\nimport re\nimport json\nimport hashlib\nfrom datetime import datetime, timedelta\nfrom typing import Optional, List, Dict, Any, Tuple\nfrom dataclasses import dataclass, field, asdict\nfrom sqlmodel import Session, select\n\nfrom models import (\n    Customer, Lead, LeadEvent, BusinessProfile, Thread, Message, Suppression,\n    ConversationMetrics, OPT_OUT_PHRASES,\n    THREAD_STATUS_OPEN, THREAD_STATUS_HUMAN_OWNED, THREAD_STATUS_AUTO, THREAD_STATUS_CLOSED,\n    MESSAGE_DIRECTION_INBOUND, MESSAGE_DIRECTION_OUTBOUND,\n    MESSAGE_STATUS_QUEUED, MESSAGE_STATUS_SENT, MESSAGE_STATUS_DRAFT, MESSAGE_STATUS_FAILED, MESSAGE_STATUS_APPROVED,\n    MESSAGE_GENERATED_AI, MESSAGE_GENERATED_HUMAN, MESSAGE_GENERATED_SYSTEM\n)\nfrom email_utils import send_email, get_sendgrid_config, EmailResult\n\n\nINBOUND_EMAIL_SECRET = os.getenv(\"INBOUND_EMAIL_SECRET\", \"\")\nAUTO_REPLY_LEVEL = os.getenv(\"AUTO_REPLY_LEVEL\", \"SAFE_ONLY\").upper()\n\n\n@dataclass\nclass InboundEmailData:\n    \"\"\"Parsed inbound email data from SendGrid webhook.\"\"\"\n    from_email: str\n    to_email: str\n    cc: Optional[str] = None\n    subject: str = \"\"\n    body_text: str = \"\"\n    body_html: Optional[str] = None\n    message_id: Optional[str] = None\n    in_reply_to: Optional[str] = None\n    references: Optional[str] = None\n    received_at: datetime = field(default_factory=datetime.utcnow)\n    raw_headers: Optional[Dict[str, Any]] = None\n\n\n@dataclass \nclass GuardrailResult:\n    \"\"\"Result of guardrail check on proposed reply.\"\"\"\n    passed: bool\n    flags: List[str] = field(default_factory=list)\n    details: Dict[str, str] = field(default_factory=dict)\n    auto_send_allowed: bool = False\n\n\nPRICING_PATTERNS = [\n    r'\\$\\d+',\n    r'\\d+\\s*(?:dollars|bucks)',\n    r'(?:price|cost|fee|rate|quote)\\s*(?:is|of|:)?\\s*\\$?\\d+',\n    r'(?:charge|bill)\\s*you\\s*\\$?\\d+',\n    r'(?:discount|off|savings?)\\s*(?:of)?\\s*\\d+%?',\n    r'(?:free|no\\s*(?:charge|cost))',\n    r'(?:per\\s*hour|hourly\\s*rate)',\n]\n\nSCHEDULING_PATTERNS = [\n    r'(?:monday|tuesday|wednesday|thursday|friday|saturday|sunday)\\s+at\\s+\\d+',\n    r'(?:tomorrow|today)\\s+at\\s+\\d+',\n    r'\\d{1,2}[:/]\\d{2}\\s*(?:am|pm)?',\n    r'(?:schedule|book|set\\s*up)\\s*(?:a|an)?\\s*(?:meeting|call|appointment)\\s*(?:for|on)',\n    r\"(?:i'll|I will|we'll|we will)\\s*(?:come|be there|arrive|visit)\\s*(?:on|at)\",\n    r'(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\\w*\\s+\\d{1,2}',\n]\n\nCOMMITMENT_PATTERNS = [\n    r\"(?:i|we)\\s*(?:'ll|will|can|shall)\\s*(?:definitely|certainly|absolutely)\\s*(?:do|provide|deliver|complete)\",\n    r'(?:guaranteed|guarantee|promise|committed)',\n    r\"(?:i|we)\\s*(?:'ll|will)\\s*(?:beat|match)\\s*(?:their|any)\\s*(?:price|quote|offer)\",\n    r'(?:lower|reduce|drop)\\s*(?:the|our)?\\s*price',\n]\n\nSENSITIVE_PATTERNS = [\n    r'(?:legal|lawsuit|sue|court|attorney|lawyer)',\n    r'(?:medical|diagnosis|prescription|treatment|doctor)',\n    r'(?:insurance|liability|claim)',\n]\n\nSAFE_REPLY_PATTERNS = [\n    r'^(?:thanks|thank you|got it|received|understood)',\n    r'(?:here\\'?s?\\s*(?:a|some|more)?\\s*(?:info|information|details|link))',\n    r'(?:let me know|feel free to|please reach out)',\n    r'(?:following up|checking in|just wanted to)',\n]\n\n\ndef validate_inbound_secret(provided_secret: str) -> bool:\n    \"\"\"Validate inbound webhook secret token.\"\"\"\n    if not INBOUND_EMAIL_SECRET:\n        print(\"[CONVERSATION][WARNING] INBOUND_EMAIL_SECRET not configured - webhook validation disabled\")\n        return True\n    return provided_secret == INBOUND_EMAIL_SECRET\n\n\ndef parse_sendgrid_inbound(request_data: Dict[str, Any]) -> InboundEmailData:\n    \"\"\"\n    Parse SendGrid Inbound Parse webhook data.\n    \n    SendGrid sends multipart form data with fields like:\n    - from, to, cc, subject, text, html\n    - headers (JSON), envelope (JSON)\n    - attachments, attachment-info (if any)\n    \"\"\"\n    from_email = request_data.get(\"from\", \"\")\n    if \"<\" in from_email and \">\" in from_email:\n        match = re.search(r'<([^>]+)>', from_email)\n        if match:\n            from_email = match.group(1)\n    \n    to_email = request_data.get(\"to\", \"\")\n    if \"<\" in to_email and \">\" in to_email:\n        match = re.search(r'<([^>]+)>', to_email)\n        if match:\n            to_email = match.group(1)\n    \n    headers_str = request_data.get(\"headers\", \"{}\")\n    try:\n        headers = json.loads(headers_str) if isinstance(headers_str, str) else headers_str\n    except json.JSONDecodeError:\n        headers = {}\n    \n    message_id = None\n    in_reply_to = None\n    references = None\n    \n    if isinstance(headers, dict):\n        message_id = headers.get(\"Message-ID\") or headers.get(\"Message-Id\") or headers.get(\"message-id\")\n        in_reply_to = headers.get(\"In-Reply-To\") or headers.get(\"in-reply-to\")\n        references = headers.get(\"References\") or headers.get(\"references\")\n    elif isinstance(headers, str):\n        mid_match = re.search(r'Message-I[dD]:\\s*<?([^>\\s]+)>?', headers)\n        if mid_match:\n            message_id = mid_match.group(1)\n        irt_match = re.search(r'In-Reply-To:\\s*<?([^>\\s]+)>?', headers)\n        if irt_match:\n            in_reply_to = irt_match.group(1)\n    \n    return InboundEmailData(\n        from_email=from_email.strip().lower(),\n        to_email=to_email.strip().lower(),\n        cc=request_data.get(\"cc\", \"\"),\n        subject=request_data.get(\"subject\", \"(No Subject)\"),\n        body_text=request_data.get(\"text\", \"\"),\n        body_html=request_data.get(\"html\"),\n        message_id=message_id,\n        in_reply_to=in_reply_to,\n        references=references,\n        received_at=datetime.utcnow(),\n        raw_headers=headers if isinstance(headers, dict) else None\n    )\n\n\ndef find_thread_by_message_id(session: Session, in_reply_to: str) -> Optional[Thread]:\n    \"\"\"Find thread by matching in_reply_to to an existing message's message_id.\"\"\"\n    if not in_reply_to:\n        return None\n    \n    existing_msg = session.exec(\n        select(Message).where(Message.message_id == in_reply_to)\n    ).first()\n    \n    if existing_msg and existing_msg.thread_id:\n        thread = session.exec(\n            select(Thread).where(Thread.id == existing_msg.thread_id)\n        ).first()\n        return thread\n    \n    return None\n\n\ndef find_thread_by_email(session: Session, lead_email: str, customer_id: int) -> Optional[Thread]:\n    \"\"\"Find existing thread by lead email and customer.\"\"\"\n    thread = session.exec(\n        select(Thread).where(\n            Thread.lead_email == lead_email.lower(),\n            Thread.customer_id == customer_id,\n            Thread.status != THREAD_STATUS_CLOSED\n        ).order_by(Thread.updated_at.desc())\n    ).first()\n    return thread\n\n\ndef find_customer_by_email(session: Session, email: str) -> Optional[Customer]:\n    \"\"\"Find customer by contact email.\"\"\"\n    return session.exec(\n        select(Customer).where(Customer.contact_email == email.lower())\n    ).first()\n\n\ndef find_lead_event_by_email(session: Session, email: str, customer_id: int = None) -> Optional[LeadEvent]:\n    \"\"\"Find LeadEvent by lead email.\"\"\"\n    query = select(LeadEvent).where(\n        (LeadEvent.lead_email == email.lower()) | \n        (LeadEvent.enriched_email == email.lower())\n    )\n    if customer_id:\n        query = query.where(LeadEvent.company_id == customer_id)\n    return session.exec(query.order_by(LeadEvent.created_at.desc())).first()\n\n\ndef detect_opt_out(body_text: str) -> bool:\n    \"\"\"Check if message body contains opt-out language.\"\"\"\n    body_lower = body_text.lower()\n    for phrase in OPT_OUT_PHRASES:\n        if phrase in body_lower:\n            return True\n    return False\n\n\ndef check_suppression(session: Session, email: str, customer_id: int = None) -> bool:\n    \"\"\"Check if email is suppressed for this customer or globally.\"\"\"\n    email_lower = email.lower()\n    domain = email_lower.split(\"@\")[1] if \"@\" in email_lower else \"\"\n    \n    query = select(Suppression).where(\n        (Suppression.email == email_lower) |\n        (Suppression.domain == domain) |\n        (Suppression.is_global == True)\n    )\n    \n    if customer_id:\n        query = query.where(\n            (Suppression.customer_id == customer_id) | \n            (Suppression.customer_id == None)\n        )\n    \n    suppression = session.exec(query).first()\n    return suppression is not None\n\n\ndef add_suppression(\n    session: Session,\n    email: str,\n    customer_id: int,\n    reason: str,\n    source_message_id: int = None,\n    source_thread_id: int = None,\n    include_domain: bool = False\n) -> Suppression:\n    \"\"\"Add email/domain to suppression list.\"\"\"\n    email_lower = email.lower()\n    domain = email_lower.split(\"@\")[1] if \"@\" in email_lower and include_domain else None\n    \n    suppression = Suppression(\n        customer_id=customer_id,\n        email=email_lower,\n        domain=domain,\n        reason=reason,\n        source_message_id=source_message_id,\n        source_thread_id=source_thread_id,\n        is_global=False\n    )\n    session.add(suppression)\n    session.commit()\n    session.refresh(suppression)\n    \n    print(f\"[CONVERSATION][SUPPRESSION] Added {email_lower} to suppression (reason: {reason})\")\n    return suppression\n\n\ndef create_thread(\n    session: Session,\n    customer_id: int,\n    lead_email: str,\n    lead_name: str = None,\n    lead_company: str = None,\n    lead_event_id: int = None,\n    lead_id: int = None\n) -> Thread:\n    \"\"\"Create a new conversation thread.\"\"\"\n    thread = Thread(\n        customer_id=customer_id,\n        lead_email=lead_email.lower(),\n        lead_name=lead_name,\n        lead_company=lead_company,\n        lead_event_id=lead_event_id,\n        lead_id=lead_id,\n        status=THREAD_STATUS_OPEN,\n        created_at=datetime.utcnow(),\n        updated_at=datetime.utcnow()\n    )\n    session.add(thread)\n    session.commit()\n    session.refresh(thread)\n    \n    print(f\"[CONVERSATION][THREAD] Created thread #{thread.id} for {lead_email}\")\n    return thread\n\n\ndef store_inbound_message(\n    session: Session,\n    data: InboundEmailData,\n    thread: Thread,\n    customer_id: int = None,\n    lead_event_id: int = None,\n    lead_id: int = None\n) -> Message:\n    \"\"\"Store inbound email as a Message record.\"\"\"\n    message = Message(\n        thread_id=thread.id,\n        customer_id=customer_id,\n        lead_event_id=lead_event_id,\n        lead_id=lead_id,\n        direction=MESSAGE_DIRECTION_INBOUND,\n        from_email=data.from_email,\n        to_email=data.to_email,\n        cc=data.cc,\n        subject=data.subject,\n        body_text=data.body_text,\n        body_html=data.body_html,\n        message_id=data.message_id,\n        in_reply_to=data.in_reply_to,\n        references=data.references,\n        raw_metadata=json.dumps(data.raw_headers) if data.raw_headers else None,\n        created_at=data.received_at\n    )\n    session.add(message)\n    \n    thread.message_count += 1\n    thread.inbound_count += 1\n    thread.last_message_at = data.received_at\n    thread.last_direction = MESSAGE_DIRECTION_INBOUND\n    thread.last_summary = data.body_text[:100] if data.body_text else data.subject[:100]\n    thread.updated_at = datetime.utcnow()\n    \n    if thread.inbound_count == 1 and thread.outbound_count > 0:\n        first_outbound = session.exec(\n            select(Message).where(\n                Message.thread_id == thread.id,\n                Message.direction == MESSAGE_DIRECTION_OUTBOUND\n            ).order_by(Message.created_at.asc())\n        ).first()\n        if first_outbound:\n            response_time = (data.received_at - first_outbound.created_at).total_seconds()\n            thread.first_response_at = data.received_at\n            thread.response_time_seconds = int(response_time)\n    \n    session.commit()\n    session.refresh(message)\n    \n    print(f\"[CONVERSATION][MESSAGE] Stored inbound message #{message.id} in thread #{thread.id}\")\n    return message\n\n\ndef store_outbound_message(\n    session: Session,\n    thread: Thread,\n    customer_id: int,\n    to_email: str,\n    subject: str,\n    body_text: str,\n    body_html: str = None,\n    status: str = MESSAGE_STATUS_DRAFT,\n    generated_by: str = MESSAGE_GENERATED_AI,\n    message_id: str = None,\n    guardrail_flags: List[str] = None,\n    lead_event_id: int = None,\n    lead_id: int = None\n) -> Message:\n    \"\"\"Store outbound email as a Message record.\"\"\"\n    config = get_sendgrid_config()\n    from_email = config.get(\"from_email\", \"hello@hossagent.net\")\n    \n    customer = session.exec(select(Customer).where(Customer.id == customer_id)).first()\n    reply_to = customer.contact_email if customer else config.get(\"reply_to\")\n    cc = customer.contact_email if customer else None\n    \n    message = Message(\n        thread_id=thread.id,\n        customer_id=customer_id,\n        lead_event_id=lead_event_id,\n        lead_id=lead_id,\n        direction=MESSAGE_DIRECTION_OUTBOUND,\n        from_email=from_email,\n        to_email=to_email.lower(),\n        cc=cc,\n        reply_to=reply_to,\n        subject=subject,\n        body_text=body_text,\n        body_html=body_html,\n        message_id=message_id,\n        status=status,\n        generated_by=generated_by,\n        guardrail_flags=json.dumps(guardrail_flags) if guardrail_flags else None,\n        created_at=datetime.utcnow()\n    )\n    session.add(message)\n    session.commit()\n    session.refresh(message)\n    \n    print(f\"[CONVERSATION][MESSAGE] Stored outbound {status} #{message.id} in thread #{thread.id}\")\n    return message\n\n\ndef check_guardrails(body_text: str) -> GuardrailResult:\n    \"\"\"\n    Check proposed reply against guardrails.\n    \n    Returns GuardrailResult indicating if message passes safety checks.\n    \"\"\"\n    flags = []\n    details = {}\n    body_lower = body_text.lower()\n    \n    for pattern in PRICING_PATTERNS:\n        if re.search(pattern, body_lower):\n            flags.append(\"pricing\")\n            match = re.search(pattern, body_lower)\n            details[\"pricing\"] = match.group(0) if match else \"pricing language detected\"\n            break\n    \n    for pattern in SCHEDULING_PATTERNS:\n        if re.search(pattern, body_lower):\n            flags.append(\"scheduling\")\n            match = re.search(pattern, body_lower)\n            details[\"scheduling\"] = match.group(0) if match else \"scheduling language detected\"\n            break\n    \n    for pattern in COMMITMENT_PATTERNS:\n        if re.search(pattern, body_lower):\n            flags.append(\"commitment\")\n            match = re.search(pattern, body_lower)\n            details[\"commitment\"] = match.group(0) if match else \"commitment language detected\"\n            break\n    \n    for pattern in SENSITIVE_PATTERNS:\n        if re.search(pattern, body_lower):\n            flags.append(\"sensitive\")\n            match = re.search(pattern, body_lower)\n            details[\"sensitive\"] = match.group(0) if match else \"sensitive content detected\"\n            break\n    \n    passed = len(flags) == 0\n    \n    auto_send_allowed = False\n    if passed and AUTO_REPLY_LEVEL in [\"SAFE_ONLY\", \"AGGRESSIVE\"]:\n        for pattern in SAFE_REPLY_PATTERNS:\n            if re.search(pattern, body_lower):\n                auto_send_allowed = True\n                break\n        \n        if AUTO_REPLY_LEVEL == \"AGGRESSIVE\":\n            auto_send_allowed = True\n    \n    return GuardrailResult(\n        passed=passed,\n        flags=flags,\n        details=details,\n        auto_send_allowed=auto_send_allowed\n    )\n\n\ndef get_thread_context(session: Session, thread_id: int, max_messages: int = 10) -> List[Dict[str, Any]]:\n    \"\"\"Get recent messages from thread for context.\"\"\"\n    messages = session.exec(\n        select(Message).where(Message.thread_id == thread_id)\n        .order_by(Message.created_at.desc())\n        .limit(max_messages)\n    ).all()\n    \n    context = []\n    for msg in reversed(messages):\n        context.append({\n            \"direction\": msg.direction,\n            \"from\": msg.from_email,\n            \"to\": msg.to_email,\n            \"subject\": msg.subject,\n            \"body\": msg.body_text[:500] if msg.body_text else \"\",\n            \"timestamp\": msg.created_at.isoformat() if msg.created_at else None\n        })\n    \n    return context\n\n\ndef get_business_profile_context(session: Session, customer_id: int) -> Dict[str, Any]:\n    \"\"\"Get business profile for AI context.\"\"\"\n    profile = session.exec(\n        select(BusinessProfile).where(BusinessProfile.customer_id == customer_id)\n    ).first()\n    \n    if not profile:\n        return {}\n    \n    return {\n        \"short_description\": profile.short_description,\n        \"services\": profile.services,\n        \"pricing_notes\": profile.pricing_notes,\n        \"ideal_customer\": profile.ideal_customer,\n        \"voice_tone\": profile.voice_tone or \"professional\",\n        \"communication_style\": profile.communication_style or \"conversational\",\n        \"constraints\": profile.constraints,\n        \"primary_contact_name\": profile.primary_contact_name\n    }\n\n\ndef generate_ai_draft_reply(\n    session: Session,\n    thread: Thread,\n    inbound_message: Message,\n    customer_id: int\n) -> Optional[str]:\n    \"\"\"\n    Generate AI draft reply using OpenAI.\n    \n    Returns generated reply text or None if generation fails.\n    \"\"\"\n    try:\n        import openai\n    except ImportError:\n        print(\"[CONVERSATION][AI] OpenAI not available - cannot generate draft\")\n        return None\n    \n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        print(\"[CONVERSATION][AI] OPENAI_API_KEY not configured\")\n        return None\n    \n    thread_context = get_thread_context(session, thread.id)\n    business_profile = get_business_profile_context(session, customer_id)\n    \n    customer = session.exec(select(Customer).where(Customer.id == customer_id)).first()\n    customer_company = customer.company if customer else \"the business\"\n    \n    system_prompt = f\"\"\"You are a professional assistant helping {customer_company} respond to a business inquiry.\n\nBusiness Context:\n- Description: {business_profile.get('short_description', 'A professional service business')}\n- Services: {business_profile.get('services', 'Various professional services')}\n- Voice/Tone: {business_profile.get('voice_tone', 'professional and friendly')}\n- Communication Style: {business_profile.get('communication_style', 'conversational but professional')}\n\nIMPORTANT RULES:\n1. DO NOT quote specific prices, rates, or discounts\n2. DO NOT commit to specific dates or times for appointments\n3. DO NOT make guarantees or promises\n4. DO NOT provide legal, medical, or insurance advice\n5. Keep responses brief and focused (2-3 short paragraphs max)\n6. Suggest next steps like scheduling a call or providing more information\n7. Be helpful but redirect specifics to a direct conversation\n\nThe response should:\n- Acknowledge the lead's message\n- Provide helpful information without overcommitting\n- Encourage further conversation\n- Maintain the business's voice and tone\"\"\"\n\n    conversation_history = \"\\n\".join([\n        f\"{'Lead' if msg['direction'] == 'INBOUND' else 'Us'}: {msg['body'][:300]}\"\n        for msg in thread_context\n    ])\n    \n    user_prompt = f\"\"\"Conversation history:\n{conversation_history}\n\nGenerate a brief, professional reply to the lead's latest message. Remember the rules above.\"\"\"\n\n    try:\n        client = openai.OpenAI(api_key=api_key)\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt}\n            ],\n            temperature=0.7,\n            max_tokens=500\n        )\n        \n        draft = response.choices[0].message.content.strip()\n        print(f\"[CONVERSATION][AI] Generated draft reply ({len(draft)} chars)\")\n        return draft\n        \n    except Exception as e:\n        print(f\"[CONVERSATION][AI] Error generating draft: {e}\")\n        return None\n\n\ndef process_inbound_email(session: Session, data: InboundEmailData) -> Dict[str, Any]:\n    \"\"\"\n    Process an inbound email through the Conversation Engine.\n    \n    Steps:\n    1. Find or create thread\n    2. Store message\n    3. Check for opt-out\n    4. Generate AI draft (if applicable)\n    5. Apply guardrails\n    6. Update thread status\n    \n    Returns processing result with thread_id, message_id, actions taken.\n    \"\"\"\n    result = {\n        \"success\": False,\n        \"thread_id\": None,\n        \"message_id\": None,\n        \"actions\": [],\n        \"error\": None\n    }\n    \n    try:\n        customer = None\n        lead_event = None\n        thread = None\n        \n        customer = find_customer_by_email(session, data.to_email)\n        if not customer:\n            cc_emails = [e.strip().lower() for e in (data.cc or \"\").split(\",\") if e.strip()]\n            for cc_email in cc_emails:\n                customer = find_customer_by_email(session, cc_email)\n                if customer:\n                    break\n        \n        if not customer:\n            result[\"error\"] = f\"No customer found for {data.to_email}\"\n            print(f\"[CONVERSATION][INBOUND] {result['error']}\")\n            return result\n        \n        if check_suppression(session, data.from_email, customer.id):\n            result[\"error\"] = f\"Sender {data.from_email} is suppressed\"\n            result[\"actions\"].append(\"suppressed_sender_blocked\")\n            print(f\"[CONVERSATION][INBOUND] {result['error']}\")\n            return result\n        \n        if data.in_reply_to:\n            thread = find_thread_by_message_id(session, data.in_reply_to)\n            if thread:\n                result[\"actions\"].append(\"matched_by_message_id\")\n        \n        if not thread:\n            thread = find_thread_by_email(session, data.from_email, customer.id)\n            if thread:\n                result[\"actions\"].append(\"matched_by_email\")\n        \n        lead_event = find_lead_event_by_email(session, data.from_email, customer.id)\n        \n        if not thread:\n            thread = create_thread(\n                session=session,\n                customer_id=customer.id,\n                lead_email=data.from_email,\n                lead_name=None,\n                lead_company=None,\n                lead_event_id=lead_event.id if lead_event else None,\n                lead_id=lead_event.lead_id if lead_event else None\n            )\n            result[\"actions\"].append(\"created_new_thread\")\n        \n        message = store_inbound_message(\n            session=session,\n            data=data,\n            thread=thread,\n            customer_id=customer.id,\n            lead_event_id=lead_event.id if lead_event else None,\n            lead_id=lead_event.lead_id if lead_event else None\n        )\n        \n        result[\"thread_id\"] = thread.id\n        result[\"message_id\"] = message.id\n        \n        if detect_opt_out(data.body_text):\n            add_suppression(\n                session=session,\n                email=data.from_email,\n                customer_id=customer.id,\n                reason=\"opt_out\",\n                source_message_id=message.id,\n                source_thread_id=thread.id\n            )\n            \n            thread.status = THREAD_STATUS_CLOSED\n            thread.closed_reason = \"opt_out\"\n            thread.closed_at = datetime.utcnow()\n            \n            if lead_event:\n                lead_event.do_not_contact = True\n                lead_event.do_not_contact_reason = \"opt_out_reply\"\n                lead_event.do_not_contact_at = datetime.utcnow()\n            \n            session.commit()\n            result[\"actions\"].append(\"opt_out_processed\")\n            result[\"success\"] = True\n            return result\n        \n        if data.from_email.lower() == customer.contact_email.lower():\n            if thread.status != THREAD_STATUS_HUMAN_OWNED:\n                thread.status = THREAD_STATUS_HUMAN_OWNED\n                session.commit()\n                result[\"actions\"].append(\"marked_human_owned\")\n            result[\"success\"] = True\n            return result\n        \n        if lead_event:\n            lead_event.status = \"RESPONDED\"\n            session.commit()\n            result[\"actions\"].append(\"lead_marked_responded\")\n        \n        if thread.status not in [THREAD_STATUS_HUMAN_OWNED, THREAD_STATUS_CLOSED]:\n            draft_text = generate_ai_draft_reply(session, thread, message, customer.id)\n            \n            if draft_text:\n                guardrails = check_guardrails(draft_text)\n                \n                draft_status = MESSAGE_STATUS_DRAFT\n                if guardrails.passed and guardrails.auto_send_allowed:\n                    draft_status = MESSAGE_STATUS_QUEUED\n                    result[\"actions\"].append(\"draft_auto_approved\")\n                elif not guardrails.passed:\n                    result[\"actions\"].append(f\"guardrails_triggered: {','.join(guardrails.flags)}\")\n                \n                draft_message = store_outbound_message(\n                    session=session,\n                    thread=thread,\n                    customer_id=customer.id,\n                    to_email=data.from_email,\n                    subject=f\"Re: {data.subject}\" if not data.subject.lower().startswith(\"re:\") else data.subject,\n                    body_text=draft_text,\n                    status=draft_status,\n                    generated_by=MESSAGE_GENERATED_AI,\n                    guardrail_flags=guardrails.flags if guardrails.flags else None,\n                    lead_event_id=lead_event.id if lead_event else None,\n                    lead_id=lead_event.lead_id if lead_event else None\n                )\n                \n                result[\"draft_message_id\"] = draft_message.id\n                result[\"actions\"].append(\"ai_draft_created\")\n        \n        result[\"success\"] = True\n        \n    except Exception as e:\n        result[\"error\"] = str(e)\n        print(f\"[CONVERSATION][ERROR] {e}\")\n    \n    return result\n\n\ndef send_queued_messages(session: Session, max_messages: int = 10) -> List[Dict[str, Any]]:\n    \"\"\"\n    Send messages with QUEUED status.\n    \n    Returns list of send results.\n    \"\"\"\n    from email_utils import send_email, EmailResult\n    \n    queued = session.exec(\n        select(Message).where(\n            Message.status == MESSAGE_STATUS_QUEUED,\n            Message.direction == MESSAGE_DIRECTION_OUTBOUND\n        ).order_by(Message.created_at.asc())\n        .limit(max_messages)\n    ).all()\n    \n    results = []\n    \n    for message in queued:\n        if check_suppression(session, message.to_email, message.customer_id):\n            message.status = MESSAGE_STATUS_FAILED\n            message.raw_metadata = json.dumps({\"error\": \"suppressed\"})\n            session.commit()\n            results.append({\"message_id\": message.id, \"status\": \"suppressed\"})\n            continue\n        \n        customer = session.exec(\n            select(Customer).where(Customer.id == message.customer_id)\n        ).first()\n        \n        cc_email = customer.contact_email if customer else None\n        reply_to = customer.contact_email if customer else None\n        \n        email_result: EmailResult = send_email(\n            to_email=message.to_email,\n            subject=message.subject,\n            body=message.body_text,\n            lead_name=\"\",\n            company=\"\",\n            cc_email=cc_email,\n            reply_to=reply_to\n        )\n        \n        if email_result.success:\n            message.status = MESSAGE_STATUS_SENT\n            message.sent_at = datetime.utcnow()\n            if email_result.sendgrid_response:\n                message.sendgrid_message_id = email_result.sendgrid_response.get(\"x_message_id\")\n            \n            if message.thread_id:\n                thread = session.exec(\n                    select(Thread).where(Thread.id == message.thread_id)\n                ).first()\n                if thread:\n                    thread.message_count += 1\n                    thread.outbound_count += 1\n                    thread.last_message_at = datetime.utcnow()\n                    thread.last_direction = MESSAGE_DIRECTION_OUTBOUND\n                    thread.last_summary = message.body_text[:100] if message.body_text else message.subject[:100]\n                    thread.updated_at = datetime.utcnow()\n            \n            results.append({\"message_id\": message.id, \"status\": \"sent\"})\n        else:\n            message.status = MESSAGE_STATUS_FAILED\n            message.raw_metadata = json.dumps({\"error\": email_result.error})\n            results.append({\"message_id\": message.id, \"status\": \"failed\", \"error\": email_result.error})\n        \n        session.commit()\n    \n    return results\n\n\ndef approve_draft(session: Session, message_id: int, approved_by: str = \"customer\") -> bool:\n    \"\"\"Approve a draft message for sending.\"\"\"\n    message = session.exec(\n        select(Message).where(\n            Message.id == message_id,\n            Message.status == MESSAGE_STATUS_DRAFT\n        )\n    ).first()\n    \n    if not message:\n        return False\n    \n    message.status = MESSAGE_STATUS_APPROVED\n    message.approved_at = datetime.utcnow()\n    message.approved_by = approved_by\n    message.guardrail_approved = True\n    session.commit()\n    \n    message.status = MESSAGE_STATUS_QUEUED\n    session.commit()\n    \n    print(f\"[CONVERSATION] Approved draft #{message_id}\")\n    return True\n\n\ndef edit_and_approve_draft(\n    session: Session,\n    message_id: int,\n    new_body_text: str,\n    new_subject: str = None,\n    approved_by: str = \"customer\"\n) -> bool:\n    \"\"\"Edit a draft message and approve for sending.\"\"\"\n    message = session.exec(\n        select(Message).where(\n            Message.id == message_id,\n            Message.status == MESSAGE_STATUS_DRAFT\n        )\n    ).first()\n    \n    if not message:\n        return False\n    \n    message.body_text = new_body_text\n    if new_subject:\n        message.subject = new_subject\n    message.generated_by = MESSAGE_GENERATED_HUMAN\n    message.status = MESSAGE_STATUS_APPROVED\n    message.approved_at = datetime.utcnow()\n    message.approved_by = approved_by\n    message.guardrail_approved = True\n    session.commit()\n    \n    message.status = MESSAGE_STATUS_QUEUED\n    session.commit()\n    \n    print(f\"[CONVERSATION] Edited and approved draft #{message_id}\")\n    return True\n\n\ndef discard_draft(session: Session, message_id: int) -> bool:\n    \"\"\"Discard a draft message.\"\"\"\n    message = session.exec(\n        select(Message).where(\n            Message.id == message_id,\n            Message.status == MESSAGE_STATUS_DRAFT\n        )\n    ).first()\n    \n    if not message:\n        return False\n    \n    session.delete(message)\n    session.commit()\n    \n    print(f\"[CONVERSATION] Discarded draft #{message_id}\")\n    return True\n\n\ndef set_thread_status(session: Session, thread_id: int, status: str) -> bool:\n    \"\"\"Update thread status (OPEN, HUMAN_OWNED, AUTO, CLOSED).\"\"\"\n    thread = session.exec(select(Thread).where(Thread.id == thread_id)).first()\n    if not thread:\n        return False\n    \n    thread.status = status\n    thread.updated_at = datetime.utcnow()\n    \n    if status == THREAD_STATUS_CLOSED:\n        thread.closed_at = datetime.utcnow()\n    \n    session.commit()\n    print(f\"[CONVERSATION] Thread #{thread_id} status -> {status}\")\n    return True\n\n\ndef calculate_customer_metrics(session: Session, customer_id: int) -> ConversationMetrics:\n    \"\"\"Calculate and update conversation metrics for a customer.\"\"\"\n    metrics = session.exec(\n        select(ConversationMetrics).where(ConversationMetrics.customer_id == customer_id)\n    ).first()\n    \n    if not metrics:\n        metrics = ConversationMetrics(customer_id=customer_id)\n        session.add(metrics)\n    \n    lead_events_count = session.exec(\n        select(func.count(LeadEvent.id)).where(LeadEvent.company_id == customer_id)\n    ).one()\n    metrics.total_lead_events = lead_events_count\n    \n    threads = session.exec(\n        select(Thread).where(Thread.customer_id == customer_id)\n    ).all()\n    metrics.total_threads = len(threads)\n    \n    threads_with_outbound = [t for t in threads if t.outbound_count > 0]\n    threads_with_inbound = [t for t in threads if t.inbound_count > 0]\n    \n    metrics.leads_contacted = len(threads_with_outbound)\n    metrics.leads_replied = len(threads_with_inbound)\n    metrics.reply_rate_pct = (\n        (metrics.leads_replied / metrics.leads_contacted * 100)\n        if metrics.leads_contacted > 0 else 0.0\n    )\n    \n    response_times = [t.response_time_seconds for t in threads if t.response_time_seconds]\n    metrics.avg_response_time_seconds = (\n        int(sum(response_times) / len(response_times))\n        if response_times else None\n    )\n    \n    outbound_count = session.exec(\n        select(func.count(Message.id)).where(\n            Message.customer_id == customer_id,\n            Message.direction == MESSAGE_DIRECTION_OUTBOUND\n        )\n    ).one()\n    inbound_count = session.exec(\n        select(func.count(Message.id)).where(\n            Message.customer_id == customer_id,\n            Message.direction == MESSAGE_DIRECTION_INBOUND\n        )\n    ).one()\n    metrics.total_outbound = outbound_count\n    metrics.total_inbound = inbound_count\n    \n    ai_drafted = session.exec(\n        select(func.count(Message.id)).where(\n            Message.customer_id == customer_id,\n            Message.generated_by == MESSAGE_GENERATED_AI\n        )\n    ).one()\n    human_sent = session.exec(\n        select(func.count(Message.id)).where(\n            Message.customer_id == customer_id,\n            Message.generated_by == MESSAGE_GENERATED_HUMAN\n        )\n    ).one()\n    metrics.messages_ai_drafted = ai_drafted\n    metrics.messages_human_sent = human_sent\n    \n    metrics.threads_human_owned = len([t for t in threads if t.status == THREAD_STATUS_HUMAN_OWNED])\n    metrics.threads_closed_opt_out = len([t for t in threads if t.closed_reason == \"opt_out\"])\n    \n    total_depth = sum(t.message_count for t in threads)\n    metrics.avg_thread_depth = total_depth / len(threads) if threads else 0.0\n    \n    metrics.last_calculated_at = datetime.utcnow()\n    session.commit()\n    \n    return metrics\n\n\ndef get_thread_summary(session: Session, thread_id: int) -> Optional[Dict[str, Any]]:\n    \"\"\"Get summary of a thread for display.\"\"\"\n    thread = session.exec(select(Thread).where(Thread.id == thread_id)).first()\n    if not thread:\n        return None\n    \n    messages = session.exec(\n        select(Message).where(Message.thread_id == thread_id)\n        .order_by(Message.created_at.asc())\n    ).all()\n    \n    drafts = [m for m in messages if m.status == MESSAGE_STATUS_DRAFT]\n    \n    return {\n        \"id\": thread.id,\n        \"lead_email\": thread.lead_email,\n        \"lead_name\": thread.lead_name,\n        \"lead_company\": thread.lead_company,\n        \"status\": thread.status,\n        \"message_count\": thread.message_count,\n        \"inbound_count\": thread.inbound_count,\n        \"outbound_count\": thread.outbound_count,\n        \"last_message_at\": thread.last_message_at.isoformat() if thread.last_message_at else None,\n        \"last_direction\": thread.last_direction,\n        \"last_summary\": thread.last_summary,\n        \"has_drafts\": len(drafts) > 0,\n        \"draft_count\": len(drafts),\n        \"created_at\": thread.created_at.isoformat() if thread.created_at else None,\n        \"messages\": [\n            {\n                \"id\": m.id,\n                \"direction\": m.direction,\n                \"from_email\": m.from_email,\n                \"to_email\": m.to_email,\n                \"subject\": m.subject,\n                \"body_text\": m.body_text,\n                \"status\": m.status,\n                \"generated_by\": m.generated_by,\n                \"guardrail_flags\": json.loads(m.guardrail_flags) if m.guardrail_flags else None,\n                \"created_at\": m.created_at.isoformat() if m.created_at else None,\n                \"sent_at\": m.sent_at.isoformat() if m.sent_at else None\n            }\n            for m in messages\n        ]\n    }\n\n\ndef get_customer_threads(\n    session: Session,\n    customer_id: int,\n    status_filter: str = None,\n    limit: int = 50\n) -> List[Dict[str, Any]]:\n    \"\"\"Get threads for a customer with optional status filter.\"\"\"\n    query = select(Thread).where(Thread.customer_id == customer_id)\n    \n    if status_filter:\n        query = query.where(Thread.status == status_filter)\n    \n    query = query.order_by(Thread.updated_at.desc()).limit(limit)\n    threads = session.exec(query).all()\n    \n    result = []\n    for thread in threads:\n        draft_count = session.exec(\n            select(func.count(Message.id)).where(\n                Message.thread_id == thread.id,\n                Message.status == MESSAGE_STATUS_DRAFT\n            )\n        ).one()\n        \n        result.append({\n            \"id\": thread.id,\n            \"lead_email\": thread.lead_email,\n            \"lead_name\": thread.lead_name,\n            \"lead_company\": thread.lead_company,\n            \"status\": thread.status,\n            \"message_count\": thread.message_count,\n            \"last_message_at\": thread.last_message_at.isoformat() if thread.last_message_at else None,\n            \"last_direction\": thread.last_direction,\n            \"last_summary\": thread.last_summary,\n            \"has_drafts\": draft_count > 0,\n            \"draft_count\": draft_count\n        })\n    \n    return result\n","path":null,"size_bytes":37901,"size_tokens":null},"outbound_utils.py":{"content":"\"\"\"\nOutbound utilities for HossAgent.\nHandles business profile lookups, do-not-contact checks, and pending outbound creation.\n\"\"\"\nimport json\nfrom typing import Optional, List\nfrom datetime import datetime\nfrom sqlmodel import Session, select\nfrom models import BusinessProfile, PendingOutbound, Customer, Signal\n\n\ndef _get_source_url_for_lead_event(session: Session, lead_event) -> Optional[str]:\n    \"\"\"\n    Get source URL from the Signal associated with a LeadEvent.\n    \n    Extracts URL from signal's raw_payload (stored as JSON).\n    \n    Args:\n        session: Database session\n        lead_event: LeadEvent object with signal_id\n        \n    Returns:\n        Source URL string if found, None otherwise\n    \"\"\"\n    if not lead_event.signal_id:\n        return None\n    \n    signal = session.exec(\n        select(Signal).where(Signal.id == lead_event.signal_id)\n    ).first()\n    \n    if not signal or not signal.raw_payload:\n        return None\n    \n    try:\n        payload = json.loads(signal.raw_payload)\n        return payload.get(\"url\") or payload.get(\"source_url\") or payload.get(\"link\")\n    except (json.JSONDecodeError, TypeError):\n        return None\n\n\ndef get_business_profile(session: Session, customer_id: int) -> Optional[BusinessProfile]:\n    \"\"\"\n    Fetch BusinessProfile for a customer.\n    \n    Args:\n        session: Database session\n        customer_id: The customer ID to fetch profile for\n        \n    Returns:\n        BusinessProfile if found, None otherwise\n    \"\"\"\n    return session.exec(\n        select(BusinessProfile).where(BusinessProfile.customer_id == customer_id)\n    ).first()\n\n\ndef parse_do_not_contact_list(do_not_contact_list: Optional[str]) -> List[str]:\n    \"\"\"\n    Parse do_not_contact_list which can be JSON array or comma-separated string.\n    \n    Args:\n        do_not_contact_list: Raw string from BusinessProfile\n        \n    Returns:\n        List of emails/domains to block\n    \"\"\"\n    if not do_not_contact_list:\n        return []\n    \n    do_not_contact_list = do_not_contact_list.strip()\n    if not do_not_contact_list:\n        return []\n    \n    if do_not_contact_list.startswith('['):\n        try:\n            parsed = json.loads(do_not_contact_list)\n            if isinstance(parsed, list):\n                return [str(item).strip().lower() for item in parsed if item]\n        except json.JSONDecodeError:\n            pass\n    \n    return [item.strip().lower() for item in do_not_contact_list.split(',') if item.strip()]\n\n\ndef check_do_not_contact(email: str, do_not_contact_list: Optional[str]) -> bool:\n    \"\"\"\n    Check if an email or its domain is in the do-not-contact list.\n    \n    Args:\n        email: Email address to check\n        do_not_contact_list: Raw string from BusinessProfile (JSON array or comma-separated)\n        \n    Returns:\n        True if email/domain is blocked, False otherwise\n    \"\"\"\n    if not email or not do_not_contact_list:\n        return False\n    \n    email = email.strip().lower()\n    blocked_entries = parse_do_not_contact_list(do_not_contact_list)\n    \n    if not blocked_entries:\n        return False\n    \n    email_domain = email.split('@')[-1] if '@' in email else ''\n    \n    for entry in blocked_entries:\n        if entry == email:\n            return True\n        if entry.startswith('@') and email.endswith(entry):\n            return True\n        if not entry.startswith('@') and entry == email_domain:\n            return True\n        if '@' not in entry and email_domain == entry:\n            return True\n    \n    return False\n\n\ndef create_pending_outbound(\n    session: Session,\n    customer_id: Optional[int],\n    lead_id: Optional[int],\n    to_email: str,\n    to_name: Optional[str],\n    subject: str,\n    body: str,\n    context_summary: Optional[str] = None,\n    lead_event_id: Optional[int] = None,\n    status: str = \"PENDING\"\n) -> PendingOutbound:\n    \"\"\"\n    Create a PendingOutbound record.\n    \n    Used for both REVIEW mode (status=PENDING) and AUTO mode (status=SENT).\n    This ensures portal can always find email content.\n    \n    Args:\n        session: Database session\n        customer_id: The customer ID this outbound belongs to (optional for AUTO)\n        lead_id: Optional lead ID this outbound is for\n        to_email: Recipient email address\n        to_name: Recipient name\n        subject: Email subject\n        body: Email body\n        context_summary: Why this email is being sent\n        lead_event_id: Optional lead event ID this outbound is for\n        status: Initial status - PENDING for review, SENT for already sent\n        \n    Returns:\n        Created PendingOutbound record\n    \"\"\"\n    pending = PendingOutbound(\n        customer_id=customer_id,\n        lead_id=lead_id,\n        lead_event_id=lead_event_id,\n        to_email=to_email,\n        to_name=to_name,\n        subject=subject,\n        body=body,\n        context_summary=context_summary,\n        status=status,\n        created_at=datetime.utcnow()\n    )\n    session.add(pending)\n    session.flush()\n    \n    status_str = \"QUEUED\" if status == \"PENDING\" else status\n    print(f\"[OUTBOUND] Created PendingOutbound {pending.id} ({status_str}) for customer {customer_id}: to={to_email} subject=\\\"{subject[:50]}...\\\"\")\n    \n    return pending\n\n\ndef get_customer_by_id(session: Session, customer_id: int) -> Optional[Customer]:\n    \"\"\"\n    Fetch a Customer by ID.\n    \n    Args:\n        session: Database session\n        customer_id: The customer ID to fetch\n        \n    Returns:\n        Customer if found, None otherwise\n    \"\"\"\n    return session.exec(\n        select(Customer).where(Customer.id == customer_id)\n    ).first()\n\n\nclass ImmediateSendResult:\n    \"\"\"Result of an immediate send operation.\"\"\"\n    def __init__(\n        self,\n        success: bool,\n        action: str,\n        reason: str = \"\",\n        email_sent: bool = False,\n        queued_for_review: bool = False\n    ):\n        self.success = success\n        self.action = action\n        self.reason = reason\n        self.email_sent = email_sent\n        self.queued_for_review = queued_for_review\n    \n    def __repr__(self):\n        return f\"ImmediateSendResult(success={self.success}, action='{self.action}', reason='{self.reason}')\"\n\n\ndef send_lead_event_immediate(session: Session, lead_event, commit: bool = True) -> ImmediateSendResult:\n    \"\"\"\n    Immediately send outbound email for a LeadEvent that has been enriched with an email.\n    \n    This is the core function that eliminates the waiting state between enrichment and sending.\n    Called directly from the enrichment pipeline when an email is discovered.\n    \n    Flow:\n    - AUTO mode: Send email immediately, update status to OUTBOUND_SENT\n    - REVIEW mode: Create PendingOutbound for customer approval\n    \n    Args:\n        session: Database session\n        lead_event: LeadEvent with lead_email set\n        commit: Whether to commit the transaction (default True)\n        \n    Returns:\n        ImmediateSendResult with success status and action taken\n    \"\"\"\n    import hashlib\n    from datetime import datetime\n    from models import (\n        OUTREACH_MODE_AUTO, OUTREACH_MODE_REVIEW,\n        LEAD_STATUS_NEW, LEAD_STATUS_CONTACTED,\n        NEXT_STEP_OWNER_AGENT, NEXT_STEP_OWNER_CUSTOMER,\n        ENRICHMENT_STATUS_OUTBOUND_SENT,\n    )\n    from email_utils import send_email\n    \n    contact_email = lead_event.lead_email or lead_event.enriched_email\n    if not contact_email:\n        return ImmediateSendResult(\n            success=False,\n            action=\"skipped\",\n            reason=\"No email address available\"\n        )\n    \n    GENERIC_PREFIXES = ['info', 'contact', 'hello', 'support', 'admin', 'sales', 'enquiry', \n                        'enquiries', 'office', 'mail', 'help', 'general', 'team', 'service',\n                        'customerservice', 'helpdesk', 'noreply', 'no-reply', 'webmaster']\n    email_local = contact_email.split('@')[0].lower() if '@' in contact_email else ''\n    is_generic_inbox = any(email_local == prefix or email_local.startswith(f\"{prefix}.\") \n                           for prefix in GENERIC_PREFIXES)\n    \n    email_confidence = getattr(lead_event, 'email_confidence', 0.5)\n    \n    if is_generic_inbox:\n        if email_confidence < 0.4:\n            print(f\"[IMMEDIATE-SEND] Skipping low-confidence generic inbox {contact_email} (confidence={email_confidence:.2f})\")\n            return ImmediateSendResult(\n                success=False,\n                action=\"skipped\",\n                reason=f\"Generic inbox ({email_local}@) with low confidence - continuing enrichment\"\n            )\n        else:\n            print(f\"[IMMEDIATE-SEND] Allowing generic inbox {contact_email} (confidence={email_confidence:.2f}) - no person-like email available\")\n    \n    if lead_event.do_not_contact:\n        return ImmediateSendResult(\n            success=False,\n            action=\"blocked\",\n            reason=\"Lead marked do_not_contact\"\n        )\n    \n    contact_name = lead_event.lead_name or lead_event.enriched_contact_name\n    company_name = lead_event.lead_company or lead_event.enriched_company_name or \"Your company\"\n    \n    customer = None\n    business_profile = None\n    outreach_mode = OUTREACH_MODE_AUTO\n    do_not_contact_list = None\n    cc_email = None\n    reply_to = None\n    niche = \"small business\"\n    city = \"Miami\"\n    outreach_style = \"transparent_ai\"\n    \n    if lead_event.company_id:\n        customer = get_customer_by_id(session, lead_event.company_id)\n        if customer:\n            outreach_mode = customer.outreach_mode or OUTREACH_MODE_AUTO\n            outreach_style = getattr(customer, 'outreach_style', 'transparent_ai') or 'transparent_ai'\n            city = customer.geography or \"Miami\"\n            niche = customer.niche or niche\n            business_profile = get_business_profile(session, customer.id)\n            if business_profile:\n                do_not_contact_list = business_profile.do_not_contact_list\n            cc_email = customer.contact_email\n            reply_to = customer.contact_email\n            if business_profile and business_profile.primary_contact_email:\n                cc_email = business_profile.primary_contact_email\n                reply_to = business_profile.primary_contact_email\n    \n    if check_do_not_contact(contact_email, do_not_contact_list):\n        return ImmediateSendResult(\n            success=False,\n            action=\"blocked\",\n            reason=\"Email in do_not_contact list\"\n        )\n    \n    from agents import check_rate_limits\n    rate_ok, rate_reason = check_rate_limits(session, contact_email, lead_event.id, lead_event.company_id)\n    if not rate_ok:\n        return ImmediateSendResult(\n            success=False,\n            action=\"rate_limited\",\n            reason=rate_reason\n        )\n    \n    source_url = _get_source_url_for_lead_event(session, lead_event)\n    \n    from agents import generate_miami_contextual_email\n    subject, body = generate_miami_contextual_email(\n        contact_name=contact_name or \"there\",\n        company_name=company_name,\n        niche=niche,\n        event_summary=lead_event.summary,\n        recommended_action=lead_event.recommended_action or \"contextual outreach\",\n        category=lead_event.category,\n        urgency_score=lead_event.urgency_score,\n        outreach_style=outreach_style,\n        event_id=lead_event.id,\n        signal_id=lead_event.signal_id,\n        city=city,\n        source_url=source_url\n    )\n    \n    if outreach_mode == OUTREACH_MODE_REVIEW and customer:\n        create_pending_outbound(\n            session=session,\n            customer_id=customer.id,\n            lead_id=lead_event.lead_id,\n            to_email=contact_email,\n            to_name=contact_name,\n            subject=subject,\n            body=body,\n            context_summary=f\"Signal-triggered: {lead_event.category} - {lead_event.summary[:100] if lead_event.summary else ''}\",\n            lead_event_id=lead_event.id\n        )\n        lead_event.outbound_message = body\n        lead_event.next_step = \"Awaiting your review\"\n        lead_event.next_step_owner = NEXT_STEP_OWNER_CUSTOMER\n        session.add(lead_event)\n        \n        if commit:\n            session.commit()\n        \n        print(f\"[IMMEDIATE-SEND] Event {lead_event.id} for {company_name}: QUEUED for review (REVIEW mode)\")\n        return ImmediateSendResult(\n            success=True,\n            action=\"queued\",\n            reason=\"Queued for customer review (REVIEW mode)\",\n            queued_for_review=True\n        )\n    \n    email_result = send_email(\n        to_email=contact_email,\n        subject=subject,\n        body=body,\n        lead_name=contact_name,\n        company=company_name,\n        cc_email=cc_email,\n        reply_to=reply_to\n    )\n    \n    lead_event.outbound_message = body\n    lead_event.outbound_subject = subject\n    \n    if email_result.actually_sent or email_result.result in (\"dry_run\", \"fallback\"):\n        lead_event.status = LEAD_STATUS_CONTACTED\n        lead_event.enrichment_status = ENRICHMENT_STATUS_OUTBOUND_SENT\n        lead_event.last_contact_at = datetime.utcnow()\n        lead_event.last_contact_summary = f\"Contextual email sent: {lead_event.category}\"\n        lead_event.next_step_owner = NEXT_STEP_OWNER_AGENT\n        lead_event.contact_count_24h = (lead_event.contact_count_24h or 0) + 1\n        lead_event.contact_count_7d = (lead_event.contact_count_7d or 0) + 1\n        lead_event.last_subject_hash = hashlib.md5(subject.encode()).hexdigest()[:16]\n        session.add(lead_event)\n        \n        if customer:\n            create_pending_outbound(\n                session=session,\n                customer_id=customer.id,\n                lead_id=lead_event.lead_id,\n                to_email=contact_email,\n                to_name=contact_name,\n                subject=subject,\n                body=body,\n                context_summary=f\"Signal-triggered: {lead_event.category} - {lead_event.summary[:100] if lead_event.summary else ''}\",\n                lead_event_id=lead_event.id,\n                status=\"SENT\"\n            )\n        \n        if commit:\n            session.commit()\n        \n        mode_str = \"SENT\" if email_result.actually_sent else f\"DRY_RUN ({email_result.mode})\"\n        print(f\"[IMMEDIATE-SEND] Event {lead_event.id} for {company_name}: {mode_str}  OUTBOUND_SENT\")\n        return ImmediateSendResult(\n            success=True,\n            action=\"sent\",\n            reason=f\"Email {mode_str.lower()}\",\n            email_sent=email_result.actually_sent\n        )\n    else:\n        print(f\"[IMMEDIATE-SEND] Event {lead_event.id} for {company_name}: FAILED error=\\\"{email_result.error}\\\"\")\n        return ImmediateSendResult(\n            success=False,\n            action=\"failed\",\n            reason=f\"Email failed: {email_result.error}\"\n        )\n","path":null,"size_bytes":14793,"size_tokens":null},"subscription_utils.py":{"content":"\"\"\"\nSubscription Management for HossAgent SaaS.\n\nHandles:\n- Trial/paid plan gating\n- Usage limits tracking\n- Stripe product/price bootstrap\n- Customer upgrade flow\n- Trial abuse prevention\n\nEnvironment Variables:\n  STRIPE_PRICE_ID - Existing price ID (optional, auto-created if missing)\n  STRIPE_PRODUCT_ID - Existing product ID (optional, auto-created if missing)\n\nPlan Rules:\n  trial: 7 days, 15 tasks, 20 leads, DRY_RUN email, no billing, no autopilot\n  paid: Unlimited, real email, full billing, autopilot enabled\n  trial_expired: Locked account, upgrade required\n\"\"\"\nimport os\nimport hashlib\nfrom datetime import datetime, timedelta\nfrom typing import Dict, Any, Optional, Tuple\nfrom dataclasses import dataclass\nfrom sqlmodel import Session, select\n\nfrom models import Customer, TrialIdentity, TRIAL_TASK_LIMIT, TRIAL_LEAD_LIMIT, TRIAL_DAYS\n\n\n@dataclass\nclass PlanStatus:\n    \"\"\"Current plan status for a customer.\"\"\"\n    plan: str  # trial, paid, trial_expired\n    is_trial: bool\n    is_paid: bool\n    is_expired: bool\n    days_remaining: int\n    tasks_used: int\n    tasks_limit: int\n    leads_used: int\n    leads_limit: int\n    can_run_tasks: bool\n    can_generate_leads: bool\n    can_send_real_email: bool\n    can_use_billing: bool\n    can_use_autopilot: bool\n    upgrade_required: bool\n    status_message: str\n\n\ndef get_customer_plan_status(customer: Customer) -> PlanStatus:\n    \"\"\"\n    Get complete plan status for a customer.\n    \n    Determines all feature access based on plan and usage.\n    \"\"\"\n    now = datetime.utcnow()\n    plan = customer.plan or \"trial\"\n    \n    if plan == \"paid\":\n        return PlanStatus(\n            plan=\"paid\",\n            is_trial=False,\n            is_paid=True,\n            is_expired=False,\n            days_remaining=999,\n            tasks_used=customer.tasks_this_period or 0,\n            tasks_limit=999999,\n            leads_used=customer.leads_this_period or 0,\n            leads_limit=999999,\n            can_run_tasks=True,\n            can_generate_leads=True,\n            can_send_real_email=True,\n            can_use_billing=True,\n            can_use_autopilot=True,\n            upgrade_required=False,\n            status_message=\"Full access - $99/month subscription active\"\n        )\n    \n    trial_end = customer.trial_end_at\n    if trial_end is None:\n        if customer.trial_start_at:\n            trial_end = customer.trial_start_at + timedelta(days=TRIAL_DAYS)\n        else:\n            trial_end = now + timedelta(days=TRIAL_DAYS)\n    \n    days_remaining = max(0, (trial_end - now).days)\n    is_expired = now >= trial_end or plan == \"trial_expired\"\n    \n    tasks_used = customer.tasks_this_period or 0\n    leads_used = customer.leads_this_period or 0\n    can_run_tasks = tasks_used < TRIAL_TASK_LIMIT and not is_expired\n    can_generate_leads = leads_used < TRIAL_LEAD_LIMIT and not is_expired\n    \n    if is_expired:\n        return PlanStatus(\n            plan=\"trial_expired\",\n            is_trial=False,\n            is_paid=False,\n            is_expired=True,\n            days_remaining=0,\n            tasks_used=tasks_used,\n            tasks_limit=TRIAL_TASK_LIMIT,\n            leads_used=leads_used,\n            leads_limit=TRIAL_LEAD_LIMIT,\n            can_run_tasks=False,\n            can_generate_leads=False,\n            can_send_real_email=False,\n            can_use_billing=False,\n            can_use_autopilot=False,\n            upgrade_required=True,\n            status_message=\"Trial expired - Upgrade to continue\"\n        )\n    \n    task_warning = f\"{tasks_used}/{TRIAL_TASK_LIMIT} tasks used\"\n    lead_warning = f\"{leads_used}/{TRIAL_LEAD_LIMIT} leads used\"\n    \n    return PlanStatus(\n        plan=\"trial\",\n        is_trial=True,\n        is_paid=False,\n        is_expired=False,\n        days_remaining=days_remaining,\n        tasks_used=tasks_used,\n        tasks_limit=TRIAL_TASK_LIMIT,\n        leads_used=leads_used,\n        leads_limit=TRIAL_LEAD_LIMIT,\n        can_run_tasks=can_run_tasks,\n        can_generate_leads=can_generate_leads,\n        can_send_real_email=False,\n        can_use_billing=False,\n        can_use_autopilot=False,\n        upgrade_required=False,\n        status_message=f\"Trial Mode - {days_remaining} days remaining ({task_warning}, {lead_warning})\"\n    )\n\n\ndef check_trial_abuse(\n    session: Session,\n    email: str,\n    ip_address: Optional[str] = None,\n    user_agent: Optional[str] = None,\n    device_fingerprint: Optional[str] = None\n) -> Tuple[bool, Optional[str]]:\n    \"\"\"\n    Check if a new trial signup would be abuse.\n    \n    Returns:\n        (is_allowed, block_reason) - (True, None) if allowed, (False, reason) if blocked\n    \"\"\"\n    user_agent_hash = None\n    if user_agent:\n        user_agent_hash = hashlib.sha256(user_agent.encode()).hexdigest()[:32]\n    \n    email_match = session.exec(\n        select(TrialIdentity).where(TrialIdentity.email == email.lower())\n    ).first()\n    if email_match:\n        return False, f\"Email already used for trial: {email}\"\n    \n    if ip_address:\n        ip_match = session.exec(\n            select(TrialIdentity).where(TrialIdentity.ip_address == ip_address)\n        ).first()\n        if ip_match:\n            return False, f\"IP address already used for trial\"\n    \n    if device_fingerprint:\n        fp_match = session.exec(\n            select(TrialIdentity).where(TrialIdentity.device_fingerprint == device_fingerprint)\n        ).first()\n        if fp_match:\n            return False, \"Device already used for trial\"\n    \n    return True, None\n\n\ndef record_trial_identity(\n    session: Session,\n    customer_id: int,\n    email: str,\n    ip_address: Optional[str] = None,\n    user_agent: Optional[str] = None,\n    device_fingerprint: Optional[str] = None,\n    blocked: bool = False,\n    block_reason: Optional[str] = None\n) -> TrialIdentity:\n    \"\"\"\n    Record trial identity for abuse prevention.\n    \"\"\"\n    user_agent_hash = None\n    if user_agent:\n        user_agent_hash = hashlib.sha256(user_agent.encode()).hexdigest()[:32]\n    \n    identity = TrialIdentity(\n        email=email.lower(),\n        ip_address=ip_address,\n        user_agent_hash=user_agent_hash,\n        device_fingerprint=device_fingerprint,\n        customer_id=customer_id,\n        blocked=blocked,\n        block_reason=block_reason\n    )\n    session.add(identity)\n    return identity\n\n\ndef initialize_trial(customer: Customer) -> Customer:\n    \"\"\"\n    Initialize a customer with trial plan.\n    Sets trial start/end dates and resets usage counters.\n    \"\"\"\n    now = datetime.utcnow()\n    customer.plan = \"trial\"\n    customer.trial_start_at = now\n    customer.trial_end_at = now + timedelta(days=TRIAL_DAYS)\n    customer.subscription_status = \"none\"\n    customer.tasks_this_period = 0\n    customer.leads_this_period = 0\n    return customer\n\n\ndef upgrade_to_paid(customer: Customer, stripe_subscription_id: Optional[str] = None) -> Customer:\n    \"\"\"\n    Upgrade a customer to paid plan.\n    \"\"\"\n    customer.plan = \"paid\"\n    customer.subscription_status = \"active\"\n    if stripe_subscription_id:\n        customer.stripe_subscription_id = stripe_subscription_id\n    customer.tasks_this_period = 0\n    customer.leads_this_period = 0\n    return customer\n\n\ndef expire_trial(customer: Customer) -> Customer:\n    \"\"\"\n    Mark a customer's trial as expired.\n    \"\"\"\n    customer.plan = \"trial_expired\"\n    customer.subscription_status = \"none\"\n    return customer\n\n\ndef increment_task_usage(session: Session, customer_id: int) -> bool:\n    \"\"\"\n    Increment task usage for a customer (with blocking check).\n    Returns True if task can proceed, False if limit reached.\n    \n    Note: Use increment_tasks_used() for soft-cap (display only) incrementing.\n    \"\"\"\n    customer = session.exec(\n        select(Customer).where(Customer.id == customer_id)\n    ).first()\n    \n    if not customer:\n        return False\n    \n    status = get_customer_plan_status(customer)\n    if not status.can_run_tasks:\n        print(f\"[SUBSCRIPTION] Customer {customer_id} cannot run tasks: {status.status_message}\")\n        return False\n    \n    customer.tasks_this_period = (customer.tasks_this_period or 0) + 1\n    session.add(customer)\n    return True\n\n\ndef increment_lead_usage(session: Session, customer_id: int) -> bool:\n    \"\"\"\n    Increment lead usage for a customer (with blocking check).\n    Returns True if lead can proceed, False if limit reached.\n    \n    Note: Use increment_leads_used() for soft-cap (display only) incrementing.\n    \"\"\"\n    customer = session.exec(\n        select(Customer).where(Customer.id == customer_id)\n    ).first()\n    \n    if not customer:\n        return False\n    \n    status = get_customer_plan_status(customer)\n    if not status.can_generate_leads:\n        print(f\"[SUBSCRIPTION] Customer {customer_id} cannot generate leads: {status.status_message}\")\n        return False\n    \n    customer.leads_this_period = (customer.leads_this_period or 0) + 1\n    session.add(customer)\n    return True\n\n\ndef increment_tasks_used(session: Session, customer_id: int) -> bool:\n    \"\"\"\n    Increment task usage counter for a customer (soft cap - display only).\n    \n    This function ALWAYS increments the counter regardless of limits.\n    Soft caps are enforced at the UI layer for display only.\n    \n    Returns True if increment was successful, False if customer not found.\n    Safe to call multiple times - handles missing customers gracefully.\n    \"\"\"\n    if not customer_id:\n        return False\n    \n    customer = session.exec(\n        select(Customer).where(Customer.id == customer_id)\n    ).first()\n    \n    if not customer:\n        print(f\"[USAGE] Customer {customer_id} not found for task increment\")\n        return False\n    \n    old_count = customer.tasks_this_period or 0\n    customer.tasks_this_period = old_count + 1\n    session.add(customer)\n    \n    status = get_customer_plan_status(customer)\n    if status.is_trial and customer.tasks_this_period > status.tasks_limit:\n        print(f\"[USAGE][SOFT_CAP] Customer {customer_id} exceeded task limit: {customer.tasks_this_period}/{status.tasks_limit}\")\n    else:\n        print(f\"[USAGE] Customer {customer_id} tasks: {customer.tasks_this_period}/{status.tasks_limit if status.is_trial else 'unlimited'}\")\n    \n    return True\n\n\ndef increment_leads_used(session: Session, customer_id: int) -> bool:\n    \"\"\"\n    Increment lead usage counter for a customer (soft cap - display only).\n    \n    This function ALWAYS increments the counter regardless of limits.\n    Soft caps are enforced at the UI layer for display only.\n    \n    Returns True if increment was successful, False if customer not found.\n    Safe to call multiple times - handles missing customers gracefully.\n    \"\"\"\n    if not customer_id:\n        return False\n    \n    customer = session.exec(\n        select(Customer).where(Customer.id == customer_id)\n    ).first()\n    \n    if not customer:\n        print(f\"[USAGE] Customer {customer_id} not found for lead increment\")\n        return False\n    \n    old_count = customer.leads_this_period or 0\n    customer.leads_this_period = old_count + 1\n    session.add(customer)\n    \n    status = get_customer_plan_status(customer)\n    if status.is_trial and customer.leads_this_period > status.leads_limit:\n        print(f\"[USAGE][SOFT_CAP] Customer {customer_id} exceeded lead limit: {customer.leads_this_period}/{status.leads_limit}\")\n    else:\n        print(f\"[USAGE] Customer {customer_id} leads: {customer.leads_this_period}/{status.leads_limit if status.is_trial else 'unlimited'}\")\n    \n    return True\n\n\ndef get_stripe_product_id() -> Optional[str]:\n    \"\"\"Get Stripe product ID from environment.\"\"\"\n    return os.getenv(\"STRIPE_PRODUCT_ID\")\n\n\ndef get_stripe_price_id() -> Optional[str]:\n    \"\"\"Get Stripe price ID from environment.\"\"\"\n    return os.getenv(\"STRIPE_PRICE_ID\")\n\n\ndef set_stripe_product_id(product_id: str) -> None:\n    \"\"\"Store Stripe product ID (in-memory for this session).\"\"\"\n    os.environ[\"STRIPE_PRODUCT_ID\"] = product_id\n\n\ndef set_stripe_price_id(price_id: str) -> None:\n    \"\"\"Store Stripe price ID (in-memory for this session).\"\"\"\n    os.environ[\"STRIPE_PRICE_ID\"] = price_id\n\n\ndef bootstrap_stripe_subscription_product() -> Dict[str, Any]:\n    \"\"\"\n    Bootstrap Stripe product and price for subscription.\n    Creates them if they don't exist.\n    \n    Returns:\n        Dict with product_id, price_id, and status\n    \"\"\"\n    from stripe_utils import is_stripe_enabled, get_stripe_api_key\n    import requests\n    \n    result = {\n        \"success\": False,\n        \"product_id\": None,\n        \"price_id\": None,\n        \"message\": \"\",\n        \"created_product\": False,\n        \"created_price\": False\n    }\n    \n    if not is_stripe_enabled():\n        result[\"message\"] = \"Stripe disabled - skipping product bootstrap\"\n        return result\n    \n    api_key = get_stripe_api_key()\n    if not api_key:\n        result[\"message\"] = \"No Stripe API key - skipping product bootstrap\"\n        return result\n    \n    existing_product_id = get_stripe_product_id()\n    existing_price_id = get_stripe_price_id()\n    \n    if existing_product_id and existing_price_id:\n        result[\"success\"] = True\n        result[\"product_id\"] = existing_product_id\n        result[\"price_id\"] = existing_price_id\n        result[\"message\"] = f\"Using existing Stripe product ...{existing_product_id[-4:]} and price ...{existing_price_id[-4:]}\"\n        print(f\"[STRIPE][SUBSCRIPTION] {result['message']}\")\n        return result\n    \n    try:\n        product_id = existing_product_id\n        if not product_id:\n            product_response = requests.post(\n                \"https://api.stripe.com/v1/products\",\n                auth=(str(api_key), \"\"),\n                data={\n                    \"name\": \"HossAgent Subscription\",\n                    \"description\": \"Full access to HossAgent autonomous business engine - $99/month\"\n                },\n                timeout=30\n            )\n            \n            if product_response.status_code != 200:\n                result[\"message\"] = f\"Failed to create Stripe product: {product_response.text[:100]}\"\n                print(f\"[STRIPE][SUBSCRIPTION][ERROR] {result['message']}\")\n                return result\n            \n            product_data = product_response.json()\n            product_id = product_data[\"id\"]\n            set_stripe_product_id(product_id)\n            result[\"created_product\"] = True\n            print(f\"[STRIPE][SUBSCRIPTION] Created product: ...{product_id[-4:]}\")\n        \n        price_id = existing_price_id\n        if not price_id:\n            price_response = requests.post(\n                \"https://api.stripe.com/v1/prices\",\n                auth=(str(api_key), \"\"),\n                data={\n                    \"product\": product_id,\n                    \"unit_amount\": 9900,  # $99.00\n                    \"currency\": \"usd\",\n                    \"recurring[interval]\": \"month\"\n                },\n                timeout=30\n            )\n            \n            if price_response.status_code != 200:\n                result[\"message\"] = f\"Failed to create Stripe price: {price_response.text[:100]}\"\n                print(f\"[STRIPE][SUBSCRIPTION][ERROR] {result['message']}\")\n                return result\n            \n            price_data = price_response.json()\n            price_id = price_data[\"id\"]\n            set_stripe_price_id(price_id)\n            result[\"created_price\"] = True\n            print(f\"[STRIPE][SUBSCRIPTION] Created price: ...{price_id[-4:]}\")\n        \n        result[\"success\"] = True\n        result[\"product_id\"] = product_id\n        result[\"price_id\"] = price_id\n        result[\"message\"] = f\"Stripe subscription ready: product ...{product_id[-4:]}, price ...{price_id[-4:]}\"\n        print(f\"[STRIPE][SUBSCRIPTION] {result['message']}\")\n        return result\n        \n    except Exception as e:\n        result[\"message\"] = f\"Error bootstrapping Stripe: {str(e)}\"\n        print(f\"[STRIPE][SUBSCRIPTION][ERROR] {result['message']}\")\n        return result\n\n\ndef create_stripe_customer(\n    customer_id: int,\n    email: str,\n    company: str\n) -> Tuple[Optional[str], Optional[str]]:\n    \"\"\"\n    Create a Stripe customer if needed.\n    \n    Returns:\n        (stripe_customer_id, error)\n    \"\"\"\n    from stripe_utils import is_stripe_enabled, get_stripe_api_key\n    import requests\n    \n    if not is_stripe_enabled():\n        return None, \"Stripe disabled\"\n    \n    api_key = get_stripe_api_key()\n    if not api_key:\n        return None, \"No Stripe API key\"\n    \n    try:\n        response = requests.post(\n            \"https://api.stripe.com/v1/customers\",\n            auth=(str(api_key), \"\"),\n            data={\n                \"email\": email,\n                \"name\": company,\n                \"metadata[hossagent_customer_id]\": str(customer_id)\n            },\n            timeout=30\n        )\n        \n        if response.status_code != 200:\n            return None, f\"Failed to create Stripe customer: {response.text[:100]}\"\n        \n        data = response.json()\n        stripe_customer_id = data[\"id\"]\n        print(f\"[STRIPE][CUSTOMER] Created Stripe customer ...{stripe_customer_id[-4:]} for HossAgent customer {customer_id}\")\n        return stripe_customer_id, None\n        \n    except Exception as e:\n        return None, str(e)\n\n\ndef create_subscription(\n    stripe_customer_id: str,\n    customer_id: int\n) -> Tuple[Optional[str], Optional[str]]:\n    \"\"\"\n    Create a Stripe subscription for a customer.\n    \n    Returns:\n        (subscription_id, error)\n    \"\"\"\n    from stripe_utils import is_stripe_enabled, get_stripe_api_key\n    import requests\n    \n    if not is_stripe_enabled():\n        return None, \"Stripe disabled\"\n    \n    api_key = get_stripe_api_key()\n    if not api_key:\n        return None, \"No Stripe API key\"\n    \n    price_id = get_stripe_price_id()\n    if not price_id:\n        return None, \"No Stripe price ID - run bootstrap first\"\n    \n    try:\n        response = requests.post(\n            \"https://api.stripe.com/v1/subscriptions\",\n            auth=(str(api_key), \"\"),\n            data={\n                \"customer\": stripe_customer_id,\n                \"items[0][price]\": price_id,\n                \"metadata[hossagent_customer_id]\": str(customer_id)\n            },\n            timeout=30\n        )\n        \n        if response.status_code != 200:\n            return None, f\"Failed to create subscription: {response.text[:100]}\"\n        \n        data = response.json()\n        subscription_id = data[\"id\"]\n        print(f\"[STRIPE][SUBSCRIPTION] Created subscription ...{subscription_id[-4:]} for customer {customer_id}\")\n        return subscription_id, None\n        \n    except Exception as e:\n        return None, str(e)\n\n\ndef get_subscription_status() -> Dict[str, Any]:\n    \"\"\"Get current subscription configuration status for admin display.\"\"\"\n    from stripe_utils import is_stripe_enabled, get_stripe_api_key, get_stripe_webhook_secret\n    \n    product_id = get_stripe_product_id()\n    price_id = get_stripe_price_id()\n    api_key = get_stripe_api_key()\n    webhook_secret = get_stripe_webhook_secret()\n    \n    return {\n        \"enabled\": is_stripe_enabled(),\n        \"api_key_present\": api_key is not None and len(api_key) > 0,\n        \"webhook_secret_present\": webhook_secret is not None and len(webhook_secret) > 0,\n        \"product_id\": f\"...{product_id[-4:]}\" if product_id else None,\n        \"price_id\": f\"...{price_id[-4:]}\" if price_id else None,\n        \"product_ready\": product_id is not None,\n        \"price_ready\": price_id is not None,\n        \"subscription_price\": \"$99/month\",\n        \"trial_days\": TRIAL_DAYS,\n        \"trial_task_limit\": TRIAL_TASK_LIMIT,\n        \"trial_lead_limit\": TRIAL_LEAD_LIMIT\n    }\n\n\ndef should_force_email_dry_run(customer: Customer) -> bool:\n    \"\"\"\n    Check if email should be forced to DRY_RUN for this customer.\n    Trial users always get DRY_RUN.\n    \"\"\"\n    status = get_customer_plan_status(customer)\n    return not status.can_send_real_email\n\n\ndef should_disable_billing_for_customer(customer: Customer) -> bool:\n    \"\"\"\n    Check if billing should be disabled for this customer.\n    Trial users don't get Stripe billing.\n    \"\"\"\n    status = get_customer_plan_status(customer)\n    return not status.can_use_billing\n\n\ndef should_disable_autopilot_for_customer(customer: Customer) -> bool:\n    \"\"\"\n    Check if autopilot should be disabled for this customer.\n    Trial users don't get autopilot.\n    \"\"\"\n    status = get_customer_plan_status(customer)\n    return not status.can_use_autopilot\n\n\ndef get_stripe_price_id_pro() -> Optional[str]:\n    \"\"\"Get the Stripe Price ID for the Pro subscription plan.\"\"\"\n    price_id = os.getenv(\"STRIPE_PRICE_ID_PRO\")\n    if price_id:\n        return price_id\n    return get_stripe_price_id()\n\n\ndef get_or_create_subscription_checkout_link(\n    customer: Customer,\n    success_url: Optional[str] = None,\n    cancel_url: Optional[str] = None\n) -> Tuple[bool, Optional[str], str, Optional[str]]:\n    \"\"\"\n    Get or create a Stripe Checkout session URL for subscription.\n    \n    Args:\n        customer: The customer to create checkout for\n        success_url: URL to redirect after successful payment\n        cancel_url: URL to redirect if payment cancelled\n    \n    Returns:\n        (success, url, mode, error)\n        - success: True if URL is available\n        - url: The checkout URL or None\n        - mode: 'live', 'dry_run', or 'disabled'\n        - error: Error message if any\n    \"\"\"\n    from stripe_utils import is_stripe_enabled, get_stripe_api_key\n    import requests\n    \n    plan_status = get_customer_plan_status(customer)\n    if plan_status.is_paid:\n        return False, None, \"already_paid\", \"Customer already has an active subscription\"\n    \n    if not is_stripe_enabled():\n        print(f\"[STRIPE][SUBSCRIPTION][DRY_RUN_FALLBACK] Stripe disabled for customer {customer.id}\")\n        return False, None, \"disabled\", \"Online billing not configured\"\n    \n    api_key = get_stripe_api_key()\n    if not api_key:\n        print(f\"[STRIPE][SUBSCRIPTION][DRY_RUN_FALLBACK] No API key for customer {customer.id}\")\n        return False, None, \"disabled\", \"Stripe API key not configured\"\n    \n    price_id = get_stripe_price_id_pro()\n    if not price_id:\n        print(f\"[STRIPE][SUBSCRIPTION][DRY_RUN_FALLBACK] No price ID for customer {customer.id}\")\n        return False, None, \"disabled\", \"Subscription price not configured\"\n    \n    if not success_url:\n        success_url = f\"/portal/{customer.public_token}?payment=success\"\n    if not cancel_url:\n        cancel_url = f\"/portal/{customer.public_token}?payment=cancelled\"\n    \n    try:\n        stripe_customer_id = customer.stripe_customer_id\n        if not stripe_customer_id:\n            stripe_customer_id, err = create_stripe_customer(\n                customer_id=customer.id,\n                email=customer.contact_email,\n                company=customer.company\n            )\n            if err:\n                return False, None, \"error\", f\"Failed to create Stripe customer: {err}\"\n        \n        response = requests.post(\n            \"https://api.stripe.com/v1/checkout/sessions\",\n            auth=(str(api_key), \"\"),\n            data={\n                \"customer\": stripe_customer_id,\n                \"mode\": \"subscription\",\n                \"line_items[0][price]\": price_id,\n                \"line_items[0][quantity]\": 1,\n                \"success_url\": success_url,\n                \"cancel_url\": cancel_url,\n                \"metadata[hossagent_customer_id]\": str(customer.id),\n                \"metadata[public_token]\": customer.public_token or \"\",\n                \"subscription_data[metadata][hossagent_customer_id]\": str(customer.id)\n            },\n            timeout=30\n        )\n        \n        if response.status_code != 200:\n            error_text = response.text[:200]\n            print(f\"[STRIPE][SUBSCRIPTION][ERROR] Checkout creation failed: {error_text}\")\n            return False, None, \"error\", f\"Failed to create checkout: {error_text}\"\n        \n        data = response.json()\n        checkout_url = data.get(\"url\")\n        session_id = data.get(\"id\")\n        \n        print(f\"[STRIPE][SUBSCRIPTION] Created checkout session {session_id[-8:]} for customer {customer.id}\")\n        return True, checkout_url, \"live\", None\n        \n    except Exception as e:\n        error_msg = str(e)\n        print(f\"[STRIPE][SUBSCRIPTION][ERROR] Exception creating checkout: {error_msg}\")\n        return False, None, \"error\", error_msg\n\n\ndef create_billing_portal_link(\n    customer: Customer,\n    return_url: Optional[str] = None\n) -> Tuple[bool, Optional[str], str, Optional[str]]:\n    \"\"\"\n    Create a Stripe Customer Portal link for managing billing.\n    \n    Args:\n        customer: The customer\n        return_url: URL to return to after portal session\n    \n    Returns:\n        (success, url, mode, error)\n    \"\"\"\n    from stripe_utils import is_stripe_enabled, get_stripe_api_key\n    import requests\n    \n    if not is_stripe_enabled():\n        return False, None, \"disabled\", \"Stripe not configured\"\n    \n    api_key = get_stripe_api_key()\n    if not api_key:\n        return False, None, \"disabled\", \"No Stripe API key\"\n    \n    stripe_customer_id = customer.stripe_customer_id\n    if not stripe_customer_id:\n        return False, None, \"error\", \"No Stripe customer ID\"\n    \n    if not return_url:\n        return_url = f\"/portal/{customer.public_token}\"\n    \n    try:\n        response = requests.post(\n            \"https://api.stripe.com/v1/billing_portal/sessions\",\n            auth=(str(api_key), \"\"),\n            data={\n                \"customer\": stripe_customer_id,\n                \"return_url\": return_url\n            },\n            timeout=30\n        )\n        \n        if response.status_code != 200:\n            error_text = response.text[:200]\n            print(f\"[STRIPE][PORTAL][ERROR] Portal creation failed: {error_text}\")\n            return False, None, \"error\", f\"Failed to create portal: {error_text}\"\n        \n        data = response.json()\n        portal_url = data.get(\"url\")\n        \n        print(f\"[STRIPE][PORTAL] Created billing portal for customer {customer.id}\")\n        return True, portal_url, \"live\", None\n        \n    except Exception as e:\n        return False, None, \"error\", str(e)\n","path":null,"size_bytes":26227,"size_tokens":null},"auth_utils.py":{"content":"\"\"\"\nAuthentication Utilities for HossAgent.\n\nHandles:\n- Password hashing with bcrypt\n- Session management with signed cookies\n- Admin authentication\n- Customer authentication\n\nEnvironment Variables:\n  SESSION_SECRET - Secret key for signing session cookies (required)\n  ADMIN_PASSWORD - Password for admin console access (default: hoss2024)\n\"\"\"\nimport os\nimport secrets\nimport bcrypt\nfrom datetime import datetime, timedelta\nfrom typing import Optional, Tuple\nfrom itsdangerous import URLSafeTimedSerializer, BadSignature, SignatureExpired\nfrom sqlmodel import Session, select\n\nfrom models import Customer\n\n\nSESSION_COOKIE_NAME = \"hossagent_session\"\nSESSION_MAX_AGE = 60 * 60 * 24 * 14  # 14 days in seconds\nADMIN_COOKIE_NAME = \"hossagent_admin\"\nADMIN_SESSION_MAX_AGE = 60 * 60 * 8  # 8 hours\n\n\ndef get_session_secret() -> str:\n    \"\"\"Get or generate session secret.\"\"\"\n    secret = os.getenv(\"SESSION_SECRET\")\n    if not secret:\n        secret = secrets.token_hex(32)\n        os.environ[\"SESSION_SECRET\"] = secret\n        print(\"[AUTH][WARNING] No SESSION_SECRET set - using generated secret (sessions won't persist across restarts)\")\n    return secret\n\n\ndef get_admin_password() -> str:\n    \"\"\"Get admin password from environment.\"\"\"\n    return os.getenv(\"ADMIN_PASSWORD\", \"hoss2024\")\n\n\ndef get_serializer() -> URLSafeTimedSerializer:\n    \"\"\"Get the session serializer.\"\"\"\n    return URLSafeTimedSerializer(get_session_secret())\n\n\ndef hash_password(password: str) -> str:\n    \"\"\"Hash a password using bcrypt.\"\"\"\n    salt = bcrypt.gensalt()\n    hashed = bcrypt.hashpw(password.encode('utf-8'), salt)\n    return hashed.decode('utf-8')\n\n\ndef verify_password(password: str, password_hash: str) -> bool:\n    \"\"\"Verify a password against its hash.\"\"\"\n    try:\n        return bcrypt.checkpw(password.encode('utf-8'), password_hash.encode('utf-8'))\n    except Exception:\n        return False\n\n\ndef create_customer_session(customer_id: int) -> str:\n    \"\"\"Create a signed session token for a customer.\"\"\"\n    serializer = get_serializer()\n    data = {\n        \"customer_id\": customer_id,\n        \"type\": \"customer\",\n        \"created_at\": datetime.utcnow().isoformat()\n    }\n    return serializer.dumps(data)\n\n\ndef verify_customer_session(token: str) -> Optional[int]:\n    \"\"\"\n    Verify a customer session token.\n    \n    Returns:\n        customer_id if valid, None if invalid or expired\n    \"\"\"\n    if not token:\n        return None\n    \n    serializer = get_serializer()\n    try:\n        data = serializer.loads(token, max_age=SESSION_MAX_AGE)\n        if data.get(\"type\") == \"customer\":\n            return data.get(\"customer_id\")\n    except (BadSignature, SignatureExpired):\n        pass\n    return None\n\n\ndef create_admin_session() -> str:\n    \"\"\"Create a signed session token for admin.\"\"\"\n    serializer = get_serializer()\n    data = {\n        \"type\": \"admin\",\n        \"created_at\": datetime.utcnow().isoformat()\n    }\n    return serializer.dumps(data)\n\n\ndef verify_admin_session(token: str) -> bool:\n    \"\"\"\n    Verify an admin session token.\n    \n    Returns:\n        True if valid, False if invalid or expired\n    \"\"\"\n    if not token:\n        return False\n    \n    serializer = get_serializer()\n    try:\n        data = serializer.loads(token, max_age=ADMIN_SESSION_MAX_AGE)\n        return data.get(\"type\") == \"admin\"\n    except (BadSignature, SignatureExpired):\n        return False\n\n\ndef authenticate_customer(\n    db_session: Session,\n    email: str,\n    password: str\n) -> Tuple[Optional[Customer], Optional[str]]:\n    \"\"\"\n    Authenticate a customer by email and password.\n    \n    Returns:\n        (customer, error_message)\n    \"\"\"\n    customer = db_session.exec(\n        select(Customer).where(Customer.contact_email == email.lower().strip())\n    ).first()\n    \n    if not customer:\n        return None, \"No account found with that email\"\n    \n    if not customer.password_hash:\n        return None, \"Account not set up for login - please contact support\"\n    \n    if not verify_password(password, customer.password_hash):\n        return None, \"Incorrect password\"\n    \n    return customer, None\n\n\ndef generate_public_token() -> str:\n    \"\"\"Generate a secure public token for portal access.\"\"\"\n    return secrets.token_urlsafe(16)\n\n\ndef get_customer_from_session(\n    db_session: Session,\n    session_token: Optional[str]\n) -> Optional[Customer]:\n    \"\"\"\n    Get customer from session token.\n    \n    Returns:\n        Customer if valid session, None otherwise\n    \"\"\"\n    customer_id = verify_customer_session(session_token)\n    if not customer_id:\n        return None\n    \n    return db_session.exec(\n        select(Customer).where(Customer.id == customer_id)\n    ).first()\n\n\ndef get_customer_from_token(\n    db_session: Session,\n    public_token: str\n) -> Optional[Customer]:\n    \"\"\"\n    Get customer from public token.\n    \n    Returns:\n        Customer if valid token, None otherwise\n    \"\"\"\n    if not public_token or len(public_token) < 10:\n        return None\n    \n    return db_session.exec(\n        select(Customer).where(Customer.public_token == public_token)\n    ).first()\n","path":null,"size_bytes":5092,"size_tokens":null},"email_discovery.py":{"content":"\"\"\"\nAutonomous Email Discovery Module\n\nScrapes company websites to find contact email addresses without using paid APIs.\nThis module provides a fallback email discovery mechanism when Hunter.io and \nClearbit are disabled.\n\nStrategy:\n1. Find contact/about pages on the company website\n2. Extract email addresses using regex patterns\n3. Validate and score discovered emails\n4. Prefer business emails over generic ones (info@, contact@, hello@)\n\nRate limiting and polite scraping practices are enforced.\n\"\"\"\n\nimport os\nimport re\nimport time\nimport json\nimport random\nimport hashlib\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass, field\nfrom typing import Optional, List, Dict, Tuple, Set\nfrom urllib.parse import urljoin, urlparse\nimport requests\nfrom requests.exceptions import RequestException, Timeout\n\n\nDISCOVERY_TIMEOUT = int(os.getenv(\"EMAIL_DISCOVERY_TIMEOUT\", \"10\"))\nDISCOVERY_MAX_PAGES = int(os.getenv(\"EMAIL_DISCOVERY_MAX_PAGES\", \"5\"))\nDISCOVERY_DELAY_MIN = float(os.getenv(\"EMAIL_DISCOVERY_DELAY_MIN\", \"1.0\"))\nDISCOVERY_DELAY_MAX = float(os.getenv(\"EMAIL_DISCOVERY_DELAY_MAX\", \"3.0\"))\nDISCOVERY_DRY_RUN = os.getenv(\"EMAIL_DISCOVERY_DRY_RUN\", \"false\").lower() == \"true\"\nDISCOVERY_ENABLED = os.getenv(\"EMAIL_DISCOVERY_ENABLED\", \"true\").lower() == \"true\"\n\nCONTACT_PAGE_PATTERNS = [\n    \"/contact\",\n    \"/contact-us\",\n    \"/contactus\",\n    \"/about\",\n    \"/about-us\",\n    \"/aboutus\",\n    \"/team\",\n    \"/our-team\",\n    \"/staff\",\n    \"/people\",\n    \"/leadership\",\n    \"/get-in-touch\",\n    \"/reach-us\",\n    \"/connect\",\n    \"/info\",\n    \"/company\",\n    \"/support\",\n    \"/help\",\n]\n\nEMAIL_REGEX = re.compile(\n    r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n)\n\nMAILTO_REGEX = re.compile(\n    r'mailto:([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,})',\n    re.IGNORECASE\n)\n\nGENERIC_EMAIL_PREFIXES = [\n    \"info\", \"contact\", \"hello\", \"support\", \"help\", \"sales\", \"enquiries\",\n    \"inquiries\", \"admin\", \"office\", \"team\", \"general\", \"mail\", \"email\",\n    \"customerservice\", \"customer-service\", \"customercare\", \"feedback\",\n    \"service\", \"helpdesk\", \"noreply\", \"no-reply\", \"webmaster\", \"enquiry\"\n]\n\nINVALID_EMAIL_PATTERNS = [\n    r\".*@example\\.com$\",\n    r\".*@test\\.com$\",\n    r\".*@localhost$\",\n    r\".*@.*\\.png$\",\n    r\".*@.*\\.jpg$\",\n    r\".*@.*\\.gif$\",\n    r\".*\\.wixpress\\.com$\",\n    r\".*sentry\\.io$\",\n    r\"noreply@.*\",\n    r\"no-reply@.*\",\n    r\"donotreply@.*\",\n    r\"do-not-reply@.*\",\n    r\"unsubscribe@.*\",\n    r\"mailer-daemon@.*\",\n    r\"postmaster@.*\",\n]\n\nUSER_AGENTS = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15\",\n]\n\n\n@dataclass\nclass DiscoveredEmail:\n    \"\"\"Represents a discovered email with metadata.\"\"\"\n    email: str\n    source_url: str\n    confidence: float  \n    is_generic: bool\n    domain_match: bool  \n    discovered_at: datetime = field(default_factory=datetime.utcnow)\n    \n    def to_dict(self) -> Dict:\n        return {\n            \"email\": self.email,\n            \"source_url\": self.source_url,\n            \"confidence\": self.confidence,\n            \"is_generic\": self.is_generic,\n            \"domain_match\": self.domain_match,\n            \"discovered_at\": self.discovered_at.isoformat()\n        }\n\n\n@dataclass\nclass DiscoveryResult:\n    \"\"\"Result of email discovery attempt.\"\"\"\n    success: bool\n    domain: str\n    emails: List[DiscoveredEmail] = field(default_factory=list)\n    best_email: Optional[str] = None\n    pages_checked: int = 0\n    error: Optional[str] = None\n    duration_ms: int = 0\n    dry_run: bool = False\n    \n    def to_dict(self) -> Dict:\n        return {\n            \"success\": self.success,\n            \"domain\": self.domain,\n            \"emails\": [e.to_dict() for e in self.emails],\n            \"best_email\": self.best_email,\n            \"pages_checked\": self.pages_checked,\n            \"error\": self.error,\n            \"duration_ms\": self.duration_ms,\n            \"dry_run\": self.dry_run\n        }\n\n\n_domain_cache: Dict[str, Tuple[DiscoveryResult, datetime]] = {}\nCACHE_TTL_HOURS = 24\n\n\ndef _get_cached_result(domain: str) -> Optional[DiscoveryResult]:\n    \"\"\"Check cache for recent discovery result.\"\"\"\n    if domain in _domain_cache:\n        result, cached_at = _domain_cache[domain]\n        if datetime.utcnow() - cached_at < timedelta(hours=CACHE_TTL_HOURS):\n            return result\n        else:\n            del _domain_cache[domain]\n    return None\n\n\ndef _cache_result(domain: str, result: DiscoveryResult) -> None:\n    \"\"\"Cache discovery result.\"\"\"\n    _domain_cache[domain] = (result, datetime.utcnow())\n\n\ndef _get_random_user_agent() -> str:\n    \"\"\"Get a random user agent for requests.\"\"\"\n    return random.choice(USER_AGENTS)\n\n\ndef _polite_delay() -> None:\n    \"\"\"Apply a polite delay between requests.\"\"\"\n    delay = random.uniform(DISCOVERY_DELAY_MIN, DISCOVERY_DELAY_MAX)\n    time.sleep(delay)\n\n\ndef _normalize_domain(domain: str) -> str:\n    \"\"\"Normalize domain to standard format.\"\"\"\n    domain = domain.lower().strip()\n    if domain.startswith(\"http://\") or domain.startswith(\"https://\"):\n        parsed = urlparse(domain)\n        domain = parsed.netloc\n    domain = domain.lstrip(\"www.\")\n    return domain\n\n\ndef _build_base_url(domain: str) -> str:\n    \"\"\"Build base URL from domain.\"\"\"\n    domain = _normalize_domain(domain)\n    return f\"https://{domain}\"\n\n\ndef _is_valid_email(email: str, target_domain: str) -> bool:\n    \"\"\"Check if email is valid and not in blocklist.\"\"\"\n    email = email.lower().strip()\n    \n    for pattern in INVALID_EMAIL_PATTERNS:\n        if re.match(pattern, email, re.IGNORECASE):\n            return False\n    \n    if len(email) < 6 or len(email) > 254:\n        return False\n    \n    if email.count(\"@\") != 1:\n        return False\n    \n    local, domain = email.rsplit(\"@\", 1)\n    if len(local) < 1 or len(domain) < 3:\n        return False\n    \n    return True\n\n\ndef _is_generic_email(email: str) -> bool:\n    \"\"\"Check if email is a generic business email.\"\"\"\n    local = email.split(\"@\")[0].lower()\n    return any(local.startswith(prefix) or local == prefix for prefix in GENERIC_EMAIL_PREFIXES)\n\n\ndef _email_matches_domain(email: str, target_domain: str) -> bool:\n    \"\"\"Check if email domain matches target domain.\"\"\"\n    email_domain = email.split(\"@\")[1].lower()\n    target = _normalize_domain(target_domain)\n    return email_domain == target or email_domain.endswith(f\".{target}\")\n\n\ndef classify_email(email: str) -> str:\n    \"\"\"\n    ARCHANGEL: Classify email type - generic vs person-like.\n    \n    Returns: 'generic', 'person', 'other'\n    \"\"\"\n    local = email.split(\"@\")[0].lower()\n    \n    if _is_generic_email(email):\n        return \"generic\"\n    \n    if \".\" in local or any(c.isupper() for c in local):\n        return \"person\"\n    \n    return \"other\"\n\n\ndef _calculate_confidence(email: str, target_domain: str, source_url: str) -> float:\n    \"\"\"\n    ARCHANGEL: Calculate confidence score for discovered email.\n    \n    Factors:\n    - Domain match (30%)\n    - Email pattern (20%)\n    - Page context (10%)\n    - Email type (40% bonus for person-like)\n    \"\"\"\n    score = 0.5  \n    \n    if _email_matches_domain(email, target_domain):\n        score += 0.3\n    else:\n        score -= 0.2\n    \n    email_type = classify_email(email)\n    if email_type == \"person\":\n        score += 0.4\n    elif email_type == \"generic\":\n        score += 0.1\n    else:\n        score += 0.0\n    \n    source_path = urlparse(source_url).path.lower()\n    if any(pattern in source_path for pattern in [\"/contact\", \"/about\", \"/team\"]):\n        score += 0.1\n    \n    return min(1.0, max(0.0, score))\n\n\ndef _extract_emails_from_html(html: str, target_domain: str) -> List[str]:\n    \"\"\"Extract email addresses from HTML content.\"\"\"\n    emails = set()\n    \n    mailto_matches = MAILTO_REGEX.findall(html)\n    emails.update(mailto_matches)\n    \n    text_matches = EMAIL_REGEX.findall(html)\n    emails.update(text_matches)\n    \n    valid_emails = []\n    for email in emails:\n        email = email.lower().strip()\n        if _is_valid_email(email, target_domain):\n            valid_emails.append(email)\n    \n    return list(set(valid_emails))\n\n\ndef _fetch_page(url: str) -> Optional[str]:\n    \"\"\"Fetch page content with error handling.\"\"\"\n    try:\n        headers = {\n            \"User-Agent\": _get_random_user_agent(),\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n            \"Accept-Language\": \"en-US,en;q=0.5\",\n            \"Accept-Encoding\": \"gzip, deflate\",\n            \"Connection\": \"keep-alive\",\n            \"Upgrade-Insecure-Requests\": \"1\",\n        }\n        \n        response = requests.get(\n            url,\n            headers=headers,\n            timeout=DISCOVERY_TIMEOUT,\n            allow_redirects=True,\n            verify=True\n        )\n        \n        if response.status_code == 200:\n            return response.text\n        else:\n            print(f\"[EMAIL_DISCOVERY] HTTP {response.status_code} for {url}\")\n            return None\n            \n    except Timeout:\n        print(f\"[EMAIL_DISCOVERY] Timeout fetching {url}\")\n        return None\n    except RequestException as e:\n        print(f\"[EMAIL_DISCOVERY] Request error for {url}: {str(e)[:100]}\")\n        return None\n\n\ndef _find_contact_pages(base_url: str, homepage_html: str) -> List[str]:\n    \"\"\"Find contact page URLs from homepage and common patterns.\"\"\"\n    contact_urls = []\n    \n    for pattern in CONTACT_PAGE_PATTERNS:\n        contact_urls.append(urljoin(base_url, pattern))\n    \n    href_pattern = re.compile(r'href=[\"\\']([^\"\\']*(?:contact|about|team)[^\"\\']*)[\"\\']', re.IGNORECASE)\n    matches = href_pattern.findall(homepage_html)\n    for match in matches[:10]:  \n        if match.startswith(\"http\"):\n            if urlparse(match).netloc == urlparse(base_url).netloc:\n                contact_urls.append(match)\n        elif match.startswith(\"/\"):\n            contact_urls.append(urljoin(base_url, match))\n    \n    seen = set()\n    unique_urls = []\n    for url in contact_urls:\n        normalized = url.rstrip(\"/\").lower()\n        if normalized not in seen:\n            seen.add(normalized)\n            unique_urls.append(url)\n    \n    return unique_urls[:DISCOVERY_MAX_PAGES]\n\n\ndef discover_emails(domain: str) -> DiscoveryResult:\n    \"\"\"\n    Discover email addresses from a company website.\n    \n    Args:\n        domain: Company domain (e.g., \"example.com\")\n        \n    Returns:\n        DiscoveryResult with found emails and metadata\n    \"\"\"\n    start_time = time.time()\n    domain = _normalize_domain(domain)\n    \n    if not DISCOVERY_ENABLED:\n        return DiscoveryResult(\n            success=False,\n            domain=domain,\n            error=\"Email discovery disabled\",\n            dry_run=True\n        )\n    \n    cached = _get_cached_result(domain)\n    if cached:\n        print(f\"[EMAIL_DISCOVERY] Cache hit for {domain}\")\n        return cached\n    \n    if DISCOVERY_DRY_RUN:\n        print(f\"[EMAIL_DISCOVERY][DRY_RUN] Would scrape {domain}\")\n        result = DiscoveryResult(\n            success=False,\n            domain=domain,\n            error=\"Dry run mode - no actual scraping\",\n            dry_run=True\n        )\n        return result\n    \n    print(f\"[EMAIL_DISCOVERY] Starting discovery for {domain}\")\n    \n    base_url = _build_base_url(domain)\n    all_emails: List[DiscoveredEmail] = []\n    pages_checked = 0\n    \n    homepage_html = _fetch_page(base_url)\n    if homepage_html:\n        pages_checked += 1\n        emails = _extract_emails_from_html(homepage_html, domain)\n        for email in emails:\n            all_emails.append(DiscoveredEmail(\n                email=email,\n                source_url=base_url,\n                confidence=_calculate_confidence(email, domain, base_url),\n                is_generic=_is_generic_email(email),\n                domain_match=_email_matches_domain(email, domain)\n            ))\n        \n        contact_urls = _find_contact_pages(base_url, homepage_html)\n        \n        for url in contact_urls:\n            if pages_checked >= DISCOVERY_MAX_PAGES:\n                break\n            \n            _polite_delay()\n            \n            page_html = _fetch_page(url)\n            if page_html:\n                pages_checked += 1\n                emails = _extract_emails_from_html(page_html, domain)\n                for email in emails:\n                    if not any(e.email == email for e in all_emails):\n                        all_emails.append(DiscoveredEmail(\n                            email=email,\n                            source_url=url,\n                            confidence=_calculate_confidence(email, domain, url),\n                            is_generic=_is_generic_email(email),\n                            domain_match=_email_matches_domain(email, domain)\n                        ))\n    else:\n        www_url = f\"https://www.{domain}\"\n        homepage_html = _fetch_page(www_url)\n        if homepage_html:\n            pages_checked += 1\n            emails = _extract_emails_from_html(homepage_html, domain)\n            for email in emails:\n                all_emails.append(DiscoveredEmail(\n                    email=email,\n                    source_url=www_url,\n                    confidence=_calculate_confidence(email, domain, www_url),\n                    is_generic=_is_generic_email(email),\n                    domain_match=_email_matches_domain(email, domain)\n                ))\n    \n    all_emails.sort(key=lambda e: (-e.confidence, e.is_generic))\n    \n    best_email = None\n    if all_emails:\n        domain_matches = [e for e in all_emails if e.domain_match]\n        if domain_matches:\n            personal = [e for e in domain_matches if not e.is_generic]\n            if personal:\n                best_email = personal[0].email\n            else:\n                best_email = domain_matches[0].email\n        else:\n            best_email = all_emails[0].email\n    \n    duration_ms = int((time.time() - start_time) * 1000)\n    \n    result = DiscoveryResult(\n        success=len(all_emails) > 0,\n        domain=domain,\n        emails=all_emails,\n        best_email=best_email,\n        pages_checked=pages_checked,\n        duration_ms=duration_ms\n    )\n    \n    _cache_result(domain, result)\n    \n    if result.success:\n        print(f\"[EMAIL_DISCOVERY] Found {len(all_emails)} email(s) for {domain}, best: {best_email}\")\n    else:\n        print(f\"[EMAIL_DISCOVERY] No emails found for {domain} (checked {pages_checked} pages)\")\n    \n    return result\n\n\ndef discover_emails_batch(domains: List[str], max_concurrent: int = 1) -> Dict[str, DiscoveryResult]:\n    \"\"\"\n    Discover emails for multiple domains.\n    \n    Args:\n        domains: List of domains to check\n        max_concurrent: Max concurrent requests (currently sequential for politeness)\n        \n    Returns:\n        Dict mapping domain to DiscoveryResult\n    \"\"\"\n    results = {}\n    \n    for domain in domains:\n        results[domain] = discover_emails(domain)\n        if domain != domains[-1]:  \n            _polite_delay()\n    \n    return results\n\n\ndef get_discovery_status() -> Dict:\n    \"\"\"Get current email discovery configuration status.\"\"\"\n    return {\n        \"enabled\": DISCOVERY_ENABLED,\n        \"dry_run\": DISCOVERY_DRY_RUN,\n        \"timeout_seconds\": DISCOVERY_TIMEOUT,\n        \"max_pages_per_domain\": DISCOVERY_MAX_PAGES,\n        \"delay_range\": f\"{DISCOVERY_DELAY_MIN}-{DISCOVERY_DELAY_MAX}s\",\n        \"cache_size\": len(_domain_cache),\n        \"cache_ttl_hours\": CACHE_TTL_HOURS\n    }\n\n\nif __name__ == \"__main__\":\n    test_domains = [\"hossagent.net\", \"google.com\"]\n    \n    print(\"Email Discovery Test\")\n    print(\"=\" * 50)\n    print(f\"Config: {get_discovery_status()}\")\n    print(\"=\" * 50)\n    \n    for domain in test_domains:\n        print(f\"\\nTesting: {domain}\")\n        result = discover_emails(domain)\n        print(f\"Result: {json.dumps(result.to_dict(), indent=2)}\")\n","path":null,"size_bytes":16192,"size_tokens":null},"domain_discovery.py":{"content":"\"\"\"\nDomain Discovery Module - Aggressive Domain Hunting for LeadEvents\n\nImplements a layered, ruthless strategy to find company domains from signals.\nNO external enrichment APIs (no Apollo, Hunter, Clearbit). Pure web intelligence.\n\nDiscovery Pipeline (in order):\n1. Extract from existing fields (lead_domain, lead_email)\n2. Extract from signal source_url (if company site vs news/directory)\n3. Parse article pages for outbound links to company websites\n4. Web search fallback (company name + geography + niche)\n\nGuardrails:\n- Reject social/directory/news domains\n- Validate TLDs\n- Match company name tokens to domain\n\nAuthor: HossAgent\n\"\"\"\n\nimport os\nimport re\nimport json\nimport time\nimport random\nfrom datetime import datetime\nfrom typing import Optional, List, Dict, Set, Tuple\nfrom urllib.parse import urljoin, urlparse\nfrom dataclasses import dataclass, field\n\nimport requests\nfrom requests.exceptions import RequestException, Timeout\n\nDISCOVERY_TIMEOUT = int(os.getenv(\"DOMAIN_DISCOVERY_TIMEOUT\", \"10\"))\nDISCOVERY_DELAY_MIN = float(os.getenv(\"DOMAIN_DISCOVERY_DELAY_MIN\", \"0.5\"))\nDISCOVERY_DELAY_MAX = float(os.getenv(\"DOMAIN_DISCOVERY_DELAY_MAX\", \"1.5\"))\n\nUSER_AGENTS = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0\",\n]\n\nBLOCKED_DOMAINS = {\n    \"facebook.com\", \"fb.com\", \"instagram.com\", \"twitter.com\", \"x.com\",\n    \"linkedin.com\", \"youtube.com\", \"tiktok.com\", \"pinterest.com\",\n    \"yelp.com\", \"angi.com\", \"angieslist.com\", \"homeadvisor.com\",\n    \"thumbtack.com\", \"houzz.com\", \"tripadvisor.com\", \"bbb.org\",\n    \"google.com\", \"maps.google.com\", \"news.google.com\", \"bing.com\",\n    \"yahoo.com\", \"msn.com\", \"aol.com\",\n    \"reddit.com\", \"quora.com\", \"medium.com\", \"substack.com\",\n    \"prnewswire.com\", \"businesswire.com\", \"globenewswire.com\",\n    \"reuters.com\", \"bloomberg.com\", \"wsj.com\", \"nytimes.com\",\n    \"cnn.com\", \"foxnews.com\", \"nbcnews.com\", \"cbsnews.com\", \"abcnews.com\",\n    \"local10.com\", \"wsvn.com\", \"nbcmiami.com\", \"cbsmiami.com\",\n    \"miamiherald.com\", \"sun-sentinel.com\", \"palmbeachpost.com\",\n    \"southfloridabusinessjournal.com\", \"bizjournals.com\",\n    \"wikipedia.org\", \"wikimedia.org\",\n    \"amazon.com\", \"ebay.com\", \"etsy.com\", \"shopify.com\",\n    \"craigslist.org\", \"nextdoor.com\",\n    \"glassdoor.com\", \"indeed.com\", \"ziprecruiter.com\",\n    \"patch.com\", \"axios.com\", \"huffpost.com\",\n    \"wix.com\", \"squarespace.com\", \"godaddy.com\", \"wordpress.com\",\n    \"mailchimp.com\", \"constantcontact.com\",\n}\n\nNEWS_DOMAIN_PATTERNS = [\n    r\".*news.*\\.com$\", r\".*herald.*\\.com$\", r\".*times.*\\.com$\",\n    r\".*post.*\\.com$\", r\".*tribune.*\\.com$\", r\".*journal.*\\.com$\",\n    r\".*gazette.*\\.com$\", r\".*observer.*\\.com$\", r\".*daily.*\\.com$\",\n    r\".*weekly.*\\.com$\", r\".*local\\d+\\.com$\", r\".*tv\\.com$\",\n]\n\nVALID_TLDS = {\n    \".com\", \".net\", \".org\", \".biz\", \".co\", \".io\", \".us\", \".info\",\n    \".pro\", \".me\", \".tv\", \".cc\", \".co.uk\", \".ca\", \".mx\", \".br\",\n}\n\n\n@dataclass\nclass DomainDiscoveryResult:\n    \"\"\"Result of domain discovery attempt - ARCHANGEL Discovery Engine.\"\"\"\n    success: bool\n    domain: Optional[str] = None\n    source: str = \"none\"\n    confidence: float = 0.0\n    company_name_match: bool = False\n    discovery_method: str = \"\"\n    attempts: int = 0\n    error: Optional[str] = None\n    company_name_candidate: Optional[str] = None  # ARCHANGEL: extracted company name\n    \n    def to_dict(self) -> Dict:\n        return {\n            \"success\": self.success,\n            \"domain\": self.domain,\n            \"source\": self.source,\n            \"confidence\": self.confidence,\n            \"company_name_match\": self.company_name_match,\n            \"discovery_method\": self.discovery_method,\n            \"attempts\": self.attempts,\n            \"error\": self.error,\n        }\n\n\ndef log_discovery(\n    action: str,\n    lead_event_id: Optional[int] = None,\n    domain: Optional[str] = None,\n    details: Optional[Dict] = None,\n    error: Optional[str] = None\n) -> None:\n    \"\"\"Log domain discovery activity - ARCHANGEL logging.\"\"\"\n    msg_parts = [f\"[ARCHANGEL][DOMAIN_DISCOVERY][{action.upper()}]\"]\n    if lead_event_id:\n        msg_parts.append(f\"event={lead_event_id}\")\n    if domain:\n        msg_parts.append(f\"domain={domain}\")\n    if details:\n        for k, v in details.items():\n            if isinstance(v, str) and len(v) > 50:\n                v = v[:50] + \"...\"\n            msg_parts.append(f\"{k}={v}\")\n    if error:\n        msg_parts.append(f\"error={error}\")\n    print(\" \".join(msg_parts))\n\n\ndef _get_random_user_agent() -> str:\n    \"\"\"Get a random user agent string.\"\"\"\n    return random.choice(USER_AGENTS)\n\n\ndef _normalize_domain(url_or_domain: str) -> Optional[str]:\n    \"\"\"\n    Extract and normalize domain from URL or domain string.\n    \n    Returns lowercase domain without www prefix, or None if invalid.\n    \"\"\"\n    if not url_or_domain:\n        return None\n    \n    url_or_domain = url_or_domain.strip().lower()\n    \n    if url_or_domain.startswith(\"http\"):\n        try:\n            parsed = urlparse(url_or_domain)\n            domain = parsed.netloc\n        except Exception:\n            return None\n    else:\n        domain = url_or_domain.split(\"/\")[0]\n    \n    domain = domain.replace(\"www.\", \"\")\n    \n    domain = re.sub(r\":\\d+$\", \"\", domain)\n    \n    if not domain or \".\" not in domain:\n        return None\n    \n    return domain\n\n\ndef _is_blocked_domain(domain: str) -> bool:\n    \"\"\"Check if domain is in the blocked list or matches news patterns.\"\"\"\n    if not domain:\n        return True\n    \n    domain_lower = domain.lower()\n    \n    if domain_lower in BLOCKED_DOMAINS:\n        return True\n    \n    for blocked in BLOCKED_DOMAINS:\n        if domain_lower.endswith(\".\" + blocked) or blocked.endswith(\".\" + domain_lower):\n            return True\n    \n    for pattern in NEWS_DOMAIN_PATTERNS:\n        if re.match(pattern, domain_lower, re.IGNORECASE):\n            return True\n    \n    return False\n\n\ndef _has_valid_tld(domain: str) -> bool:\n    \"\"\"Check if domain has a valid TLD.\"\"\"\n    if not domain:\n        return False\n    \n    for tld in VALID_TLDS:\n        if domain.endswith(tld):\n            return True\n    \n    parts = domain.split(\".\")\n    if len(parts) >= 2:\n        last_part = \".\" + parts[-1]\n        if len(last_part) <= 4:\n            return True\n    \n    return False\n\n\ndef extract_company_name_from_summary(summary: Optional[str]) -> Optional[str]:\n    \"\"\"\n    ARCHANGEL: Extract probable company name from signal summary.\n    \n    STRICT heuristics - only extract if we have high confidence:\n    - \"News: Miami Best Roofing Announces...\" -> \"Miami Best Roofing\"\n    - \"News: Cool Running Air Expands...\" -> \"Cool Running Air\"\n    - \"News: Sunny Bliss Plumbing & Air Acquires...\" -> \"Sunny Bliss Plumbing & Air\"\n    \n    REJECT generic industry descriptions:\n    - \"Texas HVAC company buys new...\" -> None (generic, not branded)\n    - \"Owner of Orlando roofing company...\" -> None (no specific name)\n    - \"Trump Supporter Breaks Down...\" -> None (not a business)\n    \"\"\"\n    if not summary:\n        return None\n    \n    summary = summary.strip()\n    \n    quoted = re.search(r'\"([^\"]+)\"', summary)\n    if quoted:\n        candidate = quoted.group(1).strip()\n        if _is_valid_branded_company(candidate):\n            return candidate\n    \n    text = summary\n    if text.startswith(\"News: \"):\n        text = text[6:]\n    \n    action_verbs = r'(?:Announces?|Expands?|Acquires?|Opens?|Launches?|Reports?|Hires?|Receives?|Wins?|Partners?|Unveils?|Introduces?|Signs?|Adds?|Completes?|Celebrates?|Strengthens?)'\n    \n    business_nouns = r'(?:Air|Roofing|Plumbing|HVAC|Electric|Electrical|Landscaping|Construction|Realty|Properties|Solutions|Services|Partners|Group|Corp|Inc|LLC|Company|Co|Associates|Consulting|Agency|Studios?|Labs?|Tech|Technologies|Systems|Holdings|Capital|Ventures|Enterprises|Industries|Manufacturing)'\n    \n    branded_pattern = rf'^([A-Z][a-zA-Z]+(?:\\s+[A-Z]?[a-zA-Z&\\'-]+)*\\s+{business_nouns})\\s+{action_verbs}'\n    match = re.match(branded_pattern, text)\n    if match:\n        candidate = match.group(1).strip()\n        if _is_valid_branded_company(candidate):\n            return candidate\n    \n    two_word_pattern = rf'^([A-Z][a-zA-Z]+\\s+[A-Z][a-zA-Z]+)\\s+{action_verbs}'\n    match = re.match(two_word_pattern, text)\n    if match:\n        candidate = match.group(1).strip()\n        if _is_valid_branded_company(candidate):\n            return candidate\n    \n    return None\n\n\ndef _is_valid_branded_company(candidate: str) -> bool:\n    \"\"\"\n    Validate that a candidate string looks like a real branded company name.\n    \n    ACCEPT: \"Miami Best Roofing\", \"Cool Running Air\", \"Sunny Bliss Plumbing & Air\"\n    REJECT: \"Texas HVAC company buys new\", \"Owner of Orlando\", \"Florida Vets\", \"South\"\n    \"\"\"\n    if not candidate or len(candidate) < 4 or len(candidate) > 60:\n        return False\n    \n    words = candidate.split()\n    if len(words) < 2 or len(words) > 6:\n        return False\n    \n    poison_patterns = [\n        r'^(?:texas|florida|miami|orlando|south|north|east|west|local|global|new|major|the|this|a|an)',\n        r'^(?:owner|trump|billionaire|breaking|latest|update|how|why|what|when|after|before)',\n        r'(?:company|business|firm|shop|store)\\s+(?:buys?|sells?|opens?|files?|seeks?)',\n        r'(?:vets?|veteran|supporter|worker|employee|customer)',\n        r'^[a-z]',\n    ]\n    \n    for pattern in poison_patterns:\n        if re.search(pattern, candidate, re.IGNORECASE):\n            return False\n    \n    business_indicators = [\n        'roofing', 'air', 'plumbing', 'hvac', 'electric', 'landscaping', 'construction',\n        'realty', 'properties', 'solutions', 'services', 'partners', 'group', 'corp',\n        'inc', 'llc', 'company', 'associates', 'consulting', 'agency', 'studios',\n        'labs', 'tech', 'technologies', 'systems', 'holdings', 'capital', 'ventures',\n        'enterprises', 'industries', 'manufacturing', 'distribution', 'logistics'\n    ]\n    \n    has_business_indicator = any(ind in candidate.lower() for ind in business_indicators)\n    \n    first_word = words[0]\n    has_branded_start = first_word[0].isupper() and len(first_word) >= 3\n    \n    if has_business_indicator and has_branded_start:\n        return True\n    \n    if len(words) >= 2 and has_branded_start:\n        second_word = words[1]\n        if second_word[0].isupper() and len(second_word) >= 3:\n            if second_word.lower() not in ['the', 'and', 'of', 'for', 'in', 'on', 'at', 'to']:\n                return True\n    \n    return False\n\n\ndef _tokenize_company_name(company_name: str) -> Set[str]:\n    \"\"\"\n    Extract meaningful tokens from company name.\n    \n    Removes common suffixes (Inc, LLC, etc) and returns lowercase tokens.\n    \"\"\"\n    if not company_name:\n        return set()\n    \n    name = company_name.lower().strip()\n    \n    suffixes = [\n        r'\\s+(inc|llc|corp|co|ltd|llp|pllc|pc|pa|plc|lp|incorporated|corporation|company)\\.?$',\n        r'\\s+&\\s+', r'\\s+and\\s+',\n    ]\n    for suffix in suffixes:\n        name = re.sub(suffix, ' ', name, flags=re.IGNORECASE)\n    \n    name = re.sub(r'[^a-z0-9\\s]', ' ', name)\n    \n    tokens = set(name.split())\n    \n    stop_words = {'the', 'a', 'an', 'of', 'in', 'at', 'on', 'for', 'by', 'to', 'and', 'or'}\n    tokens = tokens - stop_words\n    \n    tokens = {t for t in tokens if len(t) >= 2}\n    \n    return tokens\n\n\ndef _domain_matches_company(domain: str, company_name: str) -> Tuple[bool, float]:\n    \"\"\"\n    Check if domain matches company name.\n    \n    Returns (is_match, confidence_score).\n    \"\"\"\n    if not domain or not company_name:\n        return False, 0.0\n    \n    domain_lower = domain.lower().replace(\"www.\", \"\")\n    domain_name = domain_lower.split(\".\")[0]\n    \n    company_tokens = _tokenize_company_name(company_name)\n    \n    if not company_tokens:\n        return False, 0.0\n    \n    domain_slug = re.sub(r'[^a-z0-9]', '', domain_name)\n    company_slug = \"\".join(sorted(company_tokens))\n    \n    matches = 0\n    for token in company_tokens:\n        if token in domain_name:\n            matches += 1\n    \n    if matches == 0:\n        return False, 0.0\n    \n    confidence = matches / len(company_tokens)\n    \n    combined_tokens = \"\".join(sorted(company_tokens))\n    if domain_slug == combined_tokens or combined_tokens in domain_slug:\n        confidence = 1.0\n    \n    return confidence >= 0.5, confidence\n\n\ndef _fetch_page(url: str, timeout: Optional[int] = None) -> Optional[str]:\n    \"\"\"\n    Fetch a page with retry and error handling.\n    \n    Returns HTML content or None if failed.\n    \"\"\"\n    if timeout is None:\n        timeout = DISCOVERY_TIMEOUT\n    \n    try:\n        headers = {\n            \"User-Agent\": _get_random_user_agent(),\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n            \"Accept-Language\": \"en-US,en;q=0.5\",\n        }\n        \n        response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)\n        \n        if response.status_code == 200:\n            return response.text\n        \n        return None\n        \n    except (RequestException, Timeout) as e:\n        log_discovery(\"fetch_error\", details={\"url\": url[:50]}, error=str(e)[:100])\n        return None\n\n\ndef _extract_outbound_links(html: str, base_url: str) -> List[str]:\n    \"\"\"\n    Extract outbound links from HTML that might be company websites.\n    \n    Filters out social/news/directory links.\n    \"\"\"\n    if not html:\n        return []\n    \n    link_pattern = re.compile(r'<a[^>]+href=[\"\\']([^\"\\']+)[\"\\'][^>]*>([^<]*)</a>', re.IGNORECASE)\n    \n    matches = link_pattern.findall(html)\n    \n    base_domain = _normalize_domain(base_url)\n    \n    candidate_domains = []\n    \n    for href, anchor_text in matches:\n        if not href.startswith(\"http\"):\n            continue\n        \n        domain = _normalize_domain(href)\n        \n        if not domain:\n            continue\n        \n        if domain == base_domain:\n            continue\n        \n        if _is_blocked_domain(domain):\n            continue\n        \n        if not _has_valid_tld(domain):\n            continue\n        \n        candidate_domains.append(domain)\n    \n    return list(set(candidate_domains))\n\n\ndef _guess_domain_from_company_name(company_name: str) -> Optional[str]:\n    \"\"\"\n    Attempt to guess domain from company name.\n    \n    Example: \"Cool Running Air\" -> \"coolrunningair.com\"\n    \"\"\"\n    if not company_name:\n        return None\n    \n    tokens = _tokenize_company_name(company_name)\n    \n    if not tokens:\n        return None\n    \n    slug = \"\".join(sorted(tokens, key=lambda x: company_name.lower().find(x)))\n    \n    if len(slug) < 3:\n        return None\n    \n    return f\"{slug}.com\"\n\n\ndef discover_domain_from_existing_fields(\n    lead_domain: Optional[str],\n    lead_email: Optional[str],\n    lead_company: Optional[str]\n) -> DomainDiscoveryResult:\n    \"\"\"\n    Layer 1: Extract domain from existing LeadEvent fields.\n    \n    Priority:\n    1. lead_domain (if valid and not blocked)\n    2. lead_email (extract domain part)\n    \"\"\"\n    \n    if lead_domain:\n        domain = _normalize_domain(lead_domain)\n        if domain and not _is_blocked_domain(domain) and _has_valid_tld(domain):\n            match, confidence = _domain_matches_company(domain, lead_company) if lead_company else (True, 0.8)\n            return DomainDiscoveryResult(\n                success=True,\n                domain=domain,\n                source=\"existing_field\",\n                confidence=confidence if confidence > 0 else 0.8,\n                company_name_match=match,\n                discovery_method=\"lead_domain\",\n                attempts=1\n            )\n    \n    if lead_email and \"@\" in lead_email:\n        email_domain = lead_email.split(\"@\")[1].lower()\n        if not _is_blocked_domain(email_domain) and _has_valid_tld(email_domain):\n            match, confidence = _domain_matches_company(email_domain, lead_company) if lead_company else (True, 0.9)\n            return DomainDiscoveryResult(\n                success=True,\n                domain=email_domain,\n                source=\"existing_field\",\n                confidence=confidence if confidence > 0 else 0.9,\n                company_name_match=match,\n                discovery_method=\"lead_email\",\n                attempts=1\n            )\n    \n    return DomainDiscoveryResult(\n        success=False,\n        source=\"existing_field\",\n        discovery_method=\"none\",\n        attempts=1,\n        error=\"No usable domain in existing fields\"\n    )\n\n\ndef discover_domain_from_source_url(\n    source_url: Optional[str],\n    company_name: Optional[str]\n) -> DomainDiscoveryResult:\n    \"\"\"\n    Layer 2: Extract domain from signal source URL.\n    \n    If source_url is a company website (not news/directory), use it directly.\n    If source_url is a news article, fetch and parse for outbound links.\n    \"\"\"\n    if not source_url:\n        return DomainDiscoveryResult(\n            success=False,\n            source=\"source_url\",\n            discovery_method=\"none\",\n            attempts=0,\n            error=\"No source URL provided\"\n        )\n    \n    source_domain = _normalize_domain(source_url)\n    \n    if not source_domain:\n        return DomainDiscoveryResult(\n            success=False,\n            source=\"source_url\",\n            discovery_method=\"none\",\n            attempts=1,\n            error=\"Could not parse source URL\"\n        )\n    \n    if not _is_blocked_domain(source_domain):\n        match, confidence = _domain_matches_company(source_domain, company_name) if company_name else (False, 0.5)\n        \n        if match or confidence >= 0.5:\n            return DomainDiscoveryResult(\n                success=True,\n                domain=source_domain,\n                source=\"source_url\",\n                confidence=confidence,\n                company_name_match=match,\n                discovery_method=\"source_url_direct\",\n                attempts=1\n            )\n    \n    log_discovery(\"fetch_article\", details={\"url\": source_url[:50], \"reason\": \"news_site\"})\n    \n    time.sleep(random.uniform(DISCOVERY_DELAY_MIN, DISCOVERY_DELAY_MAX))\n    \n    html = _fetch_page(source_url)\n    \n    if not html:\n        return DomainDiscoveryResult(\n            success=False,\n            source=\"source_url\",\n            discovery_method=\"article_parse\",\n            attempts=1,\n            error=\"Could not fetch article page\"\n        )\n    \n    candidate_domains = _extract_outbound_links(html, source_url)\n    \n    if not candidate_domains:\n        return DomainDiscoveryResult(\n            success=False,\n            source=\"source_url\",\n            discovery_method=\"article_parse\",\n            attempts=1,\n            error=\"No candidate domains found in article\"\n        )\n    \n    if company_name:\n        best_match = None\n        best_confidence = 0.0\n        \n        for domain in candidate_domains:\n            match, confidence = _domain_matches_company(domain, company_name)\n            if confidence > best_confidence:\n                best_confidence = confidence\n                best_match = domain\n        \n        if best_match and best_confidence >= 0.5:\n            return DomainDiscoveryResult(\n                success=True,\n                domain=best_match,\n                source=\"source_url\",\n                confidence=best_confidence,\n                company_name_match=True,\n                discovery_method=\"article_link_match\",\n                attempts=1\n            )\n    \n    if len(candidate_domains) == 1:\n        return DomainDiscoveryResult(\n            success=True,\n            domain=candidate_domains[0],\n            source=\"source_url\",\n            confidence=0.6,\n            company_name_match=False,\n            discovery_method=\"article_single_link\",\n            attempts=1\n        )\n    \n    return DomainDiscoveryResult(\n        success=False,\n        source=\"source_url\",\n        discovery_method=\"article_parse\",\n        attempts=1,\n        error=f\"Multiple candidates ({len(candidate_domains)}), no clear match\"\n    )\n\n\n_ddg_consecutive_failures = 0\n_ddg_last_failure_time = 0.0\n\ndef _search_duckduckgo_html(query: str) -> List[str]:\n    \"\"\"\n    DOMAINSTORM: Search DuckDuckGo HTML for domains.\n    \n    Fetches DuckDuckGo search results as HTML and extracts result domains.\n    No API key required - pure HTML scraping.\n    Includes backoff logic to avoid throttling.\n    \"\"\"\n    global _ddg_consecutive_failures, _ddg_last_failure_time\n    \n    if _ddg_consecutive_failures >= 3:\n        time_since_failure = time.time() - _ddg_last_failure_time\n        if time_since_failure < 300:\n            log_discovery(\"ddg_backoff\", details={\"failures\": _ddg_consecutive_failures, \"wait_remaining\": int(300 - time_since_failure)})\n            return []\n        else:\n            _ddg_consecutive_failures = 0\n    \n    try:\n        url = \"https://html.duckduckgo.com/html/\"\n        headers = {\n            \"User-Agent\": _get_random_user_agent(),\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n            \"Accept-Language\": \"en-US,en;q=0.5\",\n        }\n        data = {\"q\": query, \"b\": \"\"}\n        \n        delay = random.uniform(DISCOVERY_DELAY_MIN, DISCOVERY_DELAY_MAX) * (1 + _ddg_consecutive_failures * 0.5)\n        time.sleep(delay)\n        \n        start_time = time.time()\n        response = requests.post(url, headers=headers, data=data, timeout=DISCOVERY_TIMEOUT)\n        fetch_time = time.time() - start_time\n        \n        if response.status_code != 200:\n            _ddg_consecutive_failures += 1\n            _ddg_last_failure_time = time.time()\n            log_discovery(\"ddg_http_error\", details={\"status\": response.status_code, \"query\": query[:40]})\n            return []\n        \n        if \"blocked\" in response.text.lower() or \"captcha\" in response.text.lower():\n            _ddg_consecutive_failures += 1\n            _ddg_last_failure_time = time.time()\n            log_discovery(\"ddg_blocked\", details={\"query\": query[:40]})\n            return []\n        \n        html = response.text\n        \n        domains = []\n        result_pattern = re.compile(r'href=\"//duckduckgo\\.com/l/\\?uddg=([^\"&]+)\"', re.IGNORECASE)\n        \n        for match in result_pattern.finditer(html):\n            try:\n                from urllib.parse import unquote\n                result_url = unquote(match.group(1))\n                domain = _normalize_domain(result_url)\n                \n                if domain and not _is_blocked_domain(domain) and _has_valid_tld(domain):\n                    if domain not in domains:\n                        domains.append(domain)\n            except Exception:\n                continue\n        \n        if not domains:\n            direct_pattern = re.compile(r'class=\"result__url\"[^>]*>([^<]+)<', re.IGNORECASE)\n            for match in direct_pattern.finditer(html):\n                domain_text = match.group(1).strip()\n                domain = _normalize_domain(domain_text)\n                if domain and not _is_blocked_domain(domain) and _has_valid_tld(domain):\n                    if domain not in domains:\n                        domains.append(domain)\n        \n        _ddg_consecutive_failures = 0\n        \n        if domains:\n            log_discovery(\"ddg_success\", details={\"domains\": len(domains), \"fetch_time\": f\"{fetch_time:.2f}s\"})\n        \n        return domains[:5]\n    \n    except requests.Timeout:\n        _ddg_consecutive_failures += 1\n        _ddg_last_failure_time = time.time()\n        log_discovery(\"ddg_timeout\", details={\"query\": query[:40]})\n        return []\n    except requests.RequestException as e:\n        _ddg_consecutive_failures += 1\n        _ddg_last_failure_time = time.time()\n        log_discovery(\"ddg_request_error\", error=str(e)[:50])\n        return []\n    except Exception as e:\n        _ddg_consecutive_failures += 1\n        _ddg_last_failure_time = time.time()\n        log_discovery(\"ddg_unexpected_error\", error=str(e)[:50])\n        return []\n\n\ndef discover_domain_via_web_search(\n    company_name: str,\n    geography: Optional[str] = None,\n    niche: Optional[str] = None\n) -> DomainDiscoveryResult:\n    \"\"\"\n    Layer 3: Web search fallback using company name + geography + niche.\n    \n    DOMAINSTORM Enhanced:\n    1. Try guessing domain from company name\n    2. If guess fails, search DuckDuckGo HTML\n    3. Match results against company name tokens\n    \"\"\"\n    if not company_name:\n        return DomainDiscoveryResult(\n            success=False,\n            source=\"web_search\",\n            discovery_method=\"none\",\n            attempts=0,\n            error=\"No company name provided\"\n        )\n    \n    query_parts = [f'\"{company_name}\"']\n    if geography:\n        first_geo = geography.split(\",\")[0].strip()\n        query_parts.append(first_geo)\n    if niche:\n        first_niche = niche.split(\",\")[0].strip()\n        query_parts.append(first_niche)\n    \n    query = \" \".join(query_parts)\n    \n    log_discovery(\"web_search\", details={\"query\": query})\n    \n    guessed = _guess_domain_from_company_name(company_name)\n    if guessed:\n        try:\n            time.sleep(random.uniform(DISCOVERY_DELAY_MIN, DISCOVERY_DELAY_MAX))\n            \n            test_url = f\"https://{guessed}\"\n            headers = {\"User-Agent\": _get_random_user_agent()}\n            \n            response = requests.head(test_url, headers=headers, timeout=5, allow_redirects=True)\n            \n            if response.status_code < 400:\n                html = _fetch_page(test_url)\n                \n                if html and company_name.lower() in html.lower():\n                    return DomainDiscoveryResult(\n                        success=True,\n                        domain=guessed,\n                        source=\"web_search\",\n                        confidence=0.85,\n                        company_name_match=True,\n                        discovery_method=\"guessed_domain_verified\",\n                        attempts=1\n                    )\n                \n                return DomainDiscoveryResult(\n                    success=True,\n                    domain=guessed,\n                    source=\"web_search\",\n                    confidence=0.6,\n                    company_name_match=False,\n                    discovery_method=\"guessed_domain_exists\",\n                    attempts=1\n                )\n        except Exception as e:\n            log_discovery(\"guess_failed\", details={\"guessed\": guessed}, error=str(e)[:50])\n    \n    log_discovery(\"ddg_search\", details={\"query\": query})\n    search_domains = _search_duckduckgo_html(query)\n    \n    if search_domains:\n        best_domain = None\n        best_confidence = 0.0\n        \n        for domain in search_domains:\n            match, confidence = _domain_matches_company(domain, company_name)\n            if confidence > best_confidence:\n                best_domain = domain\n                best_confidence = confidence\n        \n        if best_domain and best_confidence >= 0.5:\n            log_discovery(\"ddg_match\", details={\"domain\": best_domain, \"confidence\": best_confidence})\n            return DomainDiscoveryResult(\n                success=True,\n                domain=best_domain,\n                source=\"web_search\",\n                confidence=best_confidence,\n                company_name_match=True,\n                discovery_method=\"duckduckgo_search\",\n                attempts=2\n            )\n        \n        if search_domains:\n            first_domain = search_domains[0]\n            log_discovery(\"ddg_first_result\", details={\"domain\": first_domain})\n            return DomainDiscoveryResult(\n                success=True,\n                domain=first_domain,\n                source=\"web_search\",\n                confidence=0.5,\n                company_name_match=False,\n                discovery_method=\"duckduckgo_first_result\",\n                attempts=2\n            )\n    \n    return DomainDiscoveryResult(\n        success=False,\n        source=\"web_search\",\n        discovery_method=\"exhausted\",\n        attempts=2,\n        error=\"Domain guess and search both failed\"\n    )\n\n\ndef discover_domain_for_lead_event(\n    lead_event_id: int,\n    lead_domain: Optional[str] = None,\n    lead_email: Optional[str] = None,\n    lead_company: Optional[str] = None,\n    source_url: Optional[str] = None,\n    geography: Optional[str] = None,\n    niche: Optional[str] = None,\n    summary: Optional[str] = None\n) -> DomainDiscoveryResult:\n    \"\"\"\n    Main entry point: Layered domain discovery for a LeadEvent.\n    \n    Executes discovery layers in order:\n    1. Extract from existing fields (lead_domain, lead_email)\n    2. Extract from source_url (company site or article parsing)\n    3. Web search fallback (company name + geography + niche)\n    \n    Returns the first successful result with confidence > threshold.\n    \"\"\"\n    log_discovery(\"start\", lead_event_id=lead_event_id, \n                  details={\"company\": lead_company, \"has_domain\": bool(lead_domain), \n                           \"has_email\": bool(lead_email), \"has_source_url\": bool(source_url)})\n    \n    total_attempts = 0\n    \n    result1 = discover_domain_from_existing_fields(lead_domain, lead_email, lead_company)\n    total_attempts += result1.attempts\n    \n    if result1.success:\n        log_discovery(\"success\", lead_event_id=lead_event_id, domain=result1.domain,\n                      details={\"method\": result1.discovery_method, \"confidence\": result1.confidence})\n        result1.attempts = total_attempts\n        return result1\n    \n    effective_source_url = source_url\n    if not effective_source_url and summary:\n        url_match = re.search(r'https?://[^\\s<>\"]+', summary)\n        if url_match:\n            effective_source_url = url_match.group(0)\n    \n    if effective_source_url:\n        result2 = discover_domain_from_source_url(effective_source_url, lead_company)\n        total_attempts += result2.attempts\n        \n        if result2.success:\n            log_discovery(\"success\", lead_event_id=lead_event_id, domain=result2.domain,\n                          details={\"method\": result2.discovery_method, \"confidence\": result2.confidence})\n            result2.attempts = total_attempts\n            return result2\n    \n    if lead_company:\n        result3 = discover_domain_via_web_search(lead_company, geography, niche)\n        total_attempts += result3.attempts\n        \n        if result3.success:\n            log_discovery(\"success\", lead_event_id=lead_event_id, domain=result3.domain,\n                          details={\"method\": result3.discovery_method, \"confidence\": result3.confidence})\n            result3.attempts = total_attempts\n            return result3\n    \n    log_discovery(\"failed\", lead_event_id=lead_event_id,\n                  details={\"attempts\": total_attempts, \"company\": lead_company})\n    \n    return DomainDiscoveryResult(\n        success=False,\n        source=\"all_layers\",\n        discovery_method=\"exhausted\",\n        attempts=total_attempts,\n        error=\"All discovery layers failed\"\n    )\n\n\nprint(\"[DOMAIN_DISCOVERY][STARTUP] Module loaded - aggressive domain hunting enabled\")\n","path":null,"size_bytes":31277,"size_tokens":null},"analytics.py":{"content":"\"\"\"\nHossAgent Analytics Module\n\nServer-side analytics for tracking:\n- Page views and traffic sources\n- Signup funnel (landing  signup  trial  upgrade)\n- Abandonment events (signup started but not completed, checkout started but not completed)\n- Customer engagement (portal visits, approvals, settings changes)\n\nAll data stored in PostgreSQL for admin dashboard visibility.\n\"\"\"\n\nimport os\nimport json\nfrom datetime import datetime, timedelta\nfrom typing import Optional, Dict, Any, List\nfrom dataclasses import dataclass, asdict\nfrom enum import Enum\nfrom pathlib import Path\n\n\nclass EventType(str, Enum):\n    PAGE_VIEW = \"page_view\"\n    SIGNUP_STARTED = \"signup_started\"\n    SIGNUP_COMPLETED = \"signup_completed\"\n    SIGNUP_ABANDONED = \"signup_abandoned\"\n    LOGIN = \"login\"\n    PORTAL_VIEW = \"portal_view\"\n    SETTINGS_VIEW = \"settings_view\"\n    SETTINGS_UPDATED = \"settings_updated\"\n    CHECKOUT_STARTED = \"checkout_started\"\n    CHECKOUT_COMPLETED = \"checkout_completed\"\n    CHECKOUT_ABANDONED = \"checkout_abandoned\"\n    UPGRADE_COMPLETED = \"upgrade_completed\"\n    CANCELLATION = \"cancellation\"\n    OUTREACH_APPROVED = \"outreach_approved\"\n    OUTREACH_DISCARDED = \"outreach_discarded\"\n    LEAD_VIEWED = \"lead_viewed\"\n    REPORT_VIEWED = \"report_viewed\"\n\n\n@dataclass\nclass AnalyticsEvent:\n    \"\"\"Single analytics event.\"\"\"\n    event_type: str\n    timestamp: str\n    path: Optional[str] = None\n    referrer: Optional[str] = None\n    user_agent: Optional[str] = None\n    ip_hash: Optional[str] = None\n    customer_id: Optional[int] = None\n    session_id: Optional[str] = None\n    metadata: Optional[Dict[str, Any]] = None\n\n\nANALYTICS_LOG_FILE = Path(\"analytics_events.json\")\nMAX_EVENTS = 5000\n\n\ndef _load_events() -> List[Dict[str, Any]]:\n    \"\"\"Load analytics events from file.\"\"\"\n    try:\n        if ANALYTICS_LOG_FILE.exists():\n            with open(ANALYTICS_LOG_FILE, \"r\") as f:\n                return json.load(f)\n    except Exception:\n        pass\n    return []\n\n\ndef _save_events(events: List[Dict[str, Any]]) -> None:\n    \"\"\"Save analytics events to file (capped).\"\"\"\n    try:\n        events = events[-MAX_EVENTS:]\n        with open(ANALYTICS_LOG_FILE, \"w\") as f:\n            json.dump(events, f, indent=2)\n    except Exception as e:\n        print(f\"[ANALYTICS] Warning: Could not save events: {e}\")\n\n\ndef _hash_ip(ip: str) -> str:\n    \"\"\"Hash IP for privacy (we don't store raw IPs).\"\"\"\n    import hashlib\n    return hashlib.sha256(ip.encode()).hexdigest()[:16]\n\n\ndef track_event(\n    event_type: EventType,\n    path: Optional[str] = None,\n    referrer: Optional[str] = None,\n    user_agent: Optional[str] = None,\n    ip_address: Optional[str] = None,\n    customer_id: Optional[int] = None,\n    session_id: Optional[str] = None,\n    metadata: Optional[Dict[str, Any]] = None\n) -> None:\n    \"\"\"\n    Track an analytics event.\n    \n    Args:\n        event_type: Type of event (page_view, signup_started, etc.)\n        path: URL path\n        referrer: HTTP referrer\n        user_agent: Browser user agent\n        ip_address: Client IP (will be hashed)\n        customer_id: Associated customer ID if logged in\n        session_id: Session identifier\n        metadata: Additional event-specific data\n    \"\"\"\n    event = AnalyticsEvent(\n        event_type=event_type.value if isinstance(event_type, EventType) else event_type,\n        timestamp=datetime.utcnow().isoformat(),\n        path=path,\n        referrer=referrer,\n        user_agent=user_agent[:200] if user_agent else None,\n        ip_hash=_hash_ip(ip_address) if ip_address else None,\n        customer_id=customer_id,\n        session_id=session_id,\n        metadata=metadata\n    )\n    \n    events = _load_events()\n    events.append(asdict(event))\n    _save_events(events)\n    \n    print(f\"[ANALYTICS] {event_type}: {path or 'N/A'} (customer: {customer_id or 'anon'})\")\n\n\ndef track_page_view(\n    path: str,\n    referrer: Optional[str] = None,\n    user_agent: Optional[str] = None,\n    ip_address: Optional[str] = None,\n    customer_id: Optional[int] = None,\n    session_id: Optional[str] = None\n) -> None:\n    \"\"\"Track a page view event.\"\"\"\n    track_event(\n        EventType.PAGE_VIEW,\n        path=path,\n        referrer=referrer,\n        user_agent=user_agent,\n        ip_address=ip_address,\n        customer_id=customer_id,\n        session_id=session_id\n    )\n\n\ndef track_funnel_event(\n    event_type: EventType,\n    customer_id: Optional[int] = None,\n    metadata: Optional[Dict[str, Any]] = None,\n    ip_address: Optional[str] = None\n) -> None:\n    \"\"\"Track a funnel event (signup, checkout, upgrade, etc.).\"\"\"\n    track_event(\n        event_type,\n        customer_id=customer_id,\n        metadata=metadata,\n        ip_address=ip_address\n    )\n\n\ndef get_events(\n    limit: int = 100,\n    event_type: Optional[str] = None,\n    since: Optional[datetime] = None\n) -> List[Dict[str, Any]]:\n    \"\"\"Get analytics events with optional filtering.\"\"\"\n    events = _load_events()\n    \n    if event_type:\n        events = [e for e in events if e.get(\"event_type\") == event_type]\n    \n    if since:\n        since_str = since.isoformat()\n        events = [e for e in events if e.get(\"timestamp\", \"\") >= since_str]\n    \n    return events[-limit:]\n\n\ndef get_page_view_stats(days: int = 7) -> Dict[str, Any]:\n    \"\"\"Get page view statistics for the last N days.\"\"\"\n    since = datetime.utcnow() - timedelta(days=days)\n    events = get_events(limit=10000, since=since)\n    \n    page_views = [e for e in events if e.get(\"event_type\") == EventType.PAGE_VIEW.value]\n    \n    by_page: Dict[str, int] = {}\n    by_day: Dict[str, int] = {}\n    referrers: Dict[str, int] = {}\n    \n    for pv in page_views:\n        path = pv.get(\"path\", \"unknown\")\n        by_page[path] = by_page.get(path, 0) + 1\n        \n        day = pv.get(\"timestamp\", \"\")[:10]\n        by_day[day] = by_day.get(day, 0) + 1\n        \n        ref = pv.get(\"referrer\")\n        if ref:\n            ref_domain = ref.split(\"/\")[2] if \"//\" in ref else ref\n            referrers[ref_domain] = referrers.get(ref_domain, 0) + 1\n    \n    return {\n        \"total_views\": len(page_views),\n        \"unique_visitors\": len(set(e.get(\"ip_hash\") for e in page_views if e.get(\"ip_hash\"))),\n        \"by_page\": dict(sorted(by_page.items(), key=lambda x: x[1], reverse=True)[:10]),\n        \"by_day\": dict(sorted(by_day.items())),\n        \"top_referrers\": dict(sorted(referrers.items(), key=lambda x: x[1], reverse=True)[:10]),\n        \"period_days\": days\n    }\n\n\ndef get_funnel_stats(days: int = 30) -> Dict[str, Any]:\n    \"\"\"Get conversion funnel statistics.\"\"\"\n    since = datetime.utcnow() - timedelta(days=days)\n    events = get_events(limit=10000, since=since)\n    \n    landing_views = len([e for e in events if e.get(\"event_type\") == EventType.PAGE_VIEW.value and e.get(\"path\") == \"/\"])\n    signup_started = len([e for e in events if e.get(\"event_type\") == EventType.SIGNUP_STARTED.value])\n    signup_completed = len([e for e in events if e.get(\"event_type\") == EventType.SIGNUP_COMPLETED.value])\n    signup_abandoned = len([e for e in events if e.get(\"event_type\") == EventType.SIGNUP_ABANDONED.value])\n    \n    checkout_started = len([e for e in events if e.get(\"event_type\") == EventType.CHECKOUT_STARTED.value])\n    checkout_completed = len([e for e in events if e.get(\"event_type\") == EventType.CHECKOUT_COMPLETED.value])\n    checkout_abandoned = len([e for e in events if e.get(\"event_type\") == EventType.CHECKOUT_ABANDONED.value])\n    \n    upgrades = len([e for e in events if e.get(\"event_type\") == EventType.UPGRADE_COMPLETED.value])\n    cancellations = len([e for e in events if e.get(\"event_type\") == EventType.CANCELLATION.value])\n    \n    logins = len([e for e in events if e.get(\"event_type\") == EventType.LOGIN.value])\n    portal_views = len([e for e in events if e.get(\"event_type\") == EventType.PORTAL_VIEW.value])\n    \n    signup_rate = (signup_completed / signup_started * 100) if signup_started > 0 else 0\n    checkout_rate = (checkout_completed / checkout_started * 100) if checkout_started > 0 else 0\n    upgrade_rate = (upgrades / signup_completed * 100) if signup_completed > 0 else 0\n    \n    return {\n        \"period_days\": days,\n        \"funnel\": {\n            \"landing_views\": landing_views,\n            \"signup_started\": signup_started,\n            \"signup_completed\": signup_completed,\n            \"signup_abandoned\": signup_abandoned,\n            \"signup_conversion_rate\": round(signup_rate, 1),\n            \"checkout_started\": checkout_started,\n            \"checkout_completed\": checkout_completed,\n            \"checkout_abandoned\": checkout_abandoned,\n            \"checkout_conversion_rate\": round(checkout_rate, 1),\n            \"upgrades\": upgrades,\n            \"upgrade_rate\": round(upgrade_rate, 1),\n            \"cancellations\": cancellations\n        },\n        \"engagement\": {\n            \"logins\": logins,\n            \"portal_views\": portal_views,\n            \"unique_active_customers\": len(set(\n                e.get(\"customer_id\") for e in events \n                if e.get(\"customer_id\") and e.get(\"event_type\") in [\n                    EventType.LOGIN.value, EventType.PORTAL_VIEW.value\n                ]\n            ))\n        }\n    }\n\n\ndef get_abandonment_details(days: int = 7) -> Dict[str, Any]:\n    \"\"\"Get abandonment event details for analysis.\"\"\"\n    since = datetime.utcnow() - timedelta(days=days)\n    events = get_events(limit=10000, since=since)\n    \n    signup_abandons = [\n        e for e in events \n        if e.get(\"event_type\") == EventType.SIGNUP_ABANDONED.value\n    ]\n    \n    checkout_abandons = [\n        e for e in events \n        if e.get(\"event_type\") == EventType.CHECKOUT_ABANDONED.value\n    ]\n    \n    return {\n        \"period_days\": days,\n        \"signup_abandons\": {\n            \"count\": len(signup_abandons),\n            \"recent\": signup_abandons[-10:]\n        },\n        \"checkout_abandons\": {\n            \"count\": len(checkout_abandons),\n            \"recent\": checkout_abandons[-10:]\n        }\n    }\n\n\ndef get_analytics_summary() -> Dict[str, Any]:\n    \"\"\"Get overall analytics summary for admin dashboard.\"\"\"\n    return {\n        \"page_views_7d\": get_page_view_stats(7),\n        \"funnel_30d\": get_funnel_stats(30),\n        \"abandonment_7d\": get_abandonment_details(7),\n        \"last_updated\": datetime.utcnow().isoformat()\n    }\n","path":null,"size_bytes":10307,"size_tokens":null},"phone_extraction.py":{"content":"\"\"\"\nOPERATION PHONESTORM: Phone Number Extraction and Enrichment Pipeline\n\nExtracts, validates, normalizes, and scores phone numbers from web pages\nto accelerate domain discovery and provide additional contact channels.\n\nPhone numbers serve as:\n- Identity anchors\n- Domain-discovery accelerators\n- Contact-channel fallbacks\n- Enrichment multipliers\n\nPure web scraping - NO paid APIs required.\n\"\"\"\n\nimport os\nimport re\nimport time\nimport random\nimport hashlib\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\nfrom typing import Optional, List, Dict, Tuple, Set\nfrom urllib.parse import urljoin, urlparse\nimport requests\nfrom requests.exceptions import RequestException, Timeout\n\n\nPHONE_TIMEOUT = int(os.getenv(\"PHONE_EXTRACTION_TIMEOUT\", \"10\"))\nPHONE_MAX_PAGES = int(os.getenv(\"PHONE_EXTRACTION_MAX_PAGES\", \"5\"))\nPHONE_DELAY_MIN = float(os.getenv(\"PHONE_EXTRACTION_DELAY_MIN\", \"1.0\"))\nPHONE_DELAY_MAX = float(os.getenv(\"PHONE_EXTRACTION_DELAY_MAX\", \"3.0\"))\n\n\nPHONE_REGEX_PATTERNS = [\n    re.compile(r'\\+?1[-.\\s]?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}'),\n    re.compile(r'\\(\\d{3}\\)\\s*\\d{3}[-.\\s]?\\d{4}'),\n    re.compile(r'\\d{3}[-.\\s]\\d{3}[-.\\s]\\d{4}'),\n    re.compile(r'\\d{10}'),\n]\n\nTEL_LINK_REGEX = re.compile(r'href=[\"\\']tel:([^\"\\']+)[\"\\']', re.IGNORECASE)\n\nSCHEMA_PHONE_REGEX = re.compile(r'\"telephone\"\\s*:\\s*\"([^\"]+)\"', re.IGNORECASE)\n\nTOLL_FREE_PREFIXES = ['800', '888', '877', '866', '855', '844', '833']\n\nMOBILE_PREFIXES = [\n    '201', '202', '203', '205', '206', '207', '208', '209', '210', '212',\n    '213', '214', '215', '216', '217', '218', '219', '224', '225', '228',\n    '229', '231', '234', '239', '240', '248', '251', '252', '253', '254',\n    '256', '260', '262', '267', '269', '270', '272', '276', '281', '301',\n    '302', '303', '304', '305', '307', '308', '309', '310', '312', '313',\n    '314', '315', '316', '317', '318', '319', '320', '321', '323', '325',\n    '330', '331', '334', '336', '337', '339', '346', '347', '351', '352',\n    '360', '361', '364', '380', '385', '386', '401', '402', '404', '405',\n    '406', '407', '408', '409', '410', '412', '413', '414', '415', '417',\n    '419', '423', '424', '425', '430', '432', '434', '435', '440', '442',\n    '443', '458', '469', '470', '475', '478', '479', '480', '484', '501',\n    '502', '503', '504', '505', '507', '508', '509', '510', '512', '513',\n    '515', '516', '517', '518', '520', '530', '531', '534', '539', '540',\n    '541', '551', '559', '561', '562', '563', '564', '567', '570', '571',\n    '573', '574', '575', '580', '585', '586', '601', '602', '603', '605',\n    '606', '607', '608', '609', '610', '612', '614', '615', '616', '617',\n    '618', '619', '620', '623', '626', '628', '629', '630', '631', '636',\n    '641', '646', '650', '651', '657', '660', '661', '662', '667', '669',\n    '678', '681', '682', '689', '701', '702', '703', '704', '706', '707',\n    '708', '712', '713', '714', '715', '716', '717', '718', '719', '720',\n    '724', '725', '727', '731', '732', '734', '737', '740', '743', '747',\n    '754', '757', '760', '762', '763', '765', '769', '770', '772', '773',\n    '774', '775', '779', '781', '785', '786', '801', '802', '803', '804',\n    '805', '806', '808', '810', '812', '813', '814', '815', '816', '817',\n    '818', '828', '830', '831', '832', '843', '845', '847', '848', '850',\n    '856', '857', '858', '859', '860', '862', '863', '864', '865', '870',\n    '878', '901', '903', '904', '906', '907', '908', '909', '910', '912',\n    '913', '914', '915', '916', '917', '918', '919', '920', '925', '928',\n    '929', '930', '931', '934', '936', '937', '938', '940', '941', '947',\n    '949', '951', '952', '954', '956', '959', '970', '971', '972', '973',\n    '978', '979', '980', '984', '985', '989'\n]\n\nSOUTH_FLORIDA_AREA_CODES = ['305', '786', '954', '754', '561', '772']\n\nUSER_AGENTS = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0\",\n]\n\n_seen_phones: Dict[str, Set[str]] = {}\n_phone_cache: Dict[str, Tuple['PhoneDiscoveryResult', datetime]] = {}\nCACHE_TTL_HOURS = 24\n\n\n@dataclass\nclass DiscoveredPhone:\n    \"\"\"Represents a discovered phone number with metadata.\"\"\"\n    raw_number: str\n    e164_number: str\n    confidence: float\n    source: str\n    phone_type: str\n    source_url: str\n    discovered_at: datetime = field(default_factory=datetime.utcnow)\n    \n    def to_dict(self) -> Dict:\n        return {\n            \"raw_number\": self.raw_number,\n            \"e164_number\": self.e164_number,\n            \"confidence\": self.confidence,\n            \"source\": self.source,\n            \"phone_type\": self.phone_type,\n            \"source_url\": self.source_url,\n            \"discovered_at\": self.discovered_at.isoformat()\n        }\n\n\n@dataclass\nclass PhoneDiscoveryResult:\n    \"\"\"Result of phone discovery attempt.\"\"\"\n    success: bool\n    domain: str\n    phones: List[DiscoveredPhone] = field(default_factory=list)\n    best_phone: Optional[DiscoveredPhone] = None\n    pages_checked: int = 0\n    error: Optional[str] = None\n    duration_ms: int = 0\n    \n    def to_dict(self) -> Dict:\n        return {\n            \"success\": self.success,\n            \"domain\": self.domain,\n            \"phones\": [p.to_dict() for p in self.phones],\n            \"best_phone\": self.best_phone.to_dict() if self.best_phone else None,\n            \"pages_checked\": self.pages_checked,\n            \"error\": self.error,\n            \"duration_ms\": self.duration_ms\n        }\n\n\ndef _normalize_to_e164(phone: str) -> Optional[str]:\n    \"\"\"\n    Normalize phone number to E.164 format: +1XXXYYYZZZZ\n    Returns None if normalization fails.\n    \"\"\"\n    digits = re.sub(r'\\D', '', phone)\n    \n    if len(digits) == 10:\n        return f\"+1{digits}\"\n    elif len(digits) == 11 and digits.startswith('1'):\n        return f\"+{digits}\"\n    elif len(digits) == 12 and digits.startswith('01'):\n        return f\"+{digits[1:]}\"\n    else:\n        return None\n\n\ndef _extract_digits(phone: str) -> str:\n    \"\"\"Extract only digits from phone string.\"\"\"\n    return re.sub(r'\\D', '', phone)\n\n\ndef _get_area_code(e164: str) -> Optional[str]:\n    \"\"\"Extract area code from E.164 number.\"\"\"\n    if e164 and len(e164) >= 5:\n        return e164[2:5]\n    return None\n\n\ndef _classify_phone_type(e164: str) -> str:\n    \"\"\"\n    Classify phone type based on area code patterns.\n    Returns: mobile, landline, voip, tollfree, unknown\n    \"\"\"\n    if not e164:\n        return \"unknown\"\n    \n    area_code = _get_area_code(e164)\n    if not area_code:\n        return \"unknown\"\n    \n    if area_code in TOLL_FREE_PREFIXES:\n        return \"tollfree\"\n    \n    if area_code in MOBILE_PREFIXES:\n        return \"mobile\"\n    \n    return \"landline\"\n\n\ndef _is_valid_us_phone(e164: str) -> bool:\n    \"\"\"Check if E.164 number is a valid US phone with spam filtering.\"\"\"\n    if not e164 or not e164.startswith('+1'):\n        return False\n    \n    digits = _extract_digits(e164)\n    if len(digits) != 11:\n        return False\n    \n    area_code = digits[1:4]\n    if area_code[0] in ['0', '1']:\n        return False\n    \n    exchange = digits[4:7]\n    if exchange[0] in ['0', '1']:\n        return False\n    \n    subscriber = digits[1:]  # 10 digits after +1\n    \n    if len(set(subscriber)) <= 2:\n        return False\n    \n    repeated_patterns = ['1234567890', '0123456789', '9876543210']\n    if subscriber in repeated_patterns:\n        return False\n    \n    if subscriber == subscriber[0] * 10:\n        return False\n    \n    easy_patterns = ['5555555555', '0000000000', '1111111111']\n    if subscriber in easy_patterns:\n        return False\n    \n    if area_code in TOLL_FREE_PREFIXES:\n        return False\n    \n    return True\n\n\ndef _calculate_phone_confidence(\n    phone: DiscoveredPhone,\n    page_type: str,\n    is_local: bool = False\n) -> float:\n    \"\"\"\n    Calculate confidence score for discovered phone.\n    \n    Scoring factors:\n    - Schema.org telephone: +0.9\n    - Tel: link: +0.8\n    - Contact page: +0.8\n    - Homepage: +0.7\n    - Footer: +0.6\n    - Random page: +0.3\n    \n    Penalties:\n    - Toll-free: -0.5\n    - Seen on multiple domains: -0.3\n    \n    Bonuses:\n    - South Florida area code: +0.1\n    - Mobile type: +0.1\n    \"\"\"\n    score = 0.0\n    \n    if phone.source == \"schema\":\n        score = 0.9\n    elif phone.source == \"tel_link\":\n        score = 0.8\n    elif page_type == \"contact\":\n        score = 0.8\n    elif page_type == \"homepage\":\n        score = 0.7\n    elif page_type == \"footer\":\n        score = 0.6\n    else:\n        score = 0.3\n    \n    if phone.phone_type == \"tollfree\":\n        score -= 0.5\n    \n    area_code = _get_area_code(phone.e164_number)\n    if area_code and phone.e164_number in _seen_phones:\n        if len(_seen_phones[phone.e164_number]) > 4:\n            score -= 0.3\n    \n    if area_code in SOUTH_FLORIDA_AREA_CODES:\n        score += 0.1\n    \n    if phone.phone_type == \"mobile\":\n        score += 0.1\n    \n    return max(0.0, min(1.0, score))\n\n\ndef _get_random_user_agent() -> str:\n    \"\"\"Get random user agent.\"\"\"\n    return random.choice(USER_AGENTS)\n\n\ndef _polite_delay() -> None:\n    \"\"\"Apply polite delay between requests.\"\"\"\n    delay = random.uniform(PHONE_DELAY_MIN, PHONE_DELAY_MAX)\n    time.sleep(delay)\n\n\ndef _fetch_page(url: str) -> Optional[str]:\n    \"\"\"Fetch page content with error handling.\"\"\"\n    try:\n        headers = {\n            \"User-Agent\": _get_random_user_agent(),\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n            \"Accept-Language\": \"en-US,en;q=0.5\",\n            \"Accept-Encoding\": \"gzip, deflate\",\n            \"Connection\": \"keep-alive\",\n        }\n        \n        response = requests.get(\n            url,\n            headers=headers,\n            timeout=PHONE_TIMEOUT,\n            allow_redirects=True,\n            verify=True\n        )\n        \n        if response.status_code == 200:\n            return response.text\n        else:\n            print(f\"[PHONESTORM] HTTP {response.status_code} for {url}\")\n            return None\n            \n    except Timeout:\n        print(f\"[PHONESTORM] Timeout fetching {url}\")\n        return None\n    except RequestException as e:\n        print(f\"[PHONESTORM] Request error for {url}: {str(e)[:100]}\")\n        return None\n\n\ndef _extract_phones_from_html(\n    html: str,\n    source_url: str,\n    page_type: str = \"other\"\n) -> List[DiscoveredPhone]:\n    \"\"\"\n    Extract phone numbers from HTML using multiple methods.\n    \n    Methods:\n    1. tel: link extraction\n    2. Schema.org telephone property\n    3. Regex patterns in body text\n    4. Footer region extraction\n    \"\"\"\n    phones = []\n    seen_e164 = set()\n    \n    tel_matches = TEL_LINK_REGEX.findall(html)\n    for match in tel_matches:\n        e164 = _normalize_to_e164(match)\n        if e164 and _is_valid_us_phone(e164) and e164 not in seen_e164:\n            seen_e164.add(e164)\n            phone_type = _classify_phone_type(e164)\n            phone = DiscoveredPhone(\n                raw_number=match,\n                e164_number=e164,\n                confidence=0.0,\n                source=\"tel_link\",\n                phone_type=phone_type,\n                source_url=source_url\n            )\n            phone.confidence = _calculate_phone_confidence(phone, page_type)\n            phones.append(phone)\n            print(f\"[PHONESTORM][FOUND_PHONE] number={e164}, conf={phone.confidence:.2f}, source=tel_link\")\n    \n    schema_matches = SCHEMA_PHONE_REGEX.findall(html)\n    for match in schema_matches:\n        e164 = _normalize_to_e164(match)\n        if e164 and _is_valid_us_phone(e164) and e164 not in seen_e164:\n            seen_e164.add(e164)\n            phone_type = _classify_phone_type(e164)\n            phone = DiscoveredPhone(\n                raw_number=match,\n                e164_number=e164,\n                confidence=0.0,\n                source=\"schema\",\n                phone_type=phone_type,\n                source_url=source_url\n            )\n            phone.confidence = _calculate_phone_confidence(phone, page_type)\n            phones.append(phone)\n            print(f\"[PHONESTORM][FOUND_PHONE] number={e164}, conf={phone.confidence:.2f}, source=schema\")\n    \n    footer_match = re.search(r'<footer[^>]*>(.*?)</footer>', html, re.IGNORECASE | re.DOTALL)\n    if footer_match:\n        footer_html = footer_match.group(1)\n        for pattern in PHONE_REGEX_PATTERNS:\n            matches = pattern.findall(footer_html)\n            for match in matches:\n                e164 = _normalize_to_e164(match)\n                if e164 and _is_valid_us_phone(e164) and e164 not in seen_e164:\n                    seen_e164.add(e164)\n                    phone_type = _classify_phone_type(e164)\n                    phone = DiscoveredPhone(\n                        raw_number=match,\n                        e164_number=e164,\n                        confidence=0.0,\n                        source=\"footer\",\n                        phone_type=phone_type,\n                        source_url=source_url\n                    )\n                    phone.confidence = _calculate_phone_confidence(phone, \"footer\")\n                    phones.append(phone)\n                    print(f\"[PHONESTORM][FOUND_PHONE] number={e164}, conf={phone.confidence:.2f}, source=footer\")\n    \n    for pattern in PHONE_REGEX_PATTERNS:\n        matches = pattern.findall(html)\n        for match in matches:\n            e164 = _normalize_to_e164(match)\n            if e164 and _is_valid_us_phone(e164) and e164 not in seen_e164:\n                seen_e164.add(e164)\n                phone_type = _classify_phone_type(e164)\n                phone = DiscoveredPhone(\n                    raw_number=match,\n                    e164_number=e164,\n                    confidence=0.0,\n                    source=page_type,\n                    phone_type=phone_type,\n                    source_url=source_url\n                )\n                phone.confidence = _calculate_phone_confidence(phone, page_type)\n                phones.append(phone)\n                print(f\"[PHONESTORM][FOUND_PHONE] number={e164}, conf={phone.confidence:.2f}, source={page_type}\")\n    \n    return phones\n\n\ndef _determine_page_type(url: str) -> str:\n    \"\"\"Determine page type from URL path.\"\"\"\n    path = urlparse(url).path.lower()\n    \n    if path in ['/', '']:\n        return \"homepage\"\n    elif 'contact' in path:\n        return \"contact\"\n    elif 'about' in path:\n        return \"about\"\n    elif 'team' in path or 'staff' in path:\n        return \"team\"\n    else:\n        return \"other\"\n\n\ndef _get_cached_result(domain: str) -> Optional[PhoneDiscoveryResult]:\n    \"\"\"Check cache for recent discovery result.\"\"\"\n    if domain in _phone_cache:\n        result, cached_at = _phone_cache[domain]\n        if datetime.utcnow() - cached_at < timedelta(hours=CACHE_TTL_HOURS):\n            return result\n        else:\n            del _phone_cache[domain]\n    return None\n\n\ndef _cache_result(domain: str, result: PhoneDiscoveryResult) -> None:\n    \"\"\"Cache discovery result.\"\"\"\n    _phone_cache[domain] = (result, datetime.utcnow())\n\n\ndef _track_phone_domain(e164: str, domain: str) -> None:\n    \"\"\"Track which domains a phone number has been seen on.\"\"\"\n    if e164 not in _seen_phones:\n        _seen_phones[e164] = set()\n    _seen_phones[e164].add(domain)\n\n\ndef discover_phones(domain: str) -> PhoneDiscoveryResult:\n    \"\"\"\n    Discover phone numbers from a company website.\n    \n    Args:\n        domain: Company domain (e.g., \"example.com\")\n        \n    Returns:\n        PhoneDiscoveryResult with found phones and metadata\n    \"\"\"\n    start_time = time.time()\n    domain = domain.lower().strip().lstrip(\"www.\")\n    \n    cached = _get_cached_result(domain)\n    if cached:\n        print(f\"[PHONESTORM] Cache hit for {domain}\")\n        return cached\n    \n    print(f\"[PHONESTORM] Starting phone discovery for {domain}\")\n    \n    all_phones: List[DiscoveredPhone] = []\n    pages_checked = 0\n    \n    base_url = f\"https://{domain}\"\n    homepage_html = _fetch_page(base_url)\n    \n    if not homepage_html:\n        base_url = f\"https://www.{domain}\"\n        homepage_html = _fetch_page(base_url)\n    \n    if homepage_html:\n        pages_checked += 1\n        phones = _extract_phones_from_html(homepage_html, base_url, \"homepage\")\n        all_phones.extend(phones)\n        \n        contact_paths = ['/contact', '/contact-us', '/about', '/about-us']\n        for path in contact_paths:\n            if pages_checked >= PHONE_MAX_PAGES:\n                break\n            \n            _polite_delay()\n            url = urljoin(base_url, path)\n            page_html = _fetch_page(url)\n            \n            if page_html:\n                pages_checked += 1\n                page_type = _determine_page_type(url)\n                phones = _extract_phones_from_html(page_html, url, page_type)\n                \n                for phone in phones:\n                    if not any(p.e164_number == phone.e164_number for p in all_phones):\n                        all_phones.append(phone)\n    \n    for phone in all_phones:\n        _track_phone_domain(phone.e164_number, domain)\n    \n    all_phones.sort(key=lambda p: -p.confidence)\n    \n    best_phone = None\n    if all_phones:\n        non_tollfree = [p for p in all_phones if p.phone_type != \"tollfree\"]\n        if non_tollfree:\n            best_phone = non_tollfree[0]\n        else:\n            best_phone = all_phones[0]\n    \n    duration_ms = int((time.time() - start_time) * 1000)\n    \n    result = PhoneDiscoveryResult(\n        success=len(all_phones) > 0,\n        domain=domain,\n        phones=all_phones,\n        best_phone=best_phone,\n        pages_checked=pages_checked,\n        duration_ms=duration_ms\n    )\n    \n    _cache_result(domain, result)\n    \n    if result.success:\n        print(f\"[PHONESTORM] Found {len(all_phones)} phone(s) for {domain}, best: {best_phone.e164_number if best_phone else 'none'}\")\n        print(f\"[PHONESTORM][PHONE_TYPE] {best_phone.phone_type if best_phone else 'unknown'}\")\n        print(f\"[PHONESTORM][PHONE_CONFIDENCE] score={best_phone.confidence if best_phone else 0.0:.2f}\")\n    else:\n        print(f\"[PHONESTORM] No phones found for {domain} (checked {pages_checked} pages)\")\n    \n    return result\n\n\ndef get_domain_from_phone(phone: str, business_name: Optional[str] = None, city: Optional[str] = None) -> Optional[str]:\n    \"\"\"\n    Attempt to discover domain from phone number using reverse lookup.\n    \n    This is a fallback when ARCHANGEL cannot parse website URLs.\n    Uses internal cache first, then web search patterns.\n    \n    Args:\n        phone: Phone number (any format)\n        business_name: Optional business name for search refinement\n        city: Optional city for geographic filtering\n        \n    Returns:\n        Domain string if found, None otherwise\n    \"\"\"\n    e164 = _normalize_to_e164(phone)\n    if not e164:\n        return None\n    \n    if e164 in _seen_phones and _seen_phones[e164]:\n        domains = list(_seen_phones[e164])\n        if len(domains) == 1:\n            print(f\"[PHONESTORM][REVERSE_DOMAIN] Found cached domain for {e164}: {domains[0]}\")\n            return domains[0]\n        else:\n            print(f\"[PHONESTORM][REVERSE_DOMAIN] Multiple domains for {e164}, cannot determine unique match\")\n    \n    print(f\"[PHONESTORM][REVERSE_DOMAIN] No cached domain for {e164}\")\n    return None\n\n\ndef generate_sms_suggestion(lead_name: str, signal_context: str) -> str:\n    \"\"\"\n    Generate suggested SMS message for manual follow-up.\n    \n    Args:\n        lead_name: First name of lead\n        signal_context: Brief context from signal (e.g., \"your Miami expansion\")\n        \n    Returns:\n        Suggested SMS text\n    \"\"\"\n    first_name = lead_name.split()[0] if lead_name else \"there\"\n    \n    return f\"Hey {first_name}, Sam Holliday here in Miami  saw {signal_context}. Wanted to share a quick local insight; is this the right number?\"\n\n\ndef generate_call_script(lead_name: str, lead_company: str, signal_context: str) -> str:\n    \"\"\"\n    Generate suggested call script for manual follow-up.\n    \n    Args:\n        lead_name: Full name of lead\n        lead_company: Company name\n        signal_context: Context from signal\n        \n    Returns:\n        Call script text\n    \"\"\"\n    first_name = lead_name.split()[0] if lead_name else \"there\"\n    \n    return f\"\"\"Hi {first_name}, this is Sam Holliday calling from Miami.\n\nI noticed {lead_company} {signal_context} and wanted to reach out because I work with a lot of businesses in South Florida facing similar situations.\n\nI've put together some insights specifically for companies making this kind of move that I thought might be valuable.\n\nDo you have a couple minutes to chat, or would it be better if I sent you something to review first?\"\"\"\n\n\ndef get_phone_discovery_status() -> Dict:\n    \"\"\"Get current phone discovery configuration status.\"\"\"\n    return {\n        \"timeout_seconds\": PHONE_TIMEOUT,\n        \"max_pages_per_domain\": PHONE_MAX_PAGES,\n        \"delay_range\": f\"{PHONE_DELAY_MIN}-{PHONE_DELAY_MAX}s\",\n        \"cache_size\": len(_phone_cache),\n        \"tracked_phones\": len(_seen_phones),\n        \"cache_ttl_hours\": CACHE_TTL_HOURS\n    }\n\n\nif __name__ == \"__main__\":\n    import json\n    \n    test_domains = [\"hossagent.net\"]\n    \n    print(\"PHONESTORM Discovery Test\")\n    print(\"=\" * 50)\n    print(f\"Config: {get_phone_discovery_status()}\")\n    print(\"=\" * 50)\n    \n    for domain in test_domains:\n        print(f\"\\nTesting: {domain}\")\n        result = discover_phones(domain)\n        print(f\"Result: {json.dumps(result.to_dict(), indent=2)}\")\n","path":null,"size_bytes":21736,"size_tokens":null},"craigslist_connector.py":{"content":"\"\"\"\nOPERATION SIGNALSTORM v1: Craigslist Connector\n\nScrapes Craigslist for SMB-heavy signals:\n- Services offered (hvac, plumbing, roofing, landscaping)\n- For sale > business (business sales)\n- Jobs > trades (hiring signals)\n- Housing > office/commercial (expansion signals)\n\nFocuses on South Florida markets: Miami, Fort Lauderdale, Palm Beach.\n\nPure web scraping - NO paid APIs.\n\"\"\"\n\nimport os\nimport re\nimport json\nimport time\nimport hashlib\nfrom dataclasses import dataclass, field\nfrom typing import List, Optional, Dict, Tuple, Generator\nfrom datetime import datetime, timedelta\nfrom urllib.parse import urljoin, urlencode, quote\n\nimport requests\nfrom requests.exceptions import RequestException, Timeout\n\n\nCRAIGSLIST_TIMEOUT = int(os.getenv(\"CRAIGSLIST_TIMEOUT\", \"10\"))\nCRAIGSLIST_RATE_LIMIT = float(os.getenv(\"CRAIGSLIST_RATE_LIMIT\", \"2.0\"))\nCRAIGSLIST_DRY_RUN = os.getenv(\"CRAIGSLIST_DRY_RUN\", \"false\").lower() in (\"true\", \"1\", \"yes\")\n\nSOUTH_FLORIDA_REGIONS = {\n    \"miami\": \"https://miami.craigslist.org\",\n    \"fortlauderdale\": \"https://fortlauderdale.craigslist.org\",\n    \"palmbeach\": \"https://palmbeach.craigslist.org\",\n}\n\nSERVICE_CATEGORIES = {\n    \"hvac\": \"/search/sss?query=hvac\",\n    \"plumbing\": \"/search/sss?query=plumber\",\n    \"roofing\": \"/search/sss?query=roofing\",\n    \"electrical\": \"/search/sss?query=electrician\",\n    \"landscaping\": \"/search/sss?query=landscaping\",\n    \"pool\": \"/search/sss?query=pool+service\",\n    \"cleaning\": \"/search/sss?query=cleaning+service\",\n    \"painting\": \"/search/sss?query=painting+contractor\",\n}\n\nSIGNAL_CATEGORIES = {\n    \"business_for_sale\": \"/search/bfs\",\n    \"services_offered\": \"/search/bbb\",\n    \"gigs_labor\": \"/search/lbg\",\n    \"trades_jobs\": \"/search/trd\",\n}\n\nUSER_AGENTS = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0\",\n]\n\nNICHE_KEYWORDS = {\n    \"hvac\": [\"hvac\", \"air conditioning\", \"ac repair\", \"ac service\", \"heating\", \"cooling\", \"duct\"],\n    \"plumbing\": [\"plumber\", \"plumbing\", \"drain\", \"pipe\", \"water heater\", \"leak\"],\n    \"roofing\": [\"roofing\", \"roof repair\", \"shingle\", \"roofer\", \"roof\"],\n    \"electrical\": [\"electrician\", \"electrical\", \"wiring\", \"panel\", \"outlet\"],\n    \"landscaping\": [\"landscaping\", \"lawn\", \"tree\", \"garden\", \"irrigation\"],\n    \"pool\": [\"pool\", \"spa\", \"hot tub\", \"pool service\", \"pool cleaning\"],\n    \"cleaning\": [\"cleaning\", \"maid\", \"janitorial\", \"housekeeping\"],\n    \"painting\": [\"painting\", \"painter\", \"drywall\", \"interior painting\", \"exterior painting\"],\n    \"moving\": [\"moving\", \"movers\", \"hauling\", \"junk removal\"],\n    \"pest\": [\"pest control\", \"exterminator\", \"termite\", \"bug\"],\n    \"auto\": [\"auto repair\", \"mechanic\", \"body shop\", \"collision\"],\n    \"construction\": [\"construction\", \"contractor\", \"remodeling\", \"renovation\", \"builder\"],\n}\n\n\n@dataclass\nclass CraigslistListing:\n    \"\"\"A Craigslist listing.\"\"\"\n    listing_id: str\n    title: str\n    url: str\n    price: Optional[str] = None\n    location: Optional[str] = None\n    neighborhood: Optional[str] = None\n    posted_date: Optional[str] = None\n    body_text: Optional[str] = None\n    contact_phone: Optional[str] = None\n    contact_email: Optional[str] = None\n    images: List[str] = field(default_factory=list)\n    niche: Optional[str] = None\n    region: str = \"miami\"\n    category: str = \"services\"\n    \n    def to_dict(self) -> Dict:\n        return {\n            \"listing_id\": self.listing_id,\n            \"title\": self.title,\n            \"url\": self.url,\n            \"price\": self.price,\n            \"location\": self.location,\n            \"neighborhood\": self.neighborhood,\n            \"posted_date\": self.posted_date,\n            \"body_text\": self.body_text[:500] if self.body_text else None,\n            \"contact_phone\": self.contact_phone,\n            \"contact_email\": self.contact_email,\n            \"niche\": self.niche,\n            \"region\": self.region,\n            \"category\": self.category\n        }\n    \n    def generate_signal_id(self) -> str:\n        \"\"\"Generate a unique signal ID for deduplication.\"\"\"\n        unique_string = f\"craigslist-{self.region}-{self.listing_id}\"\n        return hashlib.sha256(unique_string.encode()).hexdigest()[:16]\n\n\n@dataclass\nclass CraigslistScanResult:\n    \"\"\"Result of a Craigslist scan.\"\"\"\n    success: bool\n    listings: List[CraigslistListing] = field(default_factory=list)\n    region: str = \"miami\"\n    category: str = \"services\"\n    niche: Optional[str] = None\n    scan_time_ms: int = 0\n    error: Optional[str] = None\n    pages_scanned: int = 0\n\n\ndef log_craigslist(action: str, region: Optional[str] = None, details: Optional[Dict] = None) -> None:\n    \"\"\"Log Craigslist connector activity.\"\"\"\n    prefix = \"[DRY_RUN]\" if CRAIGSLIST_DRY_RUN else \"\"\n    msg_parts = [f\"{prefix}[CRAIGSLIST][{action.upper()}]\"]\n    if region:\n        msg_parts.append(f\"region={region}\")\n    if details:\n        for k, v in details.items():\n            if isinstance(v, str) and len(v) > 50:\n                v = v[:50] + \"...\"\n            msg_parts.append(f\"{k}={v}\")\n    print(\" | \".join(msg_parts))\n\n\ndef _detect_niche(title: str, body: Optional[str] = None) -> Optional[str]:\n    \"\"\"Detect the business niche from title and body text.\"\"\"\n    text = (title + \" \" + (body or \"\")).lower()\n    \n    for niche, keywords in NICHE_KEYWORDS.items():\n        for keyword in keywords:\n            if keyword in text:\n                return niche\n    \n    return None\n\n\ndef _extract_phone(text: str) -> Optional[str]:\n    \"\"\"Extract phone number from text.\"\"\"\n    phone_pattern = re.compile(\n        r\"(?:\\+?1[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\",\n        re.IGNORECASE\n    )\n    match = phone_pattern.search(text)\n    if match:\n        return match.group(0)\n    return None\n\n\ndef _extract_email(text: str) -> Optional[str]:\n    \"\"\"Extract email from text (usually anonymized on Craigslist).\"\"\"\n    email_pattern = re.compile(\n        r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\",\n        re.IGNORECASE\n    )\n    match = email_pattern.search(text)\n    if match:\n        email = match.group(0)\n        if \"craigslist.org\" not in email.lower():\n            return email\n    return None\n\n\ndef _extract_company_name(title: str, body: Optional[str] = None) -> Optional[str]:\n    \"\"\"Try to extract company name from listing.\"\"\"\n    text = title + \" \" + (body or \"\")\n    \n    patterns = [\n        r'([A-Z][a-zA-Z]+(?:\\s+[A-Z]?[a-zA-Z&\\'\\-]+){0,3}\\s+(?:HVAC|Roofing|Plumbing|Electric|Landscaping|Pool|Cleaning|Painting|Moving|Construction|Services|Solutions|Inc|LLC|Corp|Co))',\n        r'\"([^\"]{5,50})\"',\n        r'\\*\\*([^*]{5,50})\\*\\*',\n    ]\n    \n    for pattern in patterns:\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            name = match.group(1).strip()\n            if len(name) >= 5 and len(name) <= 60:\n                return name\n    \n    return None\n\n\ndef _fetch_page(url: str, retries: int = 2) -> Optional[str]:\n    \"\"\"Fetch a Craigslist page with retry logic.\"\"\"\n    if CRAIGSLIST_DRY_RUN:\n        log_craigslist(\"DRY_RUN_FETCH\", details={\"url\": url[:60]})\n        return None\n    \n    for attempt in range(retries):\n        try:\n            headers = {\n                \"User-Agent\": USER_AGENTS[attempt % len(USER_AGENTS)],\n                \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n                \"Accept-Language\": \"en-US,en;q=0.5\",\n            }\n            \n            response = requests.get(url, headers=headers, timeout=CRAIGSLIST_TIMEOUT)\n            \n            if response.status_code == 200:\n                return response.text\n            elif response.status_code == 429:\n                log_craigslist(\"RATE_LIMITED\", details={\"attempt\": attempt + 1})\n                time.sleep(CRAIGSLIST_RATE_LIMIT * (attempt + 1) * 2)\n                continue\n            else:\n                log_craigslist(\"HTTP_ERROR\", details={\"status\": response.status_code})\n                return None\n                \n        except (RequestException, Timeout) as e:\n            if attempt < retries - 1:\n                time.sleep(CRAIGSLIST_RATE_LIMIT)\n                continue\n            log_craigslist(\"FETCH_ERROR\", details={\"error\": str(e)[:50]})\n            return None\n    \n    return None\n\n\ndef _parse_listing_page(html: str, url: str, region: str) -> Optional[CraigslistListing]:\n    \"\"\"Parse a Craigslist listing detail page.\"\"\"\n    try:\n        from bs4 import BeautifulSoup\n        soup = BeautifulSoup(html, 'html.parser')\n        \n        listing_id_match = re.search(r'/(\\d+)\\.html', url)\n        listing_id = listing_id_match.group(1) if listing_id_match else hashlib.md5(url.encode()).hexdigest()[:10]\n        \n        title_elem = soup.select_one('#titletextonly') or soup.select_one('.postingtitletext')\n        title = title_elem.get_text(strip=True) if title_elem else \"Unknown\"\n        \n        body_elem = soup.select_one('#postingbody')\n        body_text = \"\"\n        if body_elem:\n            for script in body_elem.find_all('script'):\n                script.decompose()\n            body_text = body_elem.get_text(strip=True, separator=\" \")\n            body_text = re.sub(r'\\s+', ' ', body_text)[:2000]\n        \n        price_elem = soup.select_one('.price')\n        price = price_elem.get_text(strip=True) if price_elem else None\n        \n        location_elem = soup.select_one('.postingtitletext small')\n        location = location_elem.get_text(strip=True).strip('()') if location_elem else None\n        \n        time_elem = soup.select_one('time.date.timeago')\n        posted_date = str(time_elem.get('datetime')) if time_elem and time_elem.get('datetime') else None\n        \n        phone = _extract_phone(body_text)\n        email = _extract_email(body_text)\n        niche = _detect_niche(title, body_text)\n        \n        images = []\n        for img in soup.select('.gallery img, .slide img')[:5]:\n            src = img.get('src') or img.get('data-src')\n            if src:\n                images.append(src)\n        \n        return CraigslistListing(\n            listing_id=listing_id,\n            title=title,\n            url=url,\n            price=price,\n            location=location,\n            posted_date=posted_date,\n            body_text=body_text,\n            contact_phone=phone,\n            contact_email=email,\n            images=images,\n            niche=niche,\n            region=region\n        )\n        \n    except Exception as e:\n        log_craigslist(\"PARSE_ERROR\", details={\"error\": str(e)[:50]})\n        return None\n\n\ndef _parse_search_results(html: str, base_url: str) -> List[Dict]:\n    \"\"\"Parse Craigslist search results page.\"\"\"\n    results = []\n    \n    try:\n        from bs4 import BeautifulSoup\n        soup = BeautifulSoup(html, 'html.parser')\n        \n        for result in soup.select('.result-row, .cl-search-result, li.cl-static-search-result'):\n            link = result.select_one('a.result-title, a.cl-app-anchor, a.titlestring')\n            if not link:\n                continue\n            \n            href_attr = link.get('href', '')\n            href = str(href_attr) if href_attr else ''\n            if not href or '/post/' in href:\n                continue\n            \n            title = link.get_text(strip=True)\n            \n            if href.startswith('/'):\n                href = urljoin(base_url, href)\n            \n            price_elem = result.select_one('.result-price, .priceinfo')\n            price = price_elem.get_text(strip=True) if price_elem else None\n            \n            loc_elem = result.select_one('.result-hood, .nearby')\n            location = loc_elem.get_text(strip=True).strip('()') if loc_elem else None\n            \n            results.append({\n                \"url\": href,\n                \"title\": title,\n                \"price\": price,\n                \"location\": location\n            })\n            \n    except ImportError:\n        log_craigslist(\"ERROR\", details={\"error\": \"BeautifulSoup not installed\"})\n    except Exception as e:\n        log_craigslist(\"PARSE_ERROR\", details={\"error\": str(e)[:50]})\n    \n    return results\n\n\ndef scan_region(\n    region: str = \"miami\",\n    category: str = \"services\",\n    niche: Optional[str] = None,\n    max_listings: int = 20,\n    fetch_details: bool = True\n) -> CraigslistScanResult:\n    \"\"\"\n    Scan a Craigslist region for listings.\n    \n    Args:\n        region: Target region (miami, fortlauderdale, palmbeach)\n        category: Search category\n        niche: Optional niche filter (hvac, plumbing, etc.)\n        max_listings: Maximum listings to return\n        fetch_details: Whether to fetch full listing details\n        \n    Returns:\n        CraigslistScanResult with discovered listings\n    \"\"\"\n    start_time = time.time()\n    listings: List[CraigslistListing] = []\n    pages_scanned = 0\n    \n    base_url = SOUTH_FLORIDA_REGIONS.get(region.lower(), SOUTH_FLORIDA_REGIONS[\"miami\"])\n    \n    search_path = SIGNAL_CATEGORIES.get(category, \"/search/sss\")\n    if niche and niche in SERVICE_CATEGORIES:\n        search_path = SERVICE_CATEGORIES[niche]\n    elif niche:\n        search_path = f\"/search/sss?query={quote(niche)}\"\n    \n    search_url = f\"{base_url}{search_path}\"\n    \n    log_craigslist(\"SCAN_START\", region, {\n        \"category\": category,\n        \"niche\": niche,\n        \"url\": search_url[:60]\n    })\n    \n    html = _fetch_page(search_url)\n    if not html:\n        return CraigslistScanResult(\n            success=False,\n            region=region,\n            category=category,\n            niche=niche,\n            error=\"Failed to fetch search results\"\n        )\n    \n    pages_scanned += 1\n    search_results = _parse_search_results(html, base_url)\n    \n    log_craigslist(\"SEARCH_RESULTS\", region, {\"count\": len(search_results)})\n    \n    for result in search_results[:max_listings]:\n        if fetch_details:\n            time.sleep(CRAIGSLIST_RATE_LIMIT)\n            \n            detail_html = _fetch_page(result[\"url\"])\n            if detail_html:\n                pages_scanned += 1\n                listing = _parse_listing_page(detail_html, result[\"url\"], region)\n                if listing:\n                    listing.category = category\n                    if niche:\n                        listing.niche = niche\n                    listings.append(listing)\n        else:\n            listing_id = re.search(r'/(\\d+)\\.html', result[\"url\"])\n            listing = CraigslistListing(\n                listing_id=listing_id.group(1) if listing_id else hashlib.md5(result[\"url\"].encode()).hexdigest()[:10],\n                title=result[\"title\"],\n                url=result[\"url\"],\n                price=result.get(\"price\"),\n                location=result.get(\"location\"),\n                region=region,\n                category=category,\n                niche=niche or _detect_niche(result[\"title\"])\n            )\n            listings.append(listing)\n    \n    elapsed_ms = int((time.time() - start_time) * 1000)\n    \n    log_craigslist(\"SCAN_COMPLETE\", region, {\n        \"listings\": len(listings),\n        \"pages\": pages_scanned,\n        \"time_ms\": elapsed_ms\n    })\n    \n    return CraigslistScanResult(\n        success=True,\n        listings=listings,\n        region=region,\n        category=category,\n        niche=niche,\n        scan_time_ms=elapsed_ms,\n        pages_scanned=pages_scanned\n    )\n\n\ndef scan_all_regions(\n    category: str = \"services\",\n    niche: Optional[str] = None,\n    max_per_region: int = 10\n) -> List[CraigslistListing]:\n    \"\"\"\n    Scan all South Florida regions for listings.\n    \n    Args:\n        category: Search category\n        niche: Optional niche filter\n        max_per_region: Max listings per region\n        \n    Returns:\n        Combined list of listings from all regions\n    \"\"\"\n    all_listings: List[CraigslistListing] = []\n    \n    for region in SOUTH_FLORIDA_REGIONS.keys():\n        result = scan_region(\n            region=region,\n            category=category,\n            niche=niche,\n            max_listings=max_per_region,\n            fetch_details=True\n        )\n        \n        if result.success:\n            all_listings.extend(result.listings)\n        \n        time.sleep(CRAIGSLIST_RATE_LIMIT * 2)\n    \n    return all_listings\n\n\ndef convert_to_signal(listing: CraigslistListing) -> Dict:\n    \"\"\"\n    Convert a Craigslist listing to a Signal-compatible dict.\n    \n    Returns a dict ready for Signal model creation.\n    \"\"\"\n    company_name = _extract_company_name(listing.title, listing.body_text)\n    \n    return {\n        \"source_type\": \"craigslist\",\n        \"raw_payload\": json.dumps(listing.to_dict()),\n        \"context_summary\": listing.title,\n        \"geography\": listing.region.title() if listing.region else \"Miami\",\n        \"extracted_contact_info\": json.dumps({\n            \"extracted_emails\": [listing.contact_email] if listing.contact_email else [],\n            \"extracted_phones\": [listing.contact_phone] if listing.contact_phone else [],\n            \"extracted_urls\": [listing.url],\n            \"source_confidence\": 0.7 if listing.contact_phone else 0.5\n        }),\n        \"metadata\": {\n            \"listing_id\": listing.listing_id,\n            \"signal_id\": listing.generate_signal_id(),\n            \"niche\": listing.niche,\n            \"category\": listing.category,\n            \"price\": listing.price,\n            \"location\": listing.location,\n            \"company_name\": company_name,\n            \"posted_date\": listing.posted_date\n        }\n    }\n\n\ndef generate_lead_event(listing: CraigslistListing) -> Dict:\n    \"\"\"\n    Generate LeadEvent-compatible data from a Craigslist listing.\n    \n    Returns a dict ready for LeadEvent model creation.\n    \"\"\"\n    company_name = _extract_company_name(listing.title, listing.body_text)\n    niche = listing.niche or _detect_niche(listing.title, listing.body_text)\n    \n    category_map = {\n        \"hvac\": \"growth_opportunity\",\n        \"plumbing\": \"growth_opportunity\",\n        \"roofing\": \"growth_opportunity\",\n        \"construction\": \"growth_opportunity\",\n        \"business_for_sale\": \"market_entry\",\n        \"services_offered\": \"competitor_intel\",\n    }\n    \n    category = \"growth_opportunity\"\n    if niche and niche in category_map:\n        category = category_map[niche]\n    elif listing.category in category_map:\n        category = category_map[listing.category]\n    \n    return {\n        \"lead_company\": company_name,\n        \"lead_phone_raw\": listing.contact_phone,\n        \"lead_email\": listing.contact_email,\n        \"summary\": f\"[Craigslist] {listing.title} - {listing.location or listing.region}\",\n        \"category\": category,\n        \"urgency_score\": 65 if listing.contact_phone else 55,\n        \"recommended_action\": f\"Potential {niche or 'service'} opportunity in {listing.region}. Contact to explore partnership or service needs.\",\n        \"enrichment_status\": \"UNENRICHED\",\n        \"metadata\": {\n            \"source\": \"craigslist\",\n            \"listing_id\": listing.listing_id,\n            \"niche\": niche,\n            \"region\": listing.region,\n            \"url\": listing.url\n        }\n    }\n","path":null,"size_bytes":19253,"size_tokens":null},"company_name_extraction.py":{"content":"\"\"\"\nOPERATION NAMESTORM: Enhanced Company Name Extraction Engine\n\nExtracts high-confidence company names from signals using multiple sources:\n1. Schema.org / JSON-LD structured data\n2. OpenGraph and meta tag parsing\n3. NER-like pattern matching for business names\n4. Article headline and body heuristics\n\nEach extraction method returns candidates with confidence scores.\nThe best candidate is selected based on weighted scoring.\n\nPure web scraping - NO paid NER APIs required.\n\"\"\"\n\nimport os\nimport re\nimport json\nfrom dataclasses import dataclass, field\nfrom typing import List, Optional, Dict, Set, Tuple\nfrom datetime import datetime\n\nimport requests\nfrom requests.exceptions import RequestException, Timeout\n\nNAMESTORM_TIMEOUT = int(os.getenv(\"NAMESTORM_TIMEOUT\", \"8\"))\n\nUSER_AGENTS = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n]\n\nBUSINESS_TERMS = {\n    'air', 'hvac', 'ac', 'cooling', 'heating', 'plumbing', 'electric', 'electrical',\n    'roofing', 'construction', 'remodeling', 'renovation', 'landscaping', 'lawn',\n    'pool', 'spa', 'med spa', 'medspa', 'medical', 'clinic', 'dental', 'chiropractic',\n    'realty', 'real estate', 'properties', 'property', 'homes', 'mortgage',\n    'salon', 'nails', 'beauty', 'barber', 'hair', 'lashes', 'aesthetics',\n    'law', 'legal', 'attorney', 'attorneys', 'lawyers', 'immigration', 'injury',\n    'marketing', 'agency', 'media', 'studio', 'design', 'creative', 'digital',\n    'consulting', 'consultants', 'advisors', 'advisory', 'partners', 'group',\n    'solutions', 'services', 'systems', 'technologies', 'tech', 'software',\n    'insurance', 'financial', 'accounting', 'tax', 'bookkeeping', 'cpa',\n    'moving', 'storage', 'logistics', 'transport', 'trucking', 'shipping',\n    'cleaning', 'janitorial', 'maid', 'housekeeping', 'restoration', 'remediation',\n    'pest', 'termite', 'exterminating', 'wildlife', 'animal',\n    'security', 'alarm', 'locksmith', 'doors', 'windows', 'glass',\n    'flooring', 'carpet', 'tile', 'painting', 'drywall', 'insulation',\n    'garage', 'doors', 'fencing', 'paving', 'concrete', 'masonry',\n    'auto', 'automotive', 'collision', 'body shop', 'mechanic', 'tire',\n    'restaurant', 'catering', 'food', 'bakery', 'cafe', 'coffee',\n    'fitness', 'gym', 'yoga', 'pilates', 'personal training', 'crossfit',\n    'daycare', 'childcare', 'preschool', 'tutoring', 'education',\n    'pet', 'veterinary', 'vet', 'grooming', 'boarding', 'kennel',\n    'photography', 'video', 'production', 'events', 'wedding', 'dj',\n    'inc', 'llc', 'corp', 'corporation', 'co', 'company', 'enterprises',\n    'associates', 'holdings', 'international', 'global', 'premier', 'elite',\n    'professional', 'pro', 'express', 'rapid', 'quick', 'fast',\n    'first', 'best', 'top', 'prime', 'superior', 'quality', 'choice',\n    'american', 'national', 'united', 'coastal', 'southern', 'florida',\n}\n\nGENERIC_NAMES_BLOCK = {\n    'the company', 'this company', 'the business', 'your company', 'our company',\n    'developer', 'owner', 'manager', 'president', 'ceo', 'founder',\n    'news', 'report', 'update', 'article', 'story', 'press', 'release',\n    'south florida', 'miami', 'broward', 'palm beach', 'orlando', 'tampa',\n    'florida', 'texas', 'california', 'new york', 'chicago',\n    'local', 'area', 'region', 'county', 'city', 'state', 'national',\n}\n\nNEWS_OUTLET_NAMES = {\n    'miami herald', 'sun sentinel', 'palm beach post', 'orlando sentinel',\n    'tampa bay times', 'south florida business journal', 'bizjournals',\n    'cbs miami', 'nbc miami', 'abc news', 'fox news', 'cnn', 'reuters',\n    'associated press', 'bloomberg', 'wall street journal', 'new york times',\n    'washington post', 'local10', 'wsvn', 'wplg', 'wfor', 'wtvj',\n}\n\n\n@dataclass\nclass CompanyCandidate:\n    \"\"\"A potential company name with confidence scoring.\"\"\"\n    name: str\n    confidence: float\n    source: str\n    raw_match: Optional[str] = None\n    \n    def to_dict(self) -> Dict:\n        return {\n            \"name\": self.name,\n            \"confidence\": self.confidence,\n            \"source\": self.source,\n            \"raw_match\": self.raw_match\n        }\n\n\n@dataclass\nclass NameStormResult:\n    \"\"\"Result of company name extraction.\"\"\"\n    success: bool\n    best_candidate: Optional[CompanyCandidate] = None\n    all_candidates: List[CompanyCandidate] = field(default_factory=list)\n    source_url: Optional[str] = None\n    extraction_time_ms: int = 0\n    error: Optional[str] = None\n    \n    def to_dict(self) -> Dict:\n        return {\n            \"success\": self.success,\n            \"best_candidate\": self.best_candidate.to_dict() if self.best_candidate else None,\n            \"all_candidates\": [c.to_dict() for c in self.all_candidates],\n            \"source_url\": self.source_url,\n            \"extraction_time_ms\": self.extraction_time_ms,\n            \"error\": self.error\n        }\n\n\ndef log_namestorm(action: str, lead_event_id: Optional[int] = None, details: Optional[Dict] = None) -> None:\n    \"\"\"Log NAMESTORM extraction activity.\"\"\"\n    msg_parts = [f\"[NAMESTORM][{action.upper()}]\"]\n    if lead_event_id:\n        msg_parts.append(f\"event={lead_event_id}\")\n    if details:\n        for k, v in details.items():\n            if isinstance(v, str) and len(v) > 60:\n                v = v[:60] + \"...\"\n            msg_parts.append(f\"{k}={v}\")\n    print(\" | \".join(msg_parts))\n\n\ndef _normalize_company_name(name: str) -> str:\n    \"\"\"Clean and normalize company name.\"\"\"\n    if not name:\n        return \"\"\n    \n    name = re.sub(r'\\s+', ' ', name.strip())\n    name = re.sub(r'[\"\"''`]', '', name)\n    name = re.sub(r'^(a|an|the)\\s+', '', name, flags=re.IGNORECASE)\n    name = re.sub(r'\\s*[,;:]\\s*$', '', name)\n    name = re.sub(r'\\s*\\.{2,}\\s*$', '', name)\n    name = re.sub(r'\\s+(announced|expands|opens|acquires|launches|hires|reports|says|to|will|has|is|are|was|were|bought|sold|filed|closes).*$', '', name, flags=re.IGNORECASE)\n    \n    return name.strip()\n\n\ndef _has_business_term(name: str) -> bool:\n    \"\"\"Check if name contains a business-related term.\"\"\"\n    name_lower = name.lower()\n    for term in BUSINESS_TERMS:\n        if term in name_lower:\n            return True\n    return False\n\n\ndef _is_blocked_name(name: str) -> bool:\n    \"\"\"Check if name should be blocked.\"\"\"\n    if not name:\n        return True\n    \n    name_lower = name.lower().strip()\n    \n    if len(name_lower) < 3:\n        return True\n    \n    if name_lower in GENERIC_NAMES_BLOCK:\n        return True\n    \n    if name_lower in NEWS_OUTLET_NAMES:\n        return True\n    \n    for blocked in GENERIC_NAMES_BLOCK:\n        if name_lower == blocked:\n            return True\n    \n    for outlet in NEWS_OUTLET_NAMES:\n        if name_lower == outlet or outlet in name_lower:\n            return True\n    \n    words = name_lower.split()\n    if len(words) == 1 and words[0] not in BUSINESS_TERMS:\n        if not words[0][0].isupper() if name else True:\n            return True\n    \n    geo_only_pattern = r'^(miami|broward|palm beach|orlando|tampa|florida|south florida|texas|california)\\s+(company|business|firm|group|owner)$'\n    if re.match(geo_only_pattern, name_lower):\n        return True\n    \n    generic_patterns = [\n        r'^(owner|manager|president|ceo|founder)\\s+of\\s+',\n        r'buys new', r'opens new', r'expands to', r'announces',\n        r'^(local|area|regional)\\s+(hvac|roofing|plumbing)',\n    ]\n    for pattern in generic_patterns:\n        if re.search(pattern, name_lower):\n            return True\n    \n    return False\n\n\ndef _calculate_confidence(name: str, source: str) -> float:\n    \"\"\"Calculate confidence score for a company name candidate.\"\"\"\n    base_confidence = {\n        \"schema_org\": 0.95,\n        \"og_site_name\": 0.85,\n        \"og_title\": 0.75,\n        \"meta_title\": 0.70,\n        \"h1_heading\": 0.65,\n        \"ner_pattern\": 0.60,\n        \"title_extraction\": 0.55,\n        \"body_heuristic\": 0.50,\n        \"summary_pattern\": 0.45,\n    }.get(source, 0.40)\n    \n    if _has_business_term(name):\n        base_confidence += 0.10\n    \n    word_count = len(name.split())\n    if 2 <= word_count <= 4:\n        base_confidence += 0.05\n    elif word_count > 6:\n        base_confidence -= 0.10\n    \n    if name[0].isupper():\n        base_confidence += 0.05\n    \n    if re.search(r'\\b(Inc|LLC|Corp|Co)\\b', name, re.IGNORECASE):\n        base_confidence += 0.05\n    \n    return min(1.0, max(0.0, base_confidence))\n\n\ndef extract_from_schema_org(html: str) -> List[CompanyCandidate]:\n    \"\"\"Extract company names from Schema.org JSON-LD structured data.\"\"\"\n    candidates = []\n    \n    try:\n        jsonld_pattern = re.compile(\n            r'<script[^>]*type=[\"\\']application/ld\\+json[\"\\'][^>]*>(.*?)</script>',\n            re.DOTALL | re.IGNORECASE\n        )\n        \n        for match in jsonld_pattern.finditer(html):\n            try:\n                data = json.loads(match.group(1).strip())\n                \n                if isinstance(data, list):\n                    for item in data:\n                        candidates.extend(_process_schema_item(item))\n                else:\n                    candidates.extend(_process_schema_item(data))\n                    \n            except json.JSONDecodeError:\n                continue\n                \n    except Exception:\n        pass\n    \n    return candidates\n\n\ndef _process_schema_item(item: Dict) -> List[CompanyCandidate]:\n    \"\"\"Process a single schema.org item for company names.\"\"\"\n    candidates = []\n    \n    if not isinstance(item, dict):\n        return candidates\n    \n    org_types = {'Organization', 'LocalBusiness', 'Corporation', 'LegalService',\n                 'HomeAndConstructionBusiness', 'ProfessionalService', 'MedicalBusiness',\n                 'FinancialService', 'RealEstateAgent', 'Store', 'Restaurant'}\n    \n    item_type = item.get('@type', '')\n    if isinstance(item_type, list):\n        item_type = item_type[0] if item_type else ''\n    \n    if item_type in org_types:\n        name = item.get('name', '')\n        if name and not _is_blocked_name(name):\n            normalized = _normalize_company_name(name)\n            if normalized and not _is_blocked_name(normalized):\n                candidates.append(CompanyCandidate(\n                    name=normalized,\n                    confidence=_calculate_confidence(normalized, \"schema_org\"),\n                    source=\"schema_org\",\n                    raw_match=name\n                ))\n    \n    if '@graph' in item:\n        for node in item['@graph']:\n            candidates.extend(_process_schema_item(node))\n    \n    return candidates\n\n\ndef extract_from_meta_tags(html: str) -> List[CompanyCandidate]:\n    \"\"\"Extract company names from OpenGraph and meta tags.\"\"\"\n    candidates = []\n    \n    try:\n        og_site_name = re.search(\n            r'<meta[^>]*property=[\"\\']og:site_name[\"\\'][^>]*content=[\"\\']([^\"\\']+)[\"\\']',\n            html, re.IGNORECASE\n        )\n        if not og_site_name:\n            og_site_name = re.search(\n                r'<meta[^>]*content=[\"\\']([^\"\\']+)[\"\\'][^>]*property=[\"\\']og:site_name[\"\\']',\n                html, re.IGNORECASE\n            )\n        \n        if og_site_name:\n            name = _normalize_company_name(og_site_name.group(1))\n            if name and not _is_blocked_name(name):\n                candidates.append(CompanyCandidate(\n                    name=name,\n                    confidence=_calculate_confidence(name, \"og_site_name\"),\n                    source=\"og_site_name\",\n                    raw_match=og_site_name.group(1)\n                ))\n        \n        og_title = re.search(\n            r'<meta[^>]*property=[\"\\']og:title[\"\\'][^>]*content=[\"\\']([^\"\\']+)[\"\\']',\n            html, re.IGNORECASE\n        )\n        if og_title:\n            title_text = og_title.group(1)\n            parts = re.split(r'\\s*[|\\-]\\s*', title_text)\n            for part in parts:\n                name = _normalize_company_name(part)\n                if name and not _is_blocked_name(name) and _has_business_term(name):\n                    candidates.append(CompanyCandidate(\n                        name=name,\n                        confidence=_calculate_confidence(name, \"og_title\"),\n                        source=\"og_title\",\n                        raw_match=part\n                    ))\n        \n        title_match = re.search(r'<title[^>]*>([^<]+)</title>', html, re.IGNORECASE)\n        if title_match:\n            title_text = title_match.group(1)\n            parts = re.split(r'\\s*[|\\-]\\s*', title_text)\n            for part in parts:\n                name = _normalize_company_name(part)\n                if name and not _is_blocked_name(name) and _has_business_term(name):\n                    candidates.append(CompanyCandidate(\n                        name=name,\n                        confidence=_calculate_confidence(name, \"meta_title\"),\n                        source=\"meta_title\",\n                        raw_match=part\n                    ))\n                    \n    except Exception:\n        pass\n    \n    return candidates\n\n\ndef extract_from_headings(html: str) -> List[CompanyCandidate]:\n    \"\"\"Extract company names from H1/H2 headings.\"\"\"\n    candidates = []\n    \n    try:\n        h1_pattern = re.compile(r'<h1[^>]*>([^<]+)</h1>', re.IGNORECASE)\n        for match in h1_pattern.finditer(html[:20000]):\n            text = match.group(1).strip()\n            name = _normalize_company_name(text)\n            if name and not _is_blocked_name(name) and _has_business_term(name):\n                candidates.append(CompanyCandidate(\n                    name=name,\n                    confidence=_calculate_confidence(name, \"h1_heading\"),\n                    source=\"h1_heading\",\n                    raw_match=text\n                ))\n                \n    except Exception:\n        pass\n    \n    return candidates\n\n\ndef extract_from_text_patterns(text: str, source_type: str = \"body_heuristic\") -> List[CompanyCandidate]:\n    \"\"\"\n    Extract company names using NER-like pattern matching.\n    \n    Patterns matched:\n    - Proper Noun + Business Term (e.g., \"Miami Best Roofing\")\n    - Business Name + Legal Suffix (e.g., \"Smith & Sons LLC\")\n    - Quote-enclosed names (e.g., '\"Acme Corp\" announced...')\n    \"\"\"\n    candidates = []\n    \n    if not text:\n        return candidates\n    \n    try:\n        business_term_pattern = '|'.join(re.escape(t) for t in sorted(BUSINESS_TERMS, key=len, reverse=True))\n        \n        patterns = [\n            rf'([A-Z][a-zA-Z]+(?:\\s+[A-Z]?[a-zA-Z&\\'\\-]+){{0,4}}\\s+(?:{business_term_pattern}))',\n            \n            r'([A-Z][a-zA-Z]+(?:\\s+[A-Z]?[a-zA-Z&\\'\\-]+){0,3}\\s+(?:Inc|LLC|Corp|Co|Ltd|LLP|PLLC|PC|PA)\\.?)',\n            \n            r'\"([A-Z][^\"]{3,50})\"',\n            \n            r'([A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+){1,3}),\\s+(?:a|an|the)\\s+(?:Miami|Florida|local|leading)',\n        ]\n        \n        for pattern in patterns:\n            for match in re.finditer(pattern, text, re.IGNORECASE):\n                raw_name = match.group(1) if match.lastindex else match.group(0)\n                name = _normalize_company_name(raw_name)\n                \n                if name and not _is_blocked_name(name) and len(name) >= 3:\n                    if _has_business_term(name) or re.search(r'\\b(Inc|LLC|Corp)\\b', name, re.IGNORECASE):\n                        candidates.append(CompanyCandidate(\n                            name=name,\n                            confidence=_calculate_confidence(name, source_type),\n                            source=source_type,\n                            raw_match=raw_name\n                        ))\n                        \n    except Exception:\n        pass\n    \n    return candidates\n\n\ndef extract_company_candidates(\n    title: Optional[str] = None,\n    summary: Optional[str] = None,\n    source_url: Optional[str] = None,\n    lead_event_id: Optional[int] = None,\n    fetch_page: bool = True\n) -> NameStormResult:\n    \"\"\"\n    NAMESTORM: Extract company name candidates from signal context.\n    \n    Uses multiple extraction methods in priority order:\n    1. Schema.org JSON-LD from article page\n    2. OpenGraph meta tags\n    3. Page title parsing\n    4. H1 heading extraction\n    5. NER-like patterns from title/summary\n    6. Heuristic patterns from article body\n    \n    Args:\n        title: Signal title or headline\n        summary: Signal summary/description\n        source_url: URL of the signal source\n        lead_event_id: Optional ID for logging\n        fetch_page: Whether to fetch the source URL for extraction\n        \n    Returns:\n        NameStormResult with sorted candidates (highest confidence first)\n    \"\"\"\n    import time\n    start_time = time.time()\n    \n    all_candidates: List[CompanyCandidate] = []\n    \n    log_namestorm(\"START\", lead_event_id, {\n        \"has_title\": bool(title),\n        \"has_summary\": bool(summary),\n        \"has_url\": bool(source_url)\n    })\n    \n    if title:\n        title_candidates = extract_from_text_patterns(title, \"title_extraction\")\n        all_candidates.extend(title_candidates)\n    \n    if summary:\n        summary_candidates = extract_from_text_patterns(summary, \"summary_pattern\")\n        all_candidates.extend(summary_candidates)\n    \n    if fetch_page and source_url and 'news.google.com' not in source_url:\n        try:\n            headers = {\"User-Agent\": USER_AGENTS[0]}\n            response = requests.get(source_url, headers=headers, timeout=NAMESTORM_TIMEOUT)\n            \n            if response.status_code == 200:\n                html = response.text[:100000]\n                \n                schema_candidates = extract_from_schema_org(html)\n                all_candidates.extend(schema_candidates)\n                \n                meta_candidates = extract_from_meta_tags(html)\n                all_candidates.extend(meta_candidates)\n                \n                heading_candidates = extract_from_headings(html)\n                all_candidates.extend(heading_candidates)\n                \n                try:\n                    from bs4 import BeautifulSoup\n                    soup = BeautifulSoup(html, 'html.parser')\n                    for tag in soup(['script', 'style', 'nav', 'footer', 'header']):\n                        tag.decompose()\n                    \n                    paragraphs = soup.find_all('p')[:10]\n                    body_text = ' '.join(p.get_text(strip=True) for p in paragraphs)\n                    \n                    if body_text:\n                        body_candidates = extract_from_text_patterns(body_text, \"body_heuristic\")\n                        all_candidates.extend(body_candidates)\n                except ImportError:\n                    pass\n                    \n        except (RequestException, Timeout) as e:\n            log_namestorm(\"FETCH_ERROR\", lead_event_id, {\"error\": str(e)[:50]})\n    \n    seen_names: Set[str] = set()\n    unique_candidates: List[CompanyCandidate] = []\n    \n    for candidate in all_candidates:\n        name_key = candidate.name.lower().strip()\n        if name_key not in seen_names:\n            seen_names.add(name_key)\n            unique_candidates.append(candidate)\n    \n    unique_candidates.sort(key=lambda c: c.confidence, reverse=True)\n    \n    elapsed_ms = int((time.time() - start_time) * 1000)\n    \n    if unique_candidates:\n        best = unique_candidates[0]\n        log_namestorm(\"CANDIDATES\", lead_event_id, {\n            \"count\": len(unique_candidates),\n            \"best\": best.name,\n            \"confidence\": f\"{best.confidence:.2f}\",\n            \"source\": best.source\n        })\n        \n        return NameStormResult(\n            success=True,\n            best_candidate=best,\n            all_candidates=unique_candidates[:10],\n            source_url=source_url,\n            extraction_time_ms=elapsed_ms\n        )\n    else:\n        log_namestorm(\"NO_CANDIDATES\", lead_event_id, {\n            \"title\": title[:40] if title else None,\n            \"summary\": summary[:40] if summary else None\n        })\n        \n        return NameStormResult(\n            success=False,\n            source_url=source_url,\n            extraction_time_ms=elapsed_ms,\n            error=\"No valid company candidates found\"\n        )\n\n\ndef get_best_company_name(\n    title: Optional[str] = None,\n    summary: Optional[str] = None,\n    source_url: Optional[str] = None,\n    lead_event_id: Optional[int] = None,\n    min_confidence: float = 0.5\n) -> Optional[str]:\n    \"\"\"\n    Convenience function to get the best company name or None.\n    \n    Returns the highest-confidence company name if it meets the threshold.\n    \"\"\"\n    result = extract_company_candidates(\n        title=title,\n        summary=summary,\n        source_url=source_url,\n        lead_event_id=lead_event_id,\n        fetch_page=True\n    )\n    \n    if result.success and result.best_candidate:\n        if result.best_candidate.confidence >= min_confidence:\n            return result.best_candidate.name\n    \n    return None\n","path":null,"size_bytes":21036,"size_tokens":null},"mission_log.py":{"content":"\"\"\"\nARCHANGEL v2: Mission Log System\n\nPer-lead enrichment history tracking - the \"black box recorder\" for enrichment attempts.\nPrevents re-doing the same queries and provides ML-ready structured logs.\n\nEach enrichment subroutine (NameStorm/DomainStorm/PhoneStorm/EmailStorm) writes structured entries.\nBefore doing something \"expensive\" (search, scrape), check log to avoid exact repeats.\n\"\"\"\n\nimport json\nfrom datetime import datetime\nfrom typing import List, Optional, Dict, Any\nfrom dataclasses import dataclass, field, asdict\n\n\n@dataclass\nclass MissionLogEntry:\n    \"\"\"Single enrichment attempt record.\"\"\"\n    timestamp: str\n    pass_number: int\n    phase: str  # NAMESTORM, DOMAINSTORM, PHONESTORM, EMAILSTORM\n    action: str  # duckduckgo_search, page_scrape, email_validation, etc.\n    query: Optional[str] = None  # Search query or URL attempted\n    result: str = \"pending\"  # success, no_result, error, skipped, cached\n    notes: Optional[str] = None  # Additional context\n    duration_ms: int = 0\n    \n    def to_dict(self) -> Dict[str, Any]:\n        return {k: v for k, v in asdict(self).items() if v is not None}\n\n\nclass MissionLog:\n    \"\"\"\n    Mission log manager for a single lead's enrichment history.\n    \n    Usage:\n        log = MissionLog.from_json(lead_event.enrichment_mission_log)\n        \n        if not log.has_attempted(\"DOMAINSTORM\", \"duckduckgo_search\", \"cool running air miami\"):\n            # Do the search\n            result = search_duckduckgo(\"cool running air miami\")\n            log.add_entry(\n                phase=\"DOMAINSTORM\",\n                action=\"duckduckgo_search\",\n                query=\"cool running air miami\",\n                result=\"success\" if result else \"no_result\"\n            )\n        \n        lead_event.enrichment_mission_log = log.to_json()\n    \"\"\"\n    \n    def __init__(self, entries: Optional[List[MissionLogEntry]] = None):\n        self.entries: List[MissionLogEntry] = entries or []\n        self._current_pass: int = 1\n    \n    @classmethod\n    def from_json(cls, json_str: Optional[str]) -> \"MissionLog\":\n        \"\"\"Parse mission log from JSON string.\"\"\"\n        if not json_str:\n            return cls()\n        \n        try:\n            data = json.loads(json_str)\n            entries = []\n            max_pass = 0\n            \n            for entry_dict in data:\n                entry = MissionLogEntry(\n                    timestamp=entry_dict.get(\"timestamp\", \"\"),\n                    pass_number=entry_dict.get(\"pass_number\", entry_dict.get(\"pass\", 1)),\n                    phase=entry_dict.get(\"phase\", \"UNKNOWN\"),\n                    action=entry_dict.get(\"action\", \"unknown\"),\n                    query=entry_dict.get(\"query\"),\n                    result=entry_dict.get(\"result\", \"pending\"),\n                    notes=entry_dict.get(\"notes\"),\n                    duration_ms=entry_dict.get(\"duration_ms\", 0)\n                )\n                entries.append(entry)\n                max_pass = max(max_pass, entry.pass_number)\n            \n            log = cls(entries)\n            log._current_pass = max_pass if max_pass > 0 else 1\n            return log\n            \n        except (json.JSONDecodeError, TypeError, KeyError):\n            return cls()\n    \n    def to_json(self) -> str:\n        \"\"\"Serialize mission log to JSON string.\"\"\"\n        return json.dumps([e.to_dict() for e in self.entries], default=str)\n    \n    def add_entry(\n        self,\n        phase: str,\n        action: str,\n        query: Optional[str] = None,\n        result: str = \"pending\",\n        notes: Optional[str] = None,\n        duration_ms: int = 0\n    ) -> MissionLogEntry:\n        \"\"\"Add a new entry to the mission log.\"\"\"\n        entry = MissionLogEntry(\n            timestamp=datetime.utcnow().isoformat(),\n            pass_number=self._current_pass,\n            phase=phase,\n            action=action,\n            query=query,\n            result=result,\n            notes=notes,\n            duration_ms=duration_ms\n        )\n        self.entries.append(entry)\n        return entry\n    \n    def start_new_pass(self) -> int:\n        \"\"\"Increment pass number for a new enrichment cycle.\"\"\"\n        self._current_pass += 1\n        return self._current_pass\n    \n    @property\n    def current_pass(self) -> int:\n        return self._current_pass\n    \n    def has_attempted(self, phase: str, action: str, query: Optional[str] = None) -> bool:\n        \"\"\"Check if this exact phase/action/query combination was already tried.\"\"\"\n        for entry in self.entries:\n            if entry.phase == phase and entry.action == action:\n                if query is None or entry.query == query:\n                    return True\n        return False\n    \n    def has_succeeded(self, phase: str, action: Optional[str] = None) -> bool:\n        \"\"\"Check if any action in this phase succeeded.\"\"\"\n        for entry in self.entries:\n            if entry.phase == phase and entry.result == \"success\":\n                if action is None or entry.action == action:\n                    return True\n        return False\n    \n    def get_attempts_for_phase(self, phase: str) -> List[MissionLogEntry]:\n        \"\"\"Get all attempts for a specific phase.\"\"\"\n        return [e for e in self.entries if e.phase == phase]\n    \n    def get_failed_queries(self, phase: str, action: str) -> List[str]:\n        \"\"\"Get list of queries that failed for this phase/action.\"\"\"\n        return [\n            e.query for e in self.entries\n            if e.phase == phase and e.action == action \n            and e.result in (\"no_result\", \"error\") and e.query\n        ]\n    \n    def count_attempts(self, phase: Optional[str] = None) -> int:\n        \"\"\"Count total attempts, optionally filtered by phase.\"\"\"\n        if phase:\n            return len([e for e in self.entries if e.phase == phase])\n        return len(self.entries)\n    \n    def get_last_entry(self, phase: Optional[str] = None) -> Optional[MissionLogEntry]:\n        \"\"\"Get the most recent entry, optionally filtered by phase.\"\"\"\n        filtered = self.entries if phase is None else [e for e in self.entries if e.phase == phase]\n        return filtered[-1] if filtered else None\n    \n    def get_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary statistics of the mission log.\"\"\"\n        phases = {}\n        for entry in self.entries:\n            if entry.phase not in phases:\n                phases[entry.phase] = {\"attempts\": 0, \"successes\": 0, \"failures\": 0}\n            phases[entry.phase][\"attempts\"] += 1\n            if entry.result == \"success\":\n                phases[entry.phase][\"successes\"] += 1\n            elif entry.result in (\"no_result\", \"error\"):\n                phases[entry.phase][\"failures\"] += 1\n        \n        return {\n            \"total_entries\": len(self.entries),\n            \"current_pass\": self._current_pass,\n            \"phases\": phases\n        }\n    \n    def get_condensed_view(self, limit: int = 5) -> List[Dict]:\n        \"\"\"Get condensed view of last N entries for UI display.\"\"\"\n        recent = self.entries[-limit:] if len(self.entries) > limit else self.entries\n        return [\n            {\n                \"pass\": e.pass_number,\n                \"phase\": e.phase,\n                \"action\": e.action,\n                \"result\": e.result,\n                \"notes\": e.notes[:50] if e.notes else None\n            }\n            for e in recent\n        ]\n\n\ndef log_enrichment_attempt(\n    mission_log: MissionLog,\n    phase: str,\n    action: str,\n    query: Optional[str] = None,\n    result: str = \"pending\",\n    notes: Optional[str] = None,\n    duration_ms: int = 0\n) -> MissionLog:\n    \"\"\"\n    Convenience function to log an enrichment attempt.\n    \n    Returns the updated mission log for chaining.\n    \"\"\"\n    mission_log.add_entry(\n        phase=phase,\n        action=action,\n        query=query,\n        result=result,\n        notes=notes,\n        duration_ms=duration_ms\n    )\n    return mission_log\n\n\ndef should_attempt_action(\n    mission_log: MissionLog,\n    phase: str,\n    action: str,\n    query: Optional[str] = None,\n    max_retries: int = 1\n) -> bool:\n    \"\"\"\n    Check if we should attempt an action based on mission log history.\n    \n    Returns False if:\n    - This exact query was already tried and succeeded\n    - This exact query was tried max_retries times\n    \"\"\"\n    attempts = [\n        e for e in mission_log.entries\n        if e.phase == phase and e.action == action\n        and (query is None or e.query == query)\n    ]\n    \n    for attempt in attempts:\n        if attempt.result == \"success\":\n            return False\n    \n    if len(attempts) >= max_retries:\n        return False\n    \n    return True\n","path":null,"size_bytes":8638,"size_tokens":null},"generate_sanity_check_pdf.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nGenerate PDF: Melissa's Sanity Check Response to HossAgent Alignment Doc\n\"\"\"\n\nfrom fpdf import FPDF\nfrom datetime import datetime\n\nclass SanityCheckPDF(FPDF):\n    def header(self):\n        self.set_font('Helvetica', 'B', 10)\n        self.set_text_color(100, 100, 100)\n        self.cell(0, 10, 'HossAgent Alignment Doc - Sanity Check Response', 0, 1, 'C')\n        self.ln(2)\n    \n    def footer(self):\n        self.set_y(-15)\n        self.set_font('Helvetica', 'I', 8)\n        self.set_text_color(128, 128, 128)\n        self.cell(0, 10, f'Page {self.page_no()}', 0, 0, 'C')\n    \n    def section_title(self, title):\n        self.set_font('Helvetica', 'B', 14)\n        self.set_text_color(0, 0, 0)\n        self.ln(5)\n        self.cell(0, 10, title, 0, 1, 'L')\n        self.set_draw_color(34, 197, 94)\n        self.set_line_width(0.5)\n        self.line(10, self.get_y(), 200, self.get_y())\n        self.ln(5)\n    \n    def subsection_title(self, title):\n        self.set_font('Helvetica', 'B', 11)\n        self.set_text_color(50, 50, 50)\n        self.ln(3)\n        self.cell(0, 8, title, 0, 1, 'L')\n        self.ln(2)\n    \n    def body_text(self, text):\n        self.set_font('Helvetica', '', 10)\n        self.set_text_color(30, 30, 30)\n        self.multi_cell(0, 6, text)\n        self.ln(2)\n    \n    def bullet_point(self, text, indent=10):\n        self.set_font('Helvetica', '', 10)\n        self.set_text_color(30, 30, 30)\n        self.set_x(indent)\n        self.cell(5, 6, chr(149), 0, 0)\n        self.multi_cell(0, 6, text)\n    \n    def table_row(self, col1, col2, col3, header=False):\n        if header:\n            self.set_font('Helvetica', 'B', 9)\n            self.set_fill_color(240, 240, 240)\n        else:\n            self.set_font('Helvetica', '', 9)\n            self.set_fill_color(255, 255, 255)\n        \n        self.set_text_color(30, 30, 30)\n        col_widths = [45, 60, 85]\n        \n        self.cell(col_widths[0], 8, col1, 1, 0, 'L', fill=header)\n        self.cell(col_widths[1], 8, col2, 1, 0, 'L', fill=header)\n        self.cell(col_widths[2], 8, col3, 1, 1, 'L', fill=header)\n    \n    def status_row(self, component, status, notes):\n        self.set_font('Helvetica', '', 9)\n        self.set_text_color(30, 30, 30)\n        col_widths = [45, 60, 85]\n        \n        self.cell(col_widths[0], 8, component, 1, 0, 'L')\n        \n        if \"Accurate\" in status:\n            self.set_text_color(34, 197, 94)\n        self.cell(col_widths[1], 8, status, 1, 0, 'L')\n        self.set_text_color(30, 30, 30)\n        \n        self.cell(col_widths[2], 8, notes, 1, 1, 'L')\n\ndef generate_pdf():\n    pdf = SanityCheckPDF()\n    pdf.set_auto_page_break(auto=True, margin=20)\n    pdf.add_page()\n    \n    pdf.set_font('Helvetica', 'B', 20)\n    pdf.set_text_color(0, 0, 0)\n    pdf.cell(0, 15, 'Section 8 Response', 0, 1, 'C')\n    pdf.set_font('Helvetica', 'I', 12)\n    pdf.set_text_color(100, 100, 100)\n    pdf.cell(0, 8, \"Melissa's Sanity Check of the HossAgent Alignment Doc\", 0, 1, 'C')\n    pdf.set_font('Helvetica', '', 10)\n    pdf.cell(0, 8, f\"Date: {datetime.now().strftime('%B %d, %Y')}\", 0, 1, 'C')\n    pdf.ln(10)\n    \n    pdf.section_title('1. Architecture Accuracy')\n    pdf.body_text('Mostly correct. A few clarifications:')\n    pdf.ln(3)\n    \n    pdf.table_row('Component', 'Your Description', 'Actual State', header=True)\n    pdf.status_row('SignalNet', 'Accurate', 'News + Reddit sources, weather disabled')\n    pdf.status_row('LeadEngine', 'Accurate', 'Signals to LeadEvents, threshold 60+')\n    pdf.status_row('ARCHANGEL', 'Accurate', 'NAMESTORM + DOMAINSTORM + PHONESTORM')\n    pdf.status_row('Outbound', 'Accurate', 'SendGrid via hossagent.net')\n    pdf.status_row('Portal/Admin', 'Accurate', 'Portal: OUTBOUND_SENT, Admin: all')\n    \n    pdf.ln(5)\n    pdf.set_font('Helvetica', 'B', 10)\n    pdf.cell(0, 6, 'Missing from doc:', 0, 1)\n    pdf.body_text('The enrichment_attempts counter exists on LeadEvent but is not yet wired into a formal max_attempts budget system.')\n    \n    pdf.section_title('2. Enrichment Behavior')\n    pdf.set_font('Helvetica', 'B', 10)\n    pdf.cell(0, 6, 'Partially updated since this doc was written:', 0, 1)\n    pdf.ln(2)\n    \n    pdf.bullet_point('NAMESTORM: Now implemented with 5 extraction layers (schema.org, NER patterns, meta tags, heuristics, article body)')\n    pdf.bullet_point('DOMAINSTORM: Now has DuckDuckGo HTML search as Layer 4 with backoff/throttle protection')\n    pdf.bullet_point('PHONESTORM LITE: Integrated - runs after domain discovery, extracts/classifies phone types')\n    \n    pdf.ln(3)\n    pdf.set_font('Helvetica', 'B', 10)\n    pdf.cell(0, 6, \"What's still missing (your doc nails this):\", 0, 1)\n    pdf.ln(2)\n    \n    pdf.bullet_point('True multi-pass recursion (retry failed leads with new strategies)')\n    pdf.bullet_point('Formal budget system (max_attempts -> ARCHIVED_UNENRICHABLE)')\n    pdf.bullet_point('\"Mission log\" tracking URLs/queries already tried')\n    \n    pdf.section_title('3. Status Lifecycle')\n    pdf.body_text('These are the current states:')\n    pdf.ln(2)\n    \n    pdf.set_font('Courier', '', 9)\n    pdf.set_x(15)\n    pdf.multi_cell(0, 5, 'UNENRICHED -> WITH_DOMAIN_NO_EMAIL -> ENRICHED_NO_OUTBOUND -> OUTBOUND_SENT\\n                                                           \\\\-> ARCHIVED (stale)')\n    pdf.ln(3)\n    \n    pdf.set_font('Helvetica', 'B', 10)\n    pdf.cell(0, 6, 'Missing state you identified:', 0, 1)\n    pdf.body_text('ARCHIVED_UNENRICHABLE with explicit reason - this does not exist yet. Currently stale leads get archived but without \"we exhausted all options\" reasoning.')\n    \n    pdf.section_title('4. Metrics / Current Numbers')\n    pdf.set_font('Helvetica', 'B', 10)\n    pdf.cell(0, 6, 'Directionally correct:', 0, 1)\n    pdf.ln(2)\n    \n    pdf.bullet_point('4,400+ signals over ~48 hours - CONFIRMED')\n    pdf.bullet_point('300+ LeadEvents - CONFIRMED')\n    pdf.bullet_point('~6 successfully enriched/emailed - CONFIRMED')\n    pdf.bullet_point('1-2% enrichment rate - CONFIRMED')\n    \n    pdf.section_title('5. Constraints')\n    pdf.body_text('Key constraints to respect:')\n    pdf.ln(3)\n    \n    pdf.table_row('Constraint', 'Current Handling', '', header=True)\n    pdf.set_font('Helvetica', '', 9)\n    col_widths = [60, 130]\n    \n    constraints = [\n        ('DuckDuckGo throttling', 'Backoff after 3 failures, 5-min cooldown'),\n        ('Article fetch timeout', '6 second limit with TTL caching (1 hour)'),\n        ('Rate limiting', '0.5s delay between enrichment attempts'),\n        ('News aggregator URLs', 'news.google.com blocked from article body extraction'),\n    ]\n    \n    for constraint, handling in constraints:\n        pdf.cell(col_widths[0], 8, constraint, 1, 0, 'L')\n        pdf.cell(col_widths[1], 8, handling, 1, 1, 'L')\n    \n    pdf.add_page()\n    \n    pdf.section_title('6. Tactical Pause Recommendation (Section 7.2)')\n    \n    pdf.set_font('Helvetica', 'B', 11)\n    pdf.set_text_color(34, 197, 94)\n    pdf.cell(0, 8, 'I agree with the 24-48 hour observation window.', 0, 1)\n    pdf.set_text_color(30, 30, 30)\n    pdf.ln(3)\n    \n    pdf.body_text('The triple-stack is deployed. Before we add:')\n    pdf.bullet_point('Multi-pass recursion')\n    pdf.bullet_point('Budget system')\n    pdf.bullet_point('Reddit/Craigslist sources')\n    pdf.bullet_point('Learning/meta-knowledge')\n    \n    pdf.ln(3)\n    pdf.body_text('We need to know: Did NAMESTORM + DOMAINSTORM + PHONESTORM actually move the needle from 2% to 5%+?')\n    \n    pdf.ln(5)\n    pdf.set_font('Helvetica', 'B', 11)\n    pdf.cell(0, 8, 'Decision Framework:', 0, 1)\n    pdf.ln(2)\n    \n    pdf.table_row('Outcome', 'What It Means', '', header=True)\n    outcomes = [\n        ('2% -> 5%', 'Triple-stack working, hitting news signal ceiling'),\n        ('2% -> 8-10%', 'Home run - extraction layers adding real value'),\n        ('Still ~2%', 'Problem is source data, not extraction'),\n    ]\n    \n    for outcome, meaning in outcomes:\n        pdf.set_font('Helvetica', '', 9)\n        pdf.cell(60, 8, outcome, 1, 0, 'L')\n        pdf.cell(130, 8, meaning, 1, 1, 'L')\n    \n    pdf.ln(8)\n    pdf.set_font('Helvetica', 'B', 11)\n    pdf.cell(0, 8, 'Next Steps Based on Observation:', 0, 1)\n    pdf.ln(2)\n    \n    pdf.body_text('If enrichment improves to 5%+:')\n    pdf.bullet_point('The bottleneck is signal source quality')\n    pdf.bullet_point('Expand SignalStorm (Reddit, Craigslist, job boards)')\n    \n    pdf.ln(3)\n    pdf.body_text('If enrichment stays at ~2%:')\n    pdf.bullet_point('The extraction layers need more work')\n    pdf.bullet_point('Focus on improving NAMESTORM/DOMAINSTORM before adding complexity')\n    \n    pdf.ln(10)\n    pdf.set_draw_color(200, 200, 200)\n    pdf.line(10, pdf.get_y(), 200, pdf.get_y())\n    pdf.ln(5)\n    \n    pdf.set_font('Helvetica', 'I', 9)\n    pdf.set_text_color(100, 100, 100)\n    pdf.multi_cell(0, 5, 'Document prepared by Melissa (Replit Agent) in response to Section 8 of the HossAgent Vision, Current State, and Enrichment Problem Statement alignment document.')\n    \n    output_path = 'attached_assets/HossAgent_Sanity_Check_Response.pdf'\n    pdf.output(output_path)\n    print(f\"PDF generated: {output_path}\")\n    return output_path\n\nif __name__ == \"__main__\":\n    generate_pdf()\n","path":null,"size_bytes":9241,"size_tokens":null},"email_storm.py":{"content":"\"\"\"\nOPERATION EMAILSTORM v1: Layered Email Discovery Engine\n\nMulti-layered email discovery with confidence scoring:\n1. Direct extraction from website contact pages\n2. Pattern-based email generation from domain\n3. Email format guessing with validation\n4. Social profile email extraction\n\nPure web scraping - NO paid APIs (NO Hunter.io, NO Clearbit for discovery).\nSMTP validation optional for high-confidence scoring.\n\"\"\"\n\nimport os\nimport re\nimport socket\nimport smtplib\nimport time\nfrom dataclasses import dataclass, field\nfrom typing import List, Optional, Dict, Tuple\nfrom urllib.parse import urlparse, urljoin\n\nimport requests\nfrom requests.exceptions import RequestException, Timeout\n\n\nEMAILSTORM_TIMEOUT = int(os.getenv(\"EMAILSTORM_TIMEOUT\", \"8\"))\nENABLE_SMTP_VALIDATION = os.getenv(\"ENABLE_SMTP_VALIDATION\", \"false\").lower() in (\"true\", \"1\", \"yes\")\n\nUSER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n\nCONTACT_PATHS = [\n    \"/contact\", \"/contact-us\", \"/contact_us\", \"/contactus\",\n    \"/about\", \"/about-us\", \"/about_us\", \"/aboutus\",\n    \"/team\", \"/our-team\", \"/meet-the-team\",\n    \"/connect\", \"/get-in-touch\", \"/reach-us\",\n    \"/support\", \"/help\", \"/inquiry\",\n]\n\nPERSON_LIKE_PREFIXES = [\n    \"john\", \"jane\", \"mike\", \"michael\", \"david\", \"james\", \"robert\", \"chris\",\n    \"sarah\", \"lisa\", \"jennifer\", \"maria\", \"jose\", \"carlos\", \"pedro\",\n    \"owner\", \"president\", \"ceo\", \"manager\", \"director\", \"admin\",\n]\n\nGENERIC_PREFIXES = [\n    \"info\", \"contact\", \"hello\", \"hi\", \"hey\", \"mail\",\n    \"sales\", \"support\", \"help\", \"service\", \"services\",\n    \"team\", \"office\", \"admin\", \"webmaster\",\n    \"inquiries\", \"enquiries\", \"questions\",\n]\n\nBLOCKED_EMAIL_PATTERNS = [\n    r\"example\\.com\", r\"domain\\.com\", r\"email\\.com\", r\"yoursite\\.com\",\n    r\"placeholder\", r\"test@\", r\"noreply\", r\"no-reply\",\n    r\"\\.png\", r\"\\.jpg\", r\"\\.gif\", r\"\\.svg\", r\"\\.webp\",\n    r\"wixpress\\.com\", r\"sentry\\.io\", r\"cloudflare\",\n    r\"google\\.com\", r\"facebook\\.com\", r\"twitter\\.com\",\n    r\"schema\\.org\", r\"w3\\.org\",\n]\n\nEMAIL_REGEX = re.compile(\n    r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\",\n    re.IGNORECASE\n)\n\nMAILTO_REGEX = re.compile(r'href=[\"\\']mailto:([^\"\\'?]+)', re.IGNORECASE)\n\n\n@dataclass\nclass EmailCandidate:\n    \"\"\"A discovered email with confidence scoring.\"\"\"\n    email: str\n    confidence: float\n    source: str\n    email_type: str\n    validation_status: str = \"unknown\"\n    \n    def to_dict(self) -> Dict:\n        return {\n            \"email\": self.email,\n            \"confidence\": self.confidence,\n            \"source\": self.source,\n            \"email_type\": self.email_type,\n            \"validation_status\": self.validation_status\n        }\n\n\n@dataclass\nclass EmailStormResult:\n    \"\"\"Result of email discovery.\"\"\"\n    success: bool\n    best_email: Optional[EmailCandidate] = None\n    all_emails: List[EmailCandidate] = field(default_factory=list)\n    domain: Optional[str] = None\n    extraction_time_ms: int = 0\n    error: Optional[str] = None\n    pages_checked: int = 0\n    \n    def to_dict(self) -> Dict:\n        return {\n            \"success\": self.success,\n            \"best_email\": self.best_email.to_dict() if self.best_email else None,\n            \"all_emails\": [e.to_dict() for e in self.all_emails],\n            \"domain\": self.domain,\n            \"extraction_time_ms\": self.extraction_time_ms,\n            \"error\": self.error,\n            \"pages_checked\": self.pages_checked\n        }\n\n\ndef log_emailstorm(action: str, domain: Optional[str] = None, details: Optional[Dict] = None) -> None:\n    \"\"\"Log EMAILSTORM activity.\"\"\"\n    msg_parts = [f\"[EMAILSTORM][{action.upper()}]\"]\n    if domain:\n        msg_parts.append(f\"domain={domain}\")\n    if details:\n        for k, v in details.items():\n            if isinstance(v, str) and len(v) > 50:\n                v = v[:50] + \"...\"\n            msg_parts.append(f\"{k}={v}\")\n    print(\" | \".join(msg_parts))\n\n\ndef _is_blocked_email(email: str) -> bool:\n    \"\"\"Check if email matches a blocked pattern.\"\"\"\n    email_lower = email.lower()\n    for pattern in BLOCKED_EMAIL_PATTERNS:\n        if re.search(pattern, email_lower):\n            return True\n    return False\n\n\ndef _classify_email_type(email: str) -> str:\n    \"\"\"Classify email as person-like or generic.\"\"\"\n    local_part = email.split(\"@\")[0].lower()\n    \n    for prefix in PERSON_LIKE_PREFIXES:\n        if local_part.startswith(prefix) or prefix in local_part:\n            return \"person_like\"\n    \n    for prefix in GENERIC_PREFIXES:\n        if local_part.startswith(prefix):\n            return \"generic\"\n    \n    if \".\" in local_part or \"_\" in local_part:\n        return \"person_like\"\n    \n    return \"unknown\"\n\n\ndef _calculate_email_confidence(email: str, source: str, domain: str) -> float:\n    \"\"\"Calculate confidence score for an email.\"\"\"\n    score = 0.5\n    \n    email_domain = email.split(\"@\")[1].lower() if \"@\" in email else \"\"\n    domain_root = domain.replace(\"www.\", \"\").lower()\n    \n    if email_domain == domain_root or domain_root in email_domain:\n        score += 0.25\n    \n    source_weights = {\n        \"mailto_link\": 0.15,\n        \"contact_page\": 0.10,\n        \"about_page\": 0.08,\n        \"homepage\": 0.05,\n        \"schema_org\": 0.12,\n        \"meta_tag\": 0.08,\n        \"guessed\": -0.15,\n    }\n    score += source_weights.get(source, 0)\n    \n    email_type = _classify_email_type(email)\n    if email_type == \"person_like\":\n        score += 0.10\n    elif email_type == \"generic\":\n        score += 0.05\n    \n    score = max(0.1, min(1.0, score))\n    \n    return score\n\n\ndef _fetch_page(url: str) -> Optional[str]:\n    \"\"\"Fetch a page with error handling.\"\"\"\n    try:\n        headers = {\"User-Agent\": USER_AGENT}\n        response = requests.get(url, headers=headers, timeout=EMAILSTORM_TIMEOUT, allow_redirects=True)\n        \n        if response.status_code == 200:\n            return response.text[:100000]\n        \n    except (RequestException, Timeout):\n        pass\n    \n    return None\n\n\ndef _extract_emails_from_html(html: str) -> List[str]:\n    \"\"\"Extract all email addresses from HTML content.\"\"\"\n    emails = []\n    \n    mailto_matches = MAILTO_REGEX.findall(html)\n    for email in mailto_matches:\n        email = email.strip().lower()\n        if \"@\" in email and not _is_blocked_email(email):\n            emails.append(email)\n    \n    general_matches = EMAIL_REGEX.findall(html)\n    for email in general_matches:\n        email = email.strip().lower()\n        if not _is_blocked_email(email) and email not in emails:\n            emails.append(email)\n    \n    return list(dict.fromkeys(emails))\n\n\ndef _extract_from_schema_org(html: str, domain: str) -> List[EmailCandidate]:\n    \"\"\"Extract emails from Schema.org JSON-LD data.\"\"\"\n    candidates = []\n    \n    try:\n        import json\n        \n        ld_pattern = re.compile(r'<script[^>]*type=[\"\\']application/ld\\+json[\"\\'][^>]*>(.*?)</script>', re.DOTALL | re.IGNORECASE)\n        \n        for match in ld_pattern.finditer(html):\n            try:\n                data = json.loads(match.group(1))\n                \n                if isinstance(data, list):\n                    data = data[0] if data else {}\n                \n                email = data.get(\"email\")\n                if email:\n                    email = email.replace(\"mailto:\", \"\").strip().lower()\n                    if \"@\" in email and not _is_blocked_email(email):\n                        candidates.append(EmailCandidate(\n                            email=email,\n                            confidence=_calculate_email_confidence(email, \"schema_org\", domain),\n                            source=\"schema_org\",\n                            email_type=_classify_email_type(email)\n                        ))\n                \n                contact = data.get(\"contactPoint\") or data.get(\"contact\")\n                if isinstance(contact, dict):\n                    email = contact.get(\"email\")\n                    if email:\n                        email = email.replace(\"mailto:\", \"\").strip().lower()\n                        if \"@\" in email and not _is_blocked_email(email):\n                            candidates.append(EmailCandidate(\n                                email=email,\n                                confidence=_calculate_email_confidence(email, \"schema_org\", domain),\n                                source=\"schema_org\",\n                                email_type=_classify_email_type(email)\n                            ))\n            except (json.JSONDecodeError, TypeError, KeyError):\n                continue\n                \n    except Exception:\n        pass\n    \n    return candidates\n\n\ndef _guess_common_emails(domain: str) -> List[EmailCandidate]:\n    \"\"\"Generate common email patterns for the domain.\"\"\"\n    candidates = []\n    domain_clean = domain.replace(\"www.\", \"\")\n    \n    common_patterns = [\n        \"info\", \"contact\", \"hello\", \"sales\", \"support\",\n        \"office\", \"team\", \"admin\", \"mail\"\n    ]\n    \n    for prefix in common_patterns:\n        email = f\"{prefix}@{domain_clean}\"\n        candidates.append(EmailCandidate(\n            email=email,\n            confidence=_calculate_email_confidence(email, \"guessed\", domain),\n            source=\"guessed\",\n            email_type=\"generic\"\n        ))\n    \n    return candidates\n\n\ndef validate_email_smtp(email: str) -> str:\n    \"\"\"\n    Validate email via SMTP (lightweight check).\n    \n    Returns: \"valid\", \"invalid\", \"unknown\", or \"error\"\n    \"\"\"\n    if not ENABLE_SMTP_VALIDATION:\n        return \"not_checked\"\n    \n    try:\n        domain = email.split(\"@\")[1]\n        \n        records = socket.getaddrinfo(domain, 25, socket.AF_INET, socket.SOCK_STREAM)\n        if not records:\n            return \"no_mx\"\n        \n        return \"mx_found\"\n        \n    except socket.gaierror:\n        return \"no_dns\"\n    except Exception:\n        return \"error\"\n\n\ndef discover_emails(\n    domain: str,\n    company_name: Optional[str] = None,\n    check_contact_pages: bool = True,\n    generate_guesses: bool = True\n) -> EmailStormResult:\n    \"\"\"\n    EMAILSTORM: Discover emails for a domain using multiple methods.\n    \n    Layered approach:\n    1. Scrape contact/about pages for mailto: links and email patterns\n    2. Extract from Schema.org structured data\n    3. Generate common email patterns (info@, contact@, etc.)\n    4. Optional SMTP validation for confidence boost\n    \n    Args:\n        domain: Target domain\n        company_name: Optional company name for context\n        check_contact_pages: Whether to scrape contact pages\n        generate_guesses: Whether to generate common email guesses\n        \n    Returns:\n        EmailStormResult with discovered emails sorted by confidence\n    \"\"\"\n    start_time = time.time()\n    all_candidates: List[EmailCandidate] = []\n    pages_checked = 0\n    \n    domain_clean = domain.replace(\"www.\", \"\").lower()\n    base_url = f\"https://{domain_clean}\"\n    \n    log_emailstorm(\"START\", domain_clean, {\"company\": company_name})\n    \n    if check_contact_pages:\n        homepage_html = _fetch_page(base_url)\n        if homepage_html:\n            pages_checked += 1\n            \n            schema_emails = _extract_from_schema_org(homepage_html, domain_clean)\n            all_candidates.extend(schema_emails)\n            \n            homepage_emails = _extract_emails_from_html(homepage_html)\n            for email in homepage_emails:\n                if not any(c.email == email for c in all_candidates):\n                    all_candidates.append(EmailCandidate(\n                        email=email,\n                        confidence=_calculate_email_confidence(email, \"homepage\", domain_clean),\n                        source=\"homepage\",\n                        email_type=_classify_email_type(email)\n                    ))\n        \n        for path in CONTACT_PATHS[:8]:\n            page_url = urljoin(base_url, path)\n            html = _fetch_page(page_url)\n            \n            if html:\n                pages_checked += 1\n                \n                source_type = \"contact_page\" if \"contact\" in path else \"about_page\"\n                page_emails = _extract_emails_from_html(html)\n                \n                for email in page_emails:\n                    if not any(c.email == email for c in all_candidates):\n                        all_candidates.append(EmailCandidate(\n                            email=email,\n                            confidence=_calculate_email_confidence(email, source_type, domain_clean),\n                            source=source_type,\n                            email_type=_classify_email_type(email)\n                        ))\n                \n                schema_emails = _extract_from_schema_org(html, domain_clean)\n                for candidate in schema_emails:\n                    if not any(c.email == candidate.email for c in all_candidates):\n                        all_candidates.append(candidate)\n            \n            time.sleep(0.2)\n            \n            if len(all_candidates) >= 5:\n                break\n    \n    if generate_guesses and len(all_candidates) < 3:\n        guessed = _guess_common_emails(domain_clean)\n        all_candidates.extend(guessed[:5])\n    \n    domain_emails = [c for c in all_candidates if domain_clean in c.email.split(\"@\")[1]]\n    other_emails = [c for c in all_candidates if domain_clean not in c.email.split(\"@\")[1]]\n    \n    domain_emails.sort(key=lambda c: c.confidence, reverse=True)\n    other_emails.sort(key=lambda c: c.confidence, reverse=True)\n    \n    sorted_candidates = domain_emails + other_emails\n    \n    if ENABLE_SMTP_VALIDATION and sorted_candidates:\n        for candidate in sorted_candidates[:3]:\n            validation_status = validate_email_smtp(candidate.email)\n            candidate.validation_status = validation_status\n            \n            if validation_status == \"mx_found\":\n                candidate.confidence = min(1.0, candidate.confidence + 0.1)\n            elif validation_status in (\"no_mx\", \"no_dns\"):\n                candidate.confidence = max(0.1, candidate.confidence - 0.2)\n        \n        sorted_candidates.sort(key=lambda c: c.confidence, reverse=True)\n    \n    elapsed_ms = int((time.time() - start_time) * 1000)\n    \n    if sorted_candidates:\n        best = sorted_candidates[0]\n        log_emailstorm(\"SUCCESS\", domain_clean, {\n            \"count\": len(sorted_candidates),\n            \"best\": best.email,\n            \"confidence\": f\"{best.confidence:.2f}\",\n            \"source\": best.source,\n            \"pages\": pages_checked\n        })\n        \n        return EmailStormResult(\n            success=True,\n            best_email=best,\n            all_emails=sorted_candidates[:10],\n            domain=domain_clean,\n            extraction_time_ms=elapsed_ms,\n            pages_checked=pages_checked\n        )\n    else:\n        log_emailstorm(\"NO_EMAILS\", domain_clean, {\"pages\": pages_checked})\n        \n        return EmailStormResult(\n            success=False,\n            domain=domain_clean,\n            extraction_time_ms=elapsed_ms,\n            error=\"No emails discovered\",\n            pages_checked=pages_checked\n        )\n\n\ndef get_best_email(\n    domain: str,\n    company_name: Optional[str] = None,\n    min_confidence: float = 0.4\n) -> Optional[str]:\n    \"\"\"\n    Convenience function to get the best email for a domain.\n    \n    Returns the highest-confidence email if it meets the threshold.\n    \"\"\"\n    result = discover_emails(domain, company_name)\n    \n    if result.success and result.best_email:\n        if result.best_email.confidence >= min_confidence:\n            return result.best_email.email\n    \n    return None\n","path":null,"size_bytes":15602,"size_tokens":null},"sec_edgar_connector.py":{"content":"\"\"\"\nSEC EDGAR Connector for HossAgent MacroStorm - EPIC 5\n\nPolls SEC EDGAR for public filings (10-K, 10-Q, 8-K) and extracts\nstrategic intelligence about big-company moves.\n\nThese filings reveal:\n- Expansion plans (new locations, markets)\n- Contraction signals (layoffs, closures)\n- Supply chain changes\n- Geographic focus\n- Risk factors\n\nEDGAR API: https://www.sec.gov/cgi-bin/browse-edgar\nRSS Feed: https://www.sec.gov/cgi-bin/browse-edgar?action=getcurrent&type=10-K&output=atom\n\nPure web scraping - NO paid APIs.\n\"\"\"\n\nimport json\nimport os\nimport re\nimport time\nimport hashlib\nimport random\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional, Tuple\nfrom urllib.parse import urljoin, quote_plus\nimport xml.etree.ElementTree as ET\n\nimport requests\nfrom sqlmodel import Session, select\n\nfrom models import (\n    MacroEvent,\n    MACRO_SOURCE_SEC_10K,\n    MACRO_SOURCE_SEC_10Q,\n    MACRO_SOURCE_SEC_8K,\n    MACRO_FORCE_TYPE_EXPANSION,\n    MACRO_FORCE_TYPE_CONTRACTION,\n    MACRO_FORCE_TYPE_RESTRUCTURING,\n    MACRO_FORCE_TYPE_MERGER,\n    MACRO_FORCE_TYPE_BANKRUPTCY,\n    MACRO_FORCE_TYPE_SUPPLY_CHAIN,\n    MACRO_FORCE_TYPE_REGULATORY,\n)\n\n\nSEC_BASE_URL = \"https://www.sec.gov\"\nEDGAR_COMPANY_SEARCH = \"https://www.sec.gov/cgi-bin/browse-edgar\"\nEDGAR_FULL_TEXT_SEARCH = \"https://efts.sec.gov/LATEST/search-index\"\n\nFILING_TYPES = {\n    \"10-K\": MACRO_SOURCE_SEC_10K,\n    \"10-Q\": MACRO_SOURCE_SEC_10Q,\n    \"8-K\": MACRO_SOURCE_SEC_8K,\n}\n\nTRACKED_TICKERS = [\n    \"MCD\",\n    \"WMT\",\n    \"HD\",\n    \"LOW\",\n    \"TGT\",\n    \"COST\",\n    \"DG\",\n    \"DLTR\",\n    \"KR\",\n    \"SBUX\",\n    \"CMG\",\n    \"DPZ\",\n    \"YUM\",\n    \"QSR\",\n    \"DRI\",\n    \"DENN\",\n    \"WEN\",\n    \"SHAK\",\n    \"CAVA\",\n    \"WING\",\n]\n\nFLORIDA_KEYWORDS = [\n    \"florida\",\n    \"miami\",\n    \"fort lauderdale\",\n    \"broward\",\n    \"palm beach\",\n    \"orlando\",\n    \"tampa\",\n    \"jacksonville\",\n    \"southeast\",\n    \"sunbelt\",\n]\n\nEXPANSION_KEYWORDS = [\n    \"new store\",\n    \"new location\",\n    \"new unit\",\n    \"expansion\",\n    \"expand\",\n    \"opening\",\n    \"open\",\n    \"growth\",\n    \"develop\",\n    \"construction\",\n    \"build\",\n    \"capital expenditure\",\n    \"capex\",\n    \"investment\",\n    \"increase capacity\",\n    \"add capacity\",\n    \"new market\",\n    \"enter\",\n    \"launch\",\n]\n\nCONTRACTION_KEYWORDS = [\n    \"close\",\n    \"closure\",\n    \"closing\",\n    \"restructur\",\n    \"layoff\",\n    \"workforce reduction\",\n    \"downsize\",\n    \"consolidat\",\n    \"exit\",\n    \"discontinue\",\n    \"impairment\",\n    \"write-off\",\n    \"write-down\",\n    \"decline\",\n    \"decrease\",\n    \"reduce\",\n    \"cut\",\n]\n\nSUPPLY_CHAIN_KEYWORDS = [\n    \"supply chain\",\n    \"supplier\",\n    \"vendor\",\n    \"distribution\",\n    \"logistics\",\n    \"warehouse\",\n    \"inventory\",\n    \"procurement\",\n    \"sourcing\",\n    \"manufacturing\",\n]\n\nUSER_AGENT = \"HossAgent/1.0 (contact@hossagent.net)\"\n\nREQUEST_TIMEOUT = 30\nMIN_DELAY = 0.5\nMAX_DELAY = 1.0\nMAX_RETRIES = 2\nCACHE_TTL = 86400\n\n_request_cache: Dict[str, Tuple[str, float]] = {}\n\n\n@dataclass\nclass SECFiling:\n    \"\"\"A single SEC filing.\"\"\"\n    cik: str\n    company_name: str\n    ticker: Optional[str]\n    filing_type: str\n    filed_date: str\n    accession_number: str\n    url: str\n    description: Optional[str] = None\n    raw_content: Optional[str] = None\n\n\n@dataclass\nclass ExtractedIntelligence:\n    \"\"\"Extracted intelligence from a filing.\"\"\"\n    force_type: str\n    headline: str\n    geographies: List[str]\n    segments_affected: List[str]\n    time_horizon: Optional[str]\n    raw_snippet: str\n    confidence: float\n\n\ndef _get_cached_response(url: str) -> Optional[str]:\n    \"\"\"Get cached response if still valid.\"\"\"\n    if url in _request_cache:\n        content, timestamp = _request_cache[url]\n        if time.time() - timestamp < CACHE_TTL:\n            return content\n    return None\n\n\ndef _cache_response(url: str, content: str) -> None:\n    \"\"\"Cache response content.\"\"\"\n    _request_cache[url] = (content, time.time())\n\n\ndef _fetch_sec_page(url: str, retries: int = MAX_RETRIES) -> Optional[str]:\n    \"\"\"Fetch a page from SEC with proper rate limiting.\"\"\"\n    cached = _get_cached_response(url)\n    if cached:\n        print(f\"[SEC_EDGAR][CACHE_HIT] {url[:60]}\")\n        return cached\n    \n    delay = random.uniform(MIN_DELAY, MAX_DELAY)\n    time.sleep(delay)\n    \n    headers = {\n        \"User-Agent\": USER_AGENT,\n        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n        \"Accept-Language\": \"en-US,en;q=0.5\",\n    }\n    \n    for attempt in range(retries):\n        try:\n            response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)\n            \n            if response.status_code == 200:\n                _cache_response(url, response.text)\n                return response.text\n            elif response.status_code == 429:\n                print(f\"[SEC_EDGAR][RATE_LIMITED] Waiting 60s...\")\n                time.sleep(60)\n                continue\n            else:\n                print(f\"[SEC_EDGAR][HTTP_{response.status_code}] {url[:60]}\")\n                if attempt < retries - 1:\n                    time.sleep(delay * (attempt + 1))\n                    continue\n                    \n        except requests.exceptions.RequestException as e:\n            print(f\"[SEC_EDGAR][ERROR] {url[:60]}: {str(e)[:50]}\")\n            if attempt < retries - 1:\n                time.sleep(delay * (attempt + 1))\n                continue\n    \n    return None\n\n\ndef _parse_rss_feed(xml_content: str) -> List[Dict]:\n    \"\"\"Parse SEC RSS feed for filings.\"\"\"\n    entries = []\n    \n    try:\n        root = ET.fromstring(xml_content)\n        \n        ns = {\n            \"atom\": \"http://www.w3.org/2005/Atom\",\n            \"edgar\": \"https://www.sec.gov/cgi-bin/browse-edgar\"\n        }\n        \n        for entry in root.findall(\".//atom:entry\", ns):\n            title_elem = entry.find(\"atom:title\", ns)\n            link_elem = entry.find(\"atom:link\", ns)\n            updated_elem = entry.find(\"atom:updated\", ns)\n            summary_elem = entry.find(\"atom:summary\", ns)\n            \n            if title_elem is not None and link_elem is not None:\n                entries.append({\n                    \"title\": title_elem.text or \"\",\n                    \"link\": link_elem.get(\"href\", \"\"),\n                    \"updated\": updated_elem.text if updated_elem is not None else \"\",\n                    \"summary\": summary_elem.text if summary_elem is not None else \"\",\n                })\n    \n    except ET.ParseError as e:\n        print(f\"[SEC_EDGAR][XML_ERROR] Failed to parse RSS: {str(e)[:50]}\")\n    \n    return entries\n\n\ndef _extract_company_info(title: str) -> Tuple[Optional[str], Optional[str], Optional[str]]:\n    \"\"\"Extract company name, CIK, and ticker from filing title.\"\"\"\n    cik_match = re.search(r'\\((\\d{10})\\)', title)\n    cik = cik_match.group(1) if cik_match else None\n    \n    company_match = re.search(r'^([^(]+)', title)\n    company_name = company_match.group(1).strip() if company_match else None\n    \n    ticker_match = re.search(r'\\(([A-Z]{1,5})\\)', title)\n    ticker = ticker_match.group(1) if ticker_match else None\n    \n    return company_name, cik, ticker\n\n\ndef _detect_force_type(text: str) -> Tuple[str, float]:\n    \"\"\"Detect the force type from filing text.\"\"\"\n    text_lower = text.lower()\n    \n    scores = {\n        MACRO_FORCE_TYPE_EXPANSION: 0,\n        MACRO_FORCE_TYPE_CONTRACTION: 0,\n        MACRO_FORCE_TYPE_SUPPLY_CHAIN: 0,\n        MACRO_FORCE_TYPE_RESTRUCTURING: 0,\n    }\n    \n    for keyword in EXPANSION_KEYWORDS:\n        if keyword in text_lower:\n            scores[MACRO_FORCE_TYPE_EXPANSION] += 1\n    \n    for keyword in CONTRACTION_KEYWORDS:\n        if keyword in text_lower:\n            scores[MACRO_FORCE_TYPE_CONTRACTION] += 1\n    \n    for keyword in SUPPLY_CHAIN_KEYWORDS:\n        if keyword in text_lower:\n            scores[MACRO_FORCE_TYPE_SUPPLY_CHAIN] += 1\n    \n    if \"restructur\" in text_lower or \"reorganiz\" in text_lower:\n        scores[MACRO_FORCE_TYPE_RESTRUCTURING] += 2\n    \n    max_type = max(scores, key=scores.get)\n    max_score = scores[max_type]\n    \n    if max_score == 0:\n        return MACRO_FORCE_TYPE_EXPANSION, 0.3\n    \n    total = sum(scores.values())\n    confidence = max_score / total if total > 0 else 0.3\n    \n    return max_type, min(0.95, confidence)\n\n\ndef _detect_geographies(text: str) -> List[str]:\n    \"\"\"Detect Florida/South Florida geographies in text.\"\"\"\n    text_lower = text.lower()\n    found = []\n    \n    for keyword in FLORIDA_KEYWORDS:\n        if keyword in text_lower:\n            if keyword == \"florida\":\n                found.append(\"Florida\")\n            elif keyword == \"miami\":\n                found.append(\"Miami\")\n            elif keyword == \"fort lauderdale\":\n                found.append(\"Fort Lauderdale\")\n            elif keyword == \"broward\":\n                found.append(\"Broward County\")\n            elif keyword == \"palm beach\":\n                found.append(\"Palm Beach County\")\n            elif keyword == \"southeast\" or keyword == \"sunbelt\":\n                found.append(\"Southeast US\")\n    \n    return list(set(found)) if found else [\"National\"]\n\n\ndef _extract_segments(text: str, force_type: str) -> List[str]:\n    \"\"\"Extract affected business segments from text.\"\"\"\n    segments = []\n    text_lower = text.lower()\n    \n    segment_keywords = {\n        \"restaurant\": \"QSR\",\n        \"food service\": \"Food Service\",\n        \"retail\": \"Retail\",\n        \"store\": \"Retail\",\n        \"distribution\": \"Distribution\",\n        \"logistics\": \"Logistics\",\n        \"warehouse\": \"Warehousing\",\n        \"real estate\": \"Real Estate\",\n        \"construction\": \"Construction\",\n        \"staffing\": \"Staffing\",\n        \"labor\": \"Labor\",\n    }\n    \n    for keyword, segment in segment_keywords.items():\n        if keyword in text_lower and segment not in segments:\n            segments.append(segment)\n    \n    return segments if segments else [\"General\"]\n\n\ndef _extract_time_horizon(text: str) -> Optional[str]:\n    \"\"\"Extract time horizon from filing text.\"\"\"\n    patterns = [\n        (r'over the next (\\d+)\\s*(?:to\\s*\\d+)?\\s*years?', lambda m: f\"{m.group(1)}-year\"),\n        (r'(\\d+)\\s*(?:to\\s*\\d+)?\\s*year', lambda m: f\"{m.group(1)}-year\"),\n        (r'fiscal (\\d{4})', lambda m: f\"FY{m.group(1)}\"),\n        (r'next\\s*(\\d+)\\s*months?', lambda m: f\"{m.group(1)}-months\"),\n        (r'(\\d{4})', lambda m: f\"By {m.group(1)}\"),\n    ]\n    \n    for pattern, formatter in patterns:\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            return formatter(match)\n    \n    return None\n\n\ndef _generate_headline(filing: SECFiling, force_type: str, snippet: str) -> str:\n    \"\"\"Generate a human-readable headline from filing intelligence.\"\"\"\n    action_words = {\n        MACRO_FORCE_TYPE_EXPANSION: \"expanding\",\n        MACRO_FORCE_TYPE_CONTRACTION: \"contracting operations\",\n        MACRO_FORCE_TYPE_SUPPLY_CHAIN: \"changing supply chain\",\n        MACRO_FORCE_TYPE_RESTRUCTURING: \"restructuring\",\n        MACRO_FORCE_TYPE_MERGER: \"merger activity\",\n        MACRO_FORCE_TYPE_BANKRUPTCY: \"bankruptcy proceedings\",\n    }\n    \n    action = action_words.get(force_type, \"strategic move\")\n    \n    numbers = re.findall(r'\\b(\\d+)\\s*(?:new\\s+)?(?:store|location|unit|restaurant)', snippet, re.IGNORECASE)\n    if numbers:\n        return f\"{filing.company_name} plans {numbers[0]} new locations ({filing.filing_type})\"\n    \n    return f\"{filing.company_name} {action} per {filing.filing_type} filing\"\n\n\ndef fetch_recent_filings(\n    filing_types: Optional[List[str]] = None,\n    tickers: Optional[List[str]] = None,\n    days_back: int = 30\n) -> List[SECFiling]:\n    \"\"\"\n    Fetch recent SEC filings from EDGAR.\n    \n    Args:\n        filing_types: List of filing types (10-K, 10-Q, 8-K)\n        tickers: List of stock tickers to monitor\n        days_back: How many days back to search\n    \n    Returns:\n        List of SECFiling objects\n    \"\"\"\n    if filing_types is None:\n        filing_types = [\"10-K\", \"10-Q\", \"8-K\"]\n    \n    if tickers is None:\n        tickers = TRACKED_TICKERS[:10]\n    \n    filings = []\n    \n    for filing_type in filing_types:\n        url = f\"{EDGAR_COMPANY_SEARCH}?action=getcurrent&type={filing_type}&company=&dateb=&owner=include&count=40&output=atom\"\n        \n        print(f\"[SEC_EDGAR][FETCH] {filing_type} filings\")\n        \n        xml_content = _fetch_sec_page(url)\n        if not xml_content:\n            continue\n        \n        entries = _parse_rss_feed(xml_content)\n        \n        for entry in entries:\n            title = entry.get(\"title\", \"\")\n            company_name, cik, ticker = _extract_company_info(title)\n            \n            if not company_name:\n                continue\n            \n            if tickers and ticker and ticker not in tickers:\n                continue\n            \n            accession = \"\"\n            link = entry.get(\"link\", \"\")\n            acc_match = re.search(r'/Archives/edgar/data/\\d+/(\\d+-\\d+-\\d+)', link)\n            if acc_match:\n                accession = acc_match.group(1)\n            \n            filing = SECFiling(\n                cik=cik or \"\",\n                company_name=company_name,\n                ticker=ticker,\n                filing_type=filing_type,\n                filed_date=entry.get(\"updated\", \"\")[:10],\n                accession_number=accession,\n                url=link,\n                description=entry.get(\"summary\", \"\"),\n            )\n            filings.append(filing)\n            \n            print(f\"[SEC_EDGAR][FOUND] {company_name} - {filing_type}\")\n    \n    return filings\n\n\ndef extract_intelligence_from_filing(filing: SECFiling) -> Optional[ExtractedIntelligence]:\n    \"\"\"\n    Extract strategic intelligence from a filing.\n    \n    For full text extraction, we would fetch the actual filing document.\n    This simplified version uses the description/summary.\n    \"\"\"\n    text = f\"{filing.description or ''} {filing.company_name}\"\n    \n    if not any(kw in text.lower() for kw in FLORIDA_KEYWORDS):\n        if not any(kw in text.lower() for kw in EXPANSION_KEYWORDS + CONTRACTION_KEYWORDS):\n            return None\n    \n    force_type, confidence = _detect_force_type(text)\n    geographies = _detect_geographies(text)\n    segments = _extract_segments(text, force_type)\n    time_horizon = _extract_time_horizon(text)\n    \n    headline = _generate_headline(filing, force_type, text)\n    \n    return ExtractedIntelligence(\n        force_type=force_type,\n        headline=headline,\n        geographies=geographies,\n        segments_affected=segments,\n        time_horizon=time_horizon,\n        raw_snippet=text[:500],\n        confidence=confidence,\n    )\n\n\ndef create_macro_events_from_filings(\n    session: Session,\n    filings: List[SECFiling],\n    dry_run: bool = False\n) -> List[MacroEvent]:\n    \"\"\"\n    Create MacroEvent records from SEC filings.\n    \n    Args:\n        session: Database session\n        filings: List of SECFiling objects\n        dry_run: If True, don't persist to database\n    \n    Returns:\n        List of created MacroEvent objects\n    \"\"\"\n    macro_events = []\n    \n    for filing in filings:\n        intel = extract_intelligence_from_filing(filing)\n        if not intel:\n            print(f\"[SEC_EDGAR][SKIP] No actionable intelligence: {filing.company_name}\")\n            continue\n        \n        event_id = f\"macro-SEC-{filing.ticker or 'UNK'}-{filing.filing_type}-{filing.accession_number}\"\n        \n        existing = session.exec(\n            select(MacroEvent).where(MacroEvent.macro_event_id == event_id)\n        ).first()\n        \n        if existing:\n            print(f\"[SEC_EDGAR][DUP] MacroEvent already exists: {event_id}\")\n            continue\n        \n        source_type = FILING_TYPES.get(filing.filing_type, MACRO_SOURCE_SEC_10K)\n        \n        macro_event = MacroEvent(\n            macro_event_id=event_id,\n            source_type=source_type,\n            source_ref=filing.accession_number,\n            source_url=filing.url,\n            company_name=filing.company_name,\n            ticker=filing.ticker,\n            headline=intel.headline,\n            geographies=json.dumps(intel.geographies),\n            segments_affected=json.dumps(intel.segments_affected),\n            force_type=intel.force_type,\n            time_horizon=intel.time_horizon,\n            raw_snippet=intel.raw_snippet,\n            confidence=intel.confidence,\n        )\n        \n        if not dry_run:\n            session.add(macro_event)\n            macro_events.append(macro_event)\n            print(f\"[SEC_EDGAR][MACRO_EVENT] Created: {intel.headline}\")\n        else:\n            print(f\"[SEC_EDGAR][DRY_RUN] Would create: {intel.headline}\")\n    \n    if not dry_run and macro_events:\n        session.commit()\n    \n    return macro_events\n\n\ndef run_sec_edgar_ingestion(\n    session: Session,\n    filing_types: Optional[List[str]] = None,\n    tickers: Optional[List[str]] = None,\n    dry_run: bool = False\n) -> Dict:\n    \"\"\"\n    Main entry point for SEC EDGAR ingestion.\n    \n    Args:\n        session: Database session\n        filing_types: Filing types to fetch\n        tickers: Stock tickers to monitor\n        dry_run: Skip database writes\n    \n    Returns:\n        Summary dict with counts\n    \"\"\"\n    print(\"[SEC_EDGAR][START] Beginning SEC EDGAR ingestion\")\n    \n    filings = fetch_recent_filings(filing_types, tickers)\n    print(f\"[SEC_EDGAR][FETCH] Found {len(filings)} filings\")\n    \n    macro_events = create_macro_events_from_filings(session, filings, dry_run)\n    print(f\"[SEC_EDGAR][MACRO_EVENTS] Created {len(macro_events)} macro events\")\n    \n    result = {\n        \"filings_found\": len(filings),\n        \"macro_events_created\": len(macro_events),\n        \"dry_run\": dry_run,\n    }\n    \n    print(f\"[SEC_EDGAR][COMPLETE] {result}\")\n    return result\n\n\nprint(\"[SEC_EDGAR][STARTUP] SEC EDGAR Connector loaded - EPIC 5\")\n","path":null,"size_bytes":17830,"size_tokens":null},"forcecast_engine.py":{"content":"\"\"\"\nForceCast Mapping Engine for HossAgent - EPIC 5\n\nMaps MacroEvents (big-company moves from SEC filings, earnings calls, etc.)\nto SMB target profiles that represent downstream opportunities.\n\nExample Flow:\n1. MacroEvent: \"McDonald's plans 120 new Florida locations over 3 years\"\n2. ForceCast identifies affected SMB segments:\n   - HVAC contractors (new builds need AC)\n   - Electrical contractors (commercial wiring)\n   - Plumbing contractors\n   - Construction suppliers\n   - Staffing agencies\n   - Commercial cleaning services\n3. Generates LeadEvents for each SMB segment in affected geographies\n\nThis creates a strategic intelligence layer that connects big-company\nmoves to small business opportunities.\n\"\"\"\n\nimport json\nimport re\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom sqlmodel import Session, select\n\nfrom models import (\n    MacroEvent,\n    LeadEvent,\n    Signal,\n    Company,\n    MACRO_FORCE_TYPE_EXPANSION,\n    MACRO_FORCE_TYPE_CONTRACTION,\n    MACRO_FORCE_TYPE_RESTRUCTURING,\n    MACRO_FORCE_TYPE_MERGER,\n    MACRO_FORCE_TYPE_BANKRUPTCY,\n    MACRO_FORCE_TYPE_SUPPLY_CHAIN,\n    MACRO_FORCE_TYPE_REGULATORY,\n    ENRICHMENT_STATUS_UNENRICHED,\n)\n\n\nFORCE_TYPE_SMB_MAPPINGS = {\n    MACRO_FORCE_TYPE_EXPANSION: {\n        \"primary_segments\": [\n            \"hvac_contractor\",\n            \"electrical_contractor\",\n            \"plumbing_contractor\",\n            \"general_contractor\",\n            \"commercial_construction\",\n            \"staffing_agency\",\n            \"commercial_cleaning\",\n            \"security_services\",\n            \"landscaping\",\n            \"signage_company\",\n        ],\n        \"secondary_segments\": [\n            \"commercial_real_estate\",\n            \"equipment_supplier\",\n            \"office_furniture\",\n            \"it_services\",\n            \"catering_services\",\n            \"uniform_supplier\",\n        ],\n        \"urgency_multiplier\": 1.2,\n        \"opportunity_type\": \"growth_opportunity\",\n    },\n    MACRO_FORCE_TYPE_CONTRACTION: {\n        \"primary_segments\": [\n            \"liquidation_services\",\n            \"commercial_real_estate\",\n            \"business_broker\",\n            \"equipment_resale\",\n            \"staffing_agency\",\n            \"outplacement_services\",\n        ],\n        \"secondary_segments\": [\n            \"moving_services\",\n            \"storage_facilities\",\n            \"legal_services\",\n        ],\n        \"urgency_multiplier\": 1.5,\n        \"opportunity_type\": \"market_shift\",\n    },\n    MACRO_FORCE_TYPE_MERGER: {\n        \"primary_segments\": [\n            \"it_integration\",\n            \"hr_consulting\",\n            \"legal_services\",\n            \"branding_agency\",\n            \"commercial_construction\",\n            \"office_furniture\",\n        ],\n        \"secondary_segments\": [\n            \"staffing_agency\",\n            \"training_services\",\n            \"security_services\",\n        ],\n        \"urgency_multiplier\": 1.3,\n        \"opportunity_type\": \"competitor_intel\",\n    },\n    MACRO_FORCE_TYPE_BANKRUPTCY: {\n        \"primary_segments\": [\n            \"liquidation_services\",\n            \"business_broker\",\n            \"legal_services\",\n            \"commercial_real_estate\",\n            \"equipment_resale\",\n        ],\n        \"secondary_segments\": [\n            \"staffing_agency\",\n            \"moving_services\",\n            \"storage_facilities\",\n        ],\n        \"urgency_multiplier\": 1.8,\n        \"opportunity_type\": \"market_shift\",\n    },\n    MACRO_FORCE_TYPE_SUPPLY_CHAIN: {\n        \"primary_segments\": [\n            \"logistics_provider\",\n            \"warehouse_services\",\n            \"freight_broker\",\n            \"equipment_supplier\",\n            \"manufacturing_services\",\n        ],\n        \"secondary_segments\": [\n            \"it_services\",\n            \"consulting_services\",\n            \"distribution_services\",\n        ],\n        \"urgency_multiplier\": 1.4,\n        \"opportunity_type\": \"growth_opportunity\",\n    },\n    MACRO_FORCE_TYPE_RESTRUCTURING: {\n        \"primary_segments\": [\n            \"consulting_services\",\n            \"hr_consulting\",\n            \"it_services\",\n            \"legal_services\",\n            \"staffing_agency\",\n        ],\n        \"secondary_segments\": [\n            \"training_services\",\n            \"commercial_construction\",\n            \"office_furniture\",\n        ],\n        \"urgency_multiplier\": 1.1,\n        \"opportunity_type\": \"market_shift\",\n    },\n    MACRO_FORCE_TYPE_REGULATORY: {\n        \"primary_segments\": [\n            \"compliance_consulting\",\n            \"legal_services\",\n            \"it_services\",\n            \"training_services\",\n            \"environmental_services\",\n        ],\n        \"secondary_segments\": [\n            \"insurance_broker\",\n            \"hr_consulting\",\n            \"safety_consulting\",\n        ],\n        \"urgency_multiplier\": 1.0,\n        \"opportunity_type\": \"market_shift\",\n    },\n}\n\nSEGMENT_TO_NICHE = {\n    \"hvac_contractor\": \"HVAC\",\n    \"electrical_contractor\": \"Electrical\",\n    \"plumbing_contractor\": \"Plumbing\",\n    \"general_contractor\": \"Construction\",\n    \"commercial_construction\": \"Construction\",\n    \"staffing_agency\": \"Staffing\",\n    \"commercial_cleaning\": \"Commercial Cleaning\",\n    \"security_services\": \"Security\",\n    \"landscaping\": \"Landscaping\",\n    \"signage_company\": \"Signage\",\n    \"commercial_real_estate\": \"Commercial Real Estate\",\n    \"equipment_supplier\": \"Equipment Supply\",\n    \"office_furniture\": \"Office Furniture\",\n    \"it_services\": \"IT Services\",\n    \"catering_services\": \"Catering\",\n    \"uniform_supplier\": \"Uniform Supply\",\n    \"liquidation_services\": \"Liquidation\",\n    \"business_broker\": \"Business Brokerage\",\n    \"equipment_resale\": \"Equipment Resale\",\n    \"outplacement_services\": \"HR Services\",\n    \"moving_services\": \"Moving Services\",\n    \"storage_facilities\": \"Storage\",\n    \"legal_services\": \"Legal\",\n    \"it_integration\": \"IT Integration\",\n    \"hr_consulting\": \"HR Consulting\",\n    \"branding_agency\": \"Marketing Agency\",\n    \"training_services\": \"Training\",\n    \"logistics_provider\": \"Logistics\",\n    \"warehouse_services\": \"Warehousing\",\n    \"freight_broker\": \"Freight Brokerage\",\n    \"manufacturing_services\": \"Manufacturing\",\n    \"consulting_services\": \"Consulting\",\n    \"distribution_services\": \"Distribution\",\n    \"compliance_consulting\": \"Compliance Consulting\",\n    \"environmental_services\": \"Environmental Services\",\n    \"insurance_broker\": \"Insurance\",\n    \"safety_consulting\": \"Safety Consulting\",\n}\n\nSOUTH_FLORIDA_GEOS = [\n    \"Miami\",\n    \"Miami-Dade\",\n    \"Fort Lauderdale\",\n    \"Broward\",\n    \"West Palm Beach\",\n    \"Palm Beach\",\n    \"Boca Raton\",\n    \"Hollywood\",\n    \"Hialeah\",\n    \"Coral Gables\",\n    \"Doral\",\n    \"South Florida\",\n    \"Florida\",\n]\n\n\n@dataclass\nclass SMBTarget:\n    \"\"\"A potential SMB target derived from a MacroEvent.\"\"\"\n    segment: str\n    niche: str\n    geography: str\n    urgency_score: int\n    opportunity_type: str\n    macro_event_id: int\n    macro_headline: str\n    company_name: str\n    time_horizon: Optional[str] = None\n    recommended_action: Optional[str] = None\n    is_primary: bool = True\n\n\n@dataclass\nclass ForceCastResult:\n    \"\"\"Result of ForceCast mapping for a single MacroEvent.\"\"\"\n    macro_event_id: int\n    force_type: str\n    targets_generated: int\n    primary_targets: List[SMBTarget] = field(default_factory=list)\n    secondary_targets: List[SMBTarget] = field(default_factory=list)\n    geographies_covered: List[str] = field(default_factory=list)\n    \n    def to_dict(self) -> Dict:\n        return {\n            \"macro_event_id\": self.macro_event_id,\n            \"force_type\": self.force_type,\n            \"targets_generated\": self.targets_generated,\n            \"primary_count\": len(self.primary_targets),\n            \"secondary_count\": len(self.secondary_targets),\n            \"geographies\": self.geographies_covered,\n        }\n\n\ndef _parse_geographies(geo_json: Optional[str]) -> List[str]:\n    \"\"\"Parse geographies JSON field from MacroEvent.\"\"\"\n    if not geo_json:\n        return [\"South Florida\"]\n    \n    try:\n        geos = json.loads(geo_json)\n        if isinstance(geos, list):\n            return geos\n        return [\"South Florida\"]\n    except json.JSONDecodeError:\n        return [\"South Florida\"]\n\n\ndef _filter_south_florida_geos(geos: List[str]) -> List[str]:\n    \"\"\"Filter geographies to South Florida regions only.\"\"\"\n    result = []\n    for geo in geos:\n        geo_lower = geo.lower()\n        for sfla_geo in SOUTH_FLORIDA_GEOS:\n            if sfla_geo.lower() in geo_lower or geo_lower in sfla_geo.lower():\n                result.append(geo)\n                break\n    \n    return result if result else [\"South Florida\"]\n\n\ndef _calculate_urgency_score(\n    base_score: int,\n    multiplier: float,\n    is_primary: bool,\n    time_horizon: Optional[str]\n) -> int:\n    \"\"\"Calculate urgency score for an SMB target.\"\"\"\n    score = base_score * multiplier\n    \n    if not is_primary:\n        score *= 0.8\n    \n    if time_horizon:\n        horizon_lower = time_horizon.lower()\n        if \"0-12\" in horizon_lower or \"immediate\" in horizon_lower:\n            score *= 1.2\n        elif \"1-3\" in horizon_lower:\n            score *= 1.0\n        elif \"3-5\" in horizon_lower:\n            score *= 0.8\n    \n    return min(100, max(10, int(score)))\n\n\ndef _generate_recommended_action(\n    segment: str,\n    force_type: str,\n    company_name: str,\n    geography: str\n) -> str:\n    \"\"\"Generate a recommended action string for the SMB target.\"\"\"\n    niche = SEGMENT_TO_NICHE.get(segment, segment.replace(\"_\", \" \").title())\n    \n    actions = {\n        MACRO_FORCE_TYPE_EXPANSION: f\"Reach out to {niche} businesses in {geography}. {company_name}'s expansion creates new commercial opportunities.\",\n        MACRO_FORCE_TYPE_CONTRACTION: f\"Alert {niche} providers in {geography}. {company_name}'s contraction may release business to local competitors.\",\n        MACRO_FORCE_TYPE_MERGER: f\"Target {niche} companies in {geography}. {company_name}'s merger creates integration and transition opportunities.\",\n        MACRO_FORCE_TYPE_BANKRUPTCY: f\"Urgent: {niche} providers in {geography} may pick up business from {company_name}'s bankruptcy.\",\n        MACRO_FORCE_TYPE_SUPPLY_CHAIN: f\"Contact {niche} businesses in {geography}. {company_name}'s supply chain changes may require new local partners.\",\n        MACRO_FORCE_TYPE_RESTRUCTURING: f\"Reach {niche} providers in {geography}. {company_name}'s restructuring may need outside expertise.\",\n        MACRO_FORCE_TYPE_REGULATORY: f\"Inform {niche} businesses in {geography}. Regulatory changes at {company_name} may affect the local market.\",\n    }\n    \n    return actions.get(force_type, f\"Research {niche} opportunities in {geography} related to {company_name}.\")\n\n\ndef map_macro_event_to_smb_targets(\n    macro_event: MacroEvent,\n    include_secondary: bool = True,\n    limit_per_segment: int = 3\n) -> ForceCastResult:\n    \"\"\"\n    Map a single MacroEvent to potential SMB targets.\n    \n    Args:\n        macro_event: The MacroEvent to map\n        include_secondary: Whether to include secondary (lower priority) segments\n        limit_per_segment: Max targets per segment per geography\n    \n    Returns:\n        ForceCastResult with all generated SMB targets\n    \"\"\"\n    force_type = macro_event.force_type\n    mapping = FORCE_TYPE_SMB_MAPPINGS.get(force_type, FORCE_TYPE_SMB_MAPPINGS[MACRO_FORCE_TYPE_EXPANSION])\n    \n    raw_geos = _parse_geographies(macro_event.geographies)\n    geographies = _filter_south_florida_geos(raw_geos)\n    \n    primary_targets = []\n    secondary_targets = []\n    \n    base_score = 70\n    multiplier = mapping.get(\"urgency_multiplier\", 1.0)\n    opportunity_type = mapping.get(\"opportunity_type\", \"growth_opportunity\")\n    \n    for segment in mapping.get(\"primary_segments\", []):\n        niche = SEGMENT_TO_NICHE.get(segment, segment.replace(\"_\", \" \").title())\n        \n        for geo in geographies[:limit_per_segment]:\n            urgency = _calculate_urgency_score(base_score, multiplier, True, macro_event.time_horizon)\n            action = _generate_recommended_action(segment, force_type, macro_event.company_name, geo)\n            \n            target = SMBTarget(\n                segment=segment,\n                niche=niche,\n                geography=geo,\n                urgency_score=urgency,\n                opportunity_type=opportunity_type,\n                macro_event_id=macro_event.id,\n                macro_headline=macro_event.headline,\n                company_name=macro_event.company_name,\n                time_horizon=macro_event.time_horizon,\n                recommended_action=action,\n                is_primary=True,\n            )\n            primary_targets.append(target)\n    \n    if include_secondary:\n        for segment in mapping.get(\"secondary_segments\", []):\n            niche = SEGMENT_TO_NICHE.get(segment, segment.replace(\"_\", \" \").title())\n            \n            for geo in geographies[:limit_per_segment]:\n                urgency = _calculate_urgency_score(base_score, multiplier, False, macro_event.time_horizon)\n                action = _generate_recommended_action(segment, force_type, macro_event.company_name, geo)\n                \n                target = SMBTarget(\n                    segment=segment,\n                    niche=niche,\n                    geography=geo,\n                    urgency_score=urgency,\n                    opportunity_type=opportunity_type,\n                    macro_event_id=macro_event.id,\n                    macro_headline=macro_event.headline,\n                    company_name=macro_event.company_name,\n                    time_horizon=macro_event.time_horizon,\n                    recommended_action=action,\n                    is_primary=False,\n                )\n                secondary_targets.append(target)\n    \n    return ForceCastResult(\n        macro_event_id=macro_event.id,\n        force_type=force_type,\n        targets_generated=len(primary_targets) + len(secondary_targets),\n        primary_targets=primary_targets,\n        secondary_targets=secondary_targets,\n        geographies_covered=geographies,\n    )\n\n\ndef create_lead_events_from_targets(\n    session: Session,\n    targets: List[SMBTarget],\n    dry_run: bool = False\n) -> List[LeadEvent]:\n    \"\"\"\n    Create LeadEvent records from SMB targets.\n    \n    Args:\n        session: Database session\n        targets: List of SMBTarget objects\n        dry_run: If True, don't persist to database\n    \n    Returns:\n        List of created LeadEvent objects\n    \"\"\"\n    lead_events = []\n    \n    for target in targets:\n        summary = f\"MacroStorm Alert: {target.macro_headline}\"\n        if target.geography and target.geography != \"South Florida\":\n            summary += f\" ({target.geography})\"\n        \n        lead_event = LeadEvent(\n            macro_event_id=target.macro_event_id,\n            lead_company=f\"{target.niche} businesses in {target.geography}\",\n            summary=summary,\n            category=target.opportunity_type.upper(),\n            urgency_score=target.urgency_score,\n            status=\"NEW\",\n            enrichment_status=ENRICHMENT_STATUS_UNENRICHED,\n            recommended_action=target.recommended_action,\n        )\n        \n        if not dry_run:\n            session.add(lead_event)\n            lead_events.append(lead_event)\n            print(f\"[FORCECAST][LEAD_EVENT] Created: {target.niche} in {target.geography}\")\n        else:\n            print(f\"[FORCECAST][DRY_RUN] Would create: {target.niche} in {target.geography}\")\n    \n    if not dry_run and lead_events:\n        session.commit()\n    \n    return lead_events\n\n\ndef process_unprocessed_macro_events(\n    session: Session,\n    limit: int = 10,\n    include_secondary: bool = True,\n    dry_run: bool = False\n) -> Dict:\n    \"\"\"\n    Process unprocessed MacroEvents and generate LeadEvents.\n    \n    Args:\n        session: Database session\n        limit: Max MacroEvents to process\n        include_secondary: Include secondary SMB segments\n        dry_run: Skip database writes\n    \n    Returns:\n        Summary dict with counts\n    \"\"\"\n    print(\"[FORCECAST][START] Processing unprocessed MacroEvents\")\n    \n    macro_events = session.exec(\n        select(MacroEvent)\n        .where(MacroEvent.processed == False)\n        .order_by(MacroEvent.created_at.desc())\n        .limit(limit)\n    ).all()\n    \n    if not macro_events:\n        print(\"[FORCECAST][SKIP] No unprocessed MacroEvents found\")\n        return {\n            \"macro_events_processed\": 0,\n            \"targets_generated\": 0,\n            \"lead_events_created\": 0,\n        }\n    \n    total_targets = 0\n    total_lead_events = 0\n    \n    for macro_event in macro_events:\n        print(f\"[FORCECAST][PROCESS] MacroEvent {macro_event.id}: {macro_event.headline[:50]}...\")\n        \n        result = map_macro_event_to_smb_targets(\n            macro_event,\n            include_secondary=include_secondary,\n        )\n        \n        all_targets = result.primary_targets + result.secondary_targets\n        total_targets += len(all_targets)\n        \n        lead_events = create_lead_events_from_targets(session, all_targets, dry_run)\n        total_lead_events += len(lead_events)\n        \n        if not dry_run:\n            macro_event.processed = True\n            macro_event.processed_at = datetime.utcnow()\n            macro_event.leads_generated = len(lead_events)\n            session.add(macro_event)\n    \n    if not dry_run:\n        session.commit()\n    \n    result = {\n        \"macro_events_processed\": len(macro_events),\n        \"targets_generated\": total_targets,\n        \"lead_events_created\": total_lead_events,\n        \"dry_run\": dry_run,\n    }\n    \n    print(f\"[FORCECAST][COMPLETE] {result}\")\n    return result\n\n\ndef get_forcecast_analytics(session: Session) -> Dict:\n    \"\"\"\n    Get analytics on ForceCast performance.\n    \n    Returns:\n        Dict with analytics data\n    \"\"\"\n    total_macro = session.exec(select(MacroEvent)).all()\n    processed_macro = [m for m in total_macro if m.processed]\n    \n    total_leads_generated = sum(m.leads_generated for m in processed_macro)\n    total_leads_enriched = sum(m.leads_enriched for m in processed_macro)\n    total_leads_contacted = sum(m.leads_contacted for m in processed_macro)\n    total_leads_replied = sum(m.leads_replied for m in processed_macro)\n    \n    force_type_breakdown = {}\n    for m in total_macro:\n        ft = m.force_type\n        if ft not in force_type_breakdown:\n            force_type_breakdown[ft] = {\"count\": 0, \"leads_generated\": 0}\n        force_type_breakdown[ft][\"count\"] += 1\n        force_type_breakdown[ft][\"leads_generated\"] += m.leads_generated\n    \n    return {\n        \"total_macro_events\": len(total_macro),\n        \"processed_macro_events\": len(processed_macro),\n        \"unprocessed_macro_events\": len(total_macro) - len(processed_macro),\n        \"total_leads_generated\": total_leads_generated,\n        \"total_leads_enriched\": total_leads_enriched,\n        \"total_leads_contacted\": total_leads_contacted,\n        \"total_leads_replied\": total_leads_replied,\n        \"conversion_rate\": round(total_leads_enriched / total_leads_generated * 100, 2) if total_leads_generated > 0 else 0,\n        \"force_type_breakdown\": force_type_breakdown,\n    }\n\n\nrun_forcecast_for_new_macro_events = process_unprocessed_macro_events\n\nprint(\"[FORCECAST][STARTUP] ForceCast Mapping Engine loaded - EPIC 5\")\n","path":null,"size_bytes":19364,"size_tokens":null},"job_board_connector.py":{"content":"\"\"\"\nJob Board Connector for HossAgent SignalNet - EPIC 3.2\n\nIngests job postings from public job boards to identify SMBs that are hiring.\nHiring signals indicate business growth and potential need for B2B services.\n\nTarget regions: South Florida (Miami-Dade, Broward, Palm Beach)\nTarget niches: HVAC, plumbing, roofing, electrical, landscaping, trades\n\nPure web scraping - NO paid APIs.\n\"\"\"\n\nimport os\nimport re\nimport json\nimport time\nimport hashlib\nimport random\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional, Tuple\nfrom urllib.parse import urljoin, urlparse, quote_plus\n\nimport requests\nfrom sqlmodel import Session, select\n\nfrom models import Signal, LeadEvent, ENRICHMENT_STATUS_UNENRICHED\n\n\nREGIONS = {\n    \"miami\": {\n        \"name\": \"Miami\",\n        \"geography\": \"Miami-Dade County, Florida\",\n        \"zip_codes\": [\"33101\", \"33125\", \"33130\", \"33132\", \"33136\", \"33139\", \"33142\", \"33145\"],\n    },\n    \"fort_lauderdale\": {\n        \"name\": \"Fort Lauderdale\",\n        \"geography\": \"Broward County, Florida\",\n        \"zip_codes\": [\"33301\", \"33304\", \"33305\", \"33306\", \"33308\", \"33309\", \"33311\"],\n    },\n    \"west_palm\": {\n        \"name\": \"West Palm Beach\",\n        \"geography\": \"Palm Beach County, Florida\",\n        \"zip_codes\": [\"33401\", \"33403\", \"33405\", \"33407\", \"33409\", \"33411\"],\n    },\n}\n\nTRADE_NICHES = {\n    \"hvac\": [\"hvac\", \"air conditioning\", \"heating\", \"ac technician\", \"refrigeration\"],\n    \"plumbing\": [\"plumber\", \"plumbing\", \"pipe fitter\", \"drain\", \"water heater\"],\n    \"roofing\": [\"roofing\", \"roofer\", \"shingle\", \"roof repair\", \"roof installer\"],\n    \"electrical\": [\"electrician\", \"electrical\", \"wiring\", \"electrical contractor\"],\n    \"landscaping\": [\"landscaping\", \"landscaper\", \"lawn care\", \"irrigation\", \"tree service\"],\n    \"construction\": [\"construction\", \"contractor\", \"general contractor\", \"builder\", \"framing\"],\n    \"painting\": [\"painter\", \"painting contractor\", \"commercial painting\"],\n    \"pool\": [\"pool service\", \"pool technician\", \"pool maintenance\", \"pool installer\"],\n    \"cleaning\": [\"cleaning service\", \"janitorial\", \"commercial cleaning\", \"pressure washing\"],\n    \"pest_control\": [\"pest control\", \"exterminator\", \"termite\", \"fumigation\"],\n}\n\nUSER_AGENTS = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15\",\n]\n\nREQUEST_TIMEOUT = 15\nMIN_DELAY = 2.0\nMAX_DELAY = 5.0\nMAX_RETRIES = 2\nRATE_LIMIT_COOLDOWN = 300\n\n_consecutive_failures = 0\n_last_failure_time = 0.0\n_request_cache: Dict[str, Tuple[str, float]] = {}\nCACHE_TTL = 3600\n\n\n@dataclass\nclass JobPosting:\n    \"\"\"A single job posting extracted from a job board.\"\"\"\n    title: str\n    company_name: str\n    location: str\n    description: Optional[str] = None\n    url: Optional[str] = None\n    posted_date: Optional[str] = None\n    niche: Optional[str] = None\n    geography: Optional[str] = None\n    source: str = \"job_board\"\n    \n    def to_signal_data(self) -> Dict:\n        \"\"\"Convert to signal data format.\"\"\"\n        return {\n            \"title\": self.title,\n            \"company_name\": self.company_name,\n            \"location\": self.location,\n            \"description\": self.description or \"\",\n            \"url\": self.url or \"\",\n            \"posted_date\": self.posted_date or \"\",\n            \"niche\": self.niche or \"\",\n            \"geography\": self.geography or \"\",\n            \"source\": self.source,\n        }\n\n\ndef _get_random_user_agent() -> str:\n    \"\"\"Get a random user agent for requests.\"\"\"\n    return random.choice(USER_AGENTS)\n\n\ndef _get_cached_response(url: str) -> Optional[str]:\n    \"\"\"Get cached response if still valid.\"\"\"\n    if url in _request_cache:\n        content, timestamp = _request_cache[url]\n        if time.time() - timestamp < CACHE_TTL:\n            return content\n    return None\n\n\ndef _cache_response(url: str, content: str) -> None:\n    \"\"\"Cache response content.\"\"\"\n    _request_cache[url] = (content, time.time())\n\n\ndef _should_backoff() -> bool:\n    \"\"\"Check if we should back off due to rate limiting.\"\"\"\n    global _consecutive_failures, _last_failure_time\n    \n    if _consecutive_failures >= 3:\n        time_since_failure = time.time() - _last_failure_time\n        if time_since_failure < RATE_LIMIT_COOLDOWN:\n            print(f\"[JOB_BOARD][BACKOFF] Waiting {int(RATE_LIMIT_COOLDOWN - time_since_failure)}s before retry\")\n            return True\n        else:\n            _consecutive_failures = 0\n    return False\n\n\ndef _record_failure() -> None:\n    \"\"\"Record a failure for backoff tracking.\"\"\"\n    global _consecutive_failures, _last_failure_time\n    _consecutive_failures += 1\n    _last_failure_time = time.time()\n\n\ndef _record_success() -> None:\n    \"\"\"Record success to reset failure count.\"\"\"\n    global _consecutive_failures\n    _consecutive_failures = 0\n\n\ndef _fetch_page(url: str, retries: int = MAX_RETRIES) -> Optional[str]:\n    \"\"\"Fetch a page with caching and rate limiting.\"\"\"\n    if _should_backoff():\n        return None\n    \n    cached = _get_cached_response(url)\n    if cached:\n        print(f\"[JOB_BOARD][CACHE_HIT] {url[:60]}\")\n        return cached\n    \n    delay = random.uniform(MIN_DELAY, MAX_DELAY)\n    time.sleep(delay)\n    \n    headers = {\n        \"User-Agent\": _get_random_user_agent(),\n        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n        \"Accept-Language\": \"en-US,en;q=0.5\",\n        \"Accept-Encoding\": \"gzip, deflate\",\n        \"Connection\": \"keep-alive\",\n    }\n    \n    for attempt in range(retries):\n        try:\n            response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)\n            \n            if response.status_code == 200:\n                _record_success()\n                _cache_response(url, response.text)\n                return response.text\n            elif response.status_code == 429:\n                _record_failure()\n                print(f\"[JOB_BOARD][RATE_LIMITED] {url[:60]}\")\n                return None\n            elif response.status_code >= 400:\n                print(f\"[JOB_BOARD][HTTP_{response.status_code}] {url[:60]}\")\n                if attempt < retries - 1:\n                    time.sleep(delay * (attempt + 1))\n                    continue\n                return None\n                \n        except requests.exceptions.Timeout:\n            print(f\"[JOB_BOARD][TIMEOUT] {url[:60]}\")\n            if attempt < retries - 1:\n                time.sleep(delay * (attempt + 1))\n                continue\n        except requests.exceptions.RequestException as e:\n            print(f\"[JOB_BOARD][ERROR] {url[:60]}: {str(e)[:50]}\")\n            if attempt < retries - 1:\n                time.sleep(delay * (attempt + 1))\n                continue\n    \n    _record_failure()\n    return None\n\n\ndef _detect_niche(text: str) -> Optional[str]:\n    \"\"\"Detect the trade niche from job posting text.\"\"\"\n    text_lower = text.lower()\n    \n    for niche, keywords in TRADE_NICHES.items():\n        for keyword in keywords:\n            if keyword in text_lower:\n                return niche\n    \n    return None\n\n\ndef _extract_company_name(html: str, title: str) -> Optional[str]:\n    \"\"\"Extract company name from job posting HTML.\"\"\"\n    patterns = [\n        r'data-company=\"([^\"]+)\"',\n        r'\"companyName\"\\s*:\\s*\"([^\"]+)\"',\n        r'\"employer\"\\s*:\\s*\\{\\s*\"name\"\\s*:\\s*\"([^\"]+)\"',\n        r'class=\"[^\"]*company[^\"]*\"[^>]*>([^<]+)<',\n        r'<span[^>]*class=\"[^\"]*employer[^\"]*\"[^>]*>([^<]+)<',\n        r'hiring organization.*?<[^>]+>([^<]+)<',\n    ]\n    \n    for pattern in patterns:\n        match = re.search(pattern, html, re.IGNORECASE | re.DOTALL)\n        if match:\n            name = match.group(1).strip()\n            if len(name) > 2 and len(name) < 100:\n                name = re.sub(r'<[^>]+>', '', name)\n                name = name.strip()\n                if name:\n                    return name\n    \n    return None\n\n\ndef _extract_location(html: str) -> Optional[str]:\n    \"\"\"Extract location from job posting HTML.\"\"\"\n    patterns = [\n        r'data-location=\"([^\"]+)\"',\n        r'\"jobLocation\"\\s*:\\s*\\{[^}]*\"addressLocality\"\\s*:\\s*\"([^\"]+)\"',\n        r'\"location\"\\s*:\\s*\"([^\"]+)\"',\n        r'class=\"[^\"]*location[^\"]*\"[^>]*>([^<]+)<',\n        r'Miami|Fort Lauderdale|West Palm Beach|Broward|Palm Beach|Dade',\n    ]\n    \n    for pattern in patterns:\n        match = re.search(pattern, html, re.IGNORECASE)\n        if match:\n            if isinstance(match.group(0), str) and match.group(0) in [\"Miami\", \"Fort Lauderdale\", \"West Palm Beach\", \"Broward\", \"Palm Beach\", \"Dade\"]:\n                return match.group(0)\n            location = match.group(1).strip() if match.lastindex else match.group(0)\n            return location\n    \n    return None\n\n\ndef _parse_indeed_html(html: str, region: str) -> List[JobPosting]:\n    \"\"\"Parse Indeed search results HTML for job postings.\"\"\"\n    jobs = []\n    \n    job_card_pattern = r'<div[^>]*class=\"[^\"]*job_seen_beacon[^\"]*\"[^>]*>(.*?)</div>\\s*</div>\\s*</div>'\n    \n    title_pattern = r'<h2[^>]*class=\"[^\"]*jobTitle[^\"]*\"[^>]*>.*?<a[^>]*>.*?<span[^>]*>([^<]+)</span>'\n    company_pattern = r'<span[^>]*data-testid=\"company-name\"[^>]*>([^<]+)</span>'\n    location_pattern = r'<div[^>]*data-testid=\"text-location\"[^>]*>([^<]+)</div>'\n    \n    titles = re.findall(title_pattern, html, re.IGNORECASE | re.DOTALL)\n    companies = re.findall(company_pattern, html, re.IGNORECASE)\n    locations = re.findall(location_pattern, html, re.IGNORECASE)\n    \n    min_len = min(len(titles), len(companies), len(locations))\n    \n    region_info = REGIONS.get(region, {})\n    geography = region_info.get(\"geography\", \"South Florida\")\n    \n    for i in range(min_len):\n        title = titles[i].strip()\n        company = companies[i].strip()\n        location = locations[i].strip()\n        \n        niche = _detect_niche(f\"{title} {company}\")\n        \n        if niche:\n            jobs.append(JobPosting(\n                title=title,\n                company_name=company,\n                location=location,\n                niche=niche,\n                geography=geography,\n                source=\"indeed\",\n            ))\n    \n    return jobs\n\n\ndef _parse_glassdoor_html(html: str, region: str) -> List[JobPosting]:\n    \"\"\"Parse Glassdoor search results HTML for job postings.\"\"\"\n    jobs = []\n    \n    job_pattern = r'<li[^>]*class=\"[^\"]*JobsList_jobListItem[^\"]*\"[^>]*>(.*?)</li>'\n    title_pattern = r'<a[^>]*class=\"[^\"]*JobCard_jobTitle[^\"]*\"[^>]*>([^<]+)</a>'\n    company_pattern = r'<span[^>]*class=\"[^\"]*EmployerProfile_compactEmployerName[^\"]*\"[^>]*>([^<]+)</span>'\n    \n    region_info = REGIONS.get(region, {})\n    geography = region_info.get(\"geography\", \"South Florida\")\n    \n    for job_match in re.finditer(job_pattern, html, re.IGNORECASE | re.DOTALL):\n        job_html = job_match.group(1)\n        \n        title_match = re.search(title_pattern, job_html, re.IGNORECASE)\n        company_match = re.search(company_pattern, job_html, re.IGNORECASE)\n        \n        if title_match and company_match:\n            title = title_match.group(1).strip()\n            company = company_match.group(1).strip()\n            \n            niche = _detect_niche(f\"{title} {company}\")\n            \n            if niche:\n                jobs.append(JobPosting(\n                    title=title,\n                    company_name=company,\n                    location=region_info.get(\"name\", \"South Florida\"),\n                    niche=niche,\n                    geography=geography,\n                    source=\"glassdoor\",\n                ))\n    \n    return jobs\n\n\ndef _search_duckduckgo_jobs(query: str, region: str) -> List[JobPosting]:\n    \"\"\"\n    Search DuckDuckGo for job postings as a fallback.\n    More reliable than scraping Indeed/Glassdoor directly.\n    \"\"\"\n    jobs = []\n    \n    region_info = REGIONS.get(region, {})\n    location = region_info.get(\"name\", \"Miami\")\n    geography = region_info.get(\"geography\", \"South Florida\")\n    \n    search_query = f\"{query} jobs {location} Florida hiring\"\n    encoded_query = quote_plus(search_query)\n    \n    url = f\"https://html.duckduckgo.com/html/?q={encoded_query}\"\n    \n    html = _fetch_page(url)\n    if not html:\n        return jobs\n    \n    result_pattern = r'<a[^>]*class=\"[^\"]*result__a[^\"]*\"[^>]*href=\"([^\"]+)\"[^>]*>([^<]+)</a>'\n    \n    for match in re.finditer(result_pattern, html, re.IGNORECASE):\n        url = match.group(1)\n        title = match.group(2).strip()\n        \n        if 'indeed.com' in url.lower() or 'glassdoor.com' in url.lower() or 'linkedin.com/jobs' in url.lower():\n            company_match = re.search(r'at\\s+([^-]+?)(?:\\s*-|\\s*\\||\\s*$)', title, re.IGNORECASE)\n            if company_match:\n                company = company_match.group(1).strip()\n            else:\n                company = None\n            \n            niche = _detect_niche(title)\n            \n            if niche and company:\n                jobs.append(JobPosting(\n                    title=title,\n                    company_name=company,\n                    location=location,\n                    url=url,\n                    niche=niche,\n                    geography=geography,\n                    source=\"duckduckgo_jobs\",\n                ))\n    \n    return jobs\n\n\ndef fetch_job_postings(\n    niches: Optional[List[str]] = None,\n    regions: Optional[List[str]] = None,\n    max_per_niche: int = 5\n) -> List[JobPosting]:\n    \"\"\"\n    Fetch job postings from multiple sources.\n    \n    Uses DuckDuckGo search as the primary method since direct scraping\n    of Indeed/Glassdoor is heavily rate-limited.\n    \n    Args:\n        niches: List of trade niches to search (default: all)\n        regions: List of regions to search (default: all South Florida)\n        max_per_niche: Maximum jobs per niche per region\n    \n    Returns:\n        List of JobPosting objects\n    \"\"\"\n    if niches is None:\n        niches = list(TRADE_NICHES.keys())[:5]\n    \n    if regions is None:\n        regions = list(REGIONS.keys())\n    \n    all_jobs = []\n    seen_companies = set()\n    \n    for region in regions:\n        for niche in niches:\n            if _should_backoff():\n                print(f\"[JOB_BOARD][SKIP] Backing off - skipping {niche} in {region}\")\n                continue\n            \n            keywords = TRADE_NICHES.get(niche, [niche])\n            primary_keyword = keywords[0] if keywords else niche\n            \n            print(f\"[JOB_BOARD][SEARCH] {primary_keyword} jobs in {region}\")\n            \n            jobs = _search_duckduckgo_jobs(primary_keyword, region)\n            \n            added = 0\n            for job in jobs:\n                if added >= max_per_niche:\n                    break\n                \n                company_key = f\"{job.company_name.lower()}:{job.geography}\"\n                if company_key in seen_companies:\n                    continue\n                \n                seen_companies.add(company_key)\n                all_jobs.append(job)\n                added += 1\n            \n            print(f\"[JOB_BOARD][FOUND] {added} unique jobs for {primary_keyword} in {region}\")\n    \n    return all_jobs\n\n\ndef create_signals_from_jobs(\n    session: Session,\n    jobs: List[JobPosting],\n    dry_run: bool = False\n) -> List[Signal]:\n    \"\"\"\n    Create Signal records from job postings.\n    \n    Args:\n        session: Database session\n        jobs: List of JobPosting objects\n        dry_run: If True, don't persist to database\n    \n    Returns:\n        List of created Signal objects\n    \"\"\"\n    signals = []\n    \n    for job in jobs:\n        signal_hash = hashlib.md5(\n            f\"{job.company_name}:{job.title}:{job.geography}\".encode()\n        ).hexdigest()[:16]\n        \n        existing = session.exec(\n            select(Signal).where(Signal.source_ref == signal_hash)\n        ).first()\n        \n        if existing:\n            print(f\"[JOB_BOARD][DUP] Signal already exists: {job.company_name}\")\n            continue\n        \n        summary = f\"{job.company_name} is hiring: {job.title} in {job.location}\"\n        if job.niche:\n            summary += f\" ({job.niche.upper()} sector)\"\n        \n        signal = Signal(\n            source_type=\"job_board\",\n            source_ref=signal_hash,\n            source_url=job.url or \"\",\n            context_summary=summary,\n            geography=job.geography or \"South Florida\",\n            niche=job.niche or \"\",\n            raw_data=json.dumps(job.to_signal_data()),\n            score=70,\n        )\n        \n        if not dry_run:\n            session.add(signal)\n            signals.append(signal)\n            print(f\"[JOB_BOARD][SIGNAL] Created: {job.company_name} - {job.title}\")\n        else:\n            print(f\"[JOB_BOARD][DRY_RUN] Would create: {job.company_name} - {job.title}\")\n    \n    if not dry_run and signals:\n        session.commit()\n    \n    return signals\n\n\ndef create_lead_events_from_signals(\n    session: Session,\n    signals: List[Signal],\n    dry_run: bool = False\n) -> List[LeadEvent]:\n    \"\"\"\n    Create LeadEvent records from job board signals.\n    \n    Args:\n        session: Database session\n        signals: List of Signal objects\n        dry_run: If True, don't persist to database\n    \n    Returns:\n        List of created LeadEvent objects\n    \"\"\"\n    lead_events = []\n    \n    for signal in signals:\n        try:\n            raw_data = json.loads(signal.raw_data) if signal.raw_data else {}\n        except json.JSONDecodeError:\n            raw_data = {}\n        \n        company_name = raw_data.get(\"company_name\", \"Unknown Company\")\n        niche = raw_data.get(\"niche\", signal.niche or \"trades\")\n        \n        lead_event = LeadEvent(\n            signal_id=signal.id,\n            lead_company=company_name,\n            summary=signal.context_summary,\n            category=\"JOB_POSTING\",\n            urgency_score=70,\n            status=\"NEW\",\n            enrichment_status=ENRICHMENT_STATUS_UNENRICHED,\n            recommended_action=f\"Hiring signal detected for {niche.upper()} company. Research and reach out with relevant B2B services.\",\n        )\n        \n        if not dry_run:\n            session.add(lead_event)\n            lead_events.append(lead_event)\n            print(f\"[JOB_BOARD][LEAD_EVENT] Created for: {company_name}\")\n        else:\n            print(f\"[JOB_BOARD][DRY_RUN] Would create LeadEvent for: {company_name}\")\n    \n    if not dry_run and lead_events:\n        session.commit()\n    \n    return lead_events\n\n\ndef run_job_board_ingestion(\n    session: Session,\n    niches: Optional[List[str]] = None,\n    regions: Optional[List[str]] = None,\n    max_per_niche: int = 3,\n    dry_run: bool = False\n) -> Dict:\n    \"\"\"\n    Main entry point for job board ingestion.\n    \n    Args:\n        session: Database session\n        niches: Trade niches to search\n        regions: Regions to search\n        max_per_niche: Max jobs per niche per region\n        dry_run: Skip database writes\n    \n    Returns:\n        Summary dict with counts\n    \"\"\"\n    print(\"[JOB_BOARD][START] Beginning job board ingestion\")\n    \n    jobs = fetch_job_postings(niches, regions, max_per_niche)\n    print(f\"[JOB_BOARD][FETCH] Found {len(jobs)} job postings\")\n    \n    signals = create_signals_from_jobs(session, jobs, dry_run)\n    print(f\"[JOB_BOARD][SIGNALS] Created {len(signals)} signals\")\n    \n    lead_events = create_lead_events_from_signals(session, signals, dry_run)\n    print(f\"[JOB_BOARD][LEAD_EVENTS] Created {len(lead_events)} lead events\")\n    \n    result = {\n        \"jobs_found\": len(jobs),\n        \"signals_created\": len(signals),\n        \"lead_events_created\": len(lead_events),\n        \"dry_run\": dry_run,\n    }\n    \n    print(f\"[JOB_BOARD][COMPLETE] {result}\")\n    return result\n\n\nprint(\"[JOB_BOARD][STARTUP] Job Board Connector loaded - EPIC 3.2\")\n","path":null,"size_bytes":20094,"size_tokens":null}},"version":2}